{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Resnet\"\n",
    "author: \"galopy\"\n",
    "date: \"December 4, 2023\"\n",
    "toc: true\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: galopyz/delicious-nbdev\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog, we will talk about Residual network (Resnet). Resnet came from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He et al. We have seen Kaiming/He initialization from the author before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot.png\" alt=\"Data\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Colab Notebooks\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/'Colab Notebooks'\n",
    "!pip -q install torcheval\n",
    "!pip -q install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "# from miniai.resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.xtras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = get_dls()\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "astats = ActivationStats(fc.risinstance(GeneralRelu))\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=False), astats]\n",
    "iw = partial(init_weights, leaky=0.1)\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the code, let's see what resent is and why it works conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, the team found that deep neural networks performed worse than shallow neural networks. In theory, a deeper net should capture more details and perform better. The problem persisted even when they built the deep neural net from the shallow one appended with additional layers. If appended layers did nothing, the deeper net should perform as well as the shallower net. However, these appended layers hampered the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resblock.png\" alt=\"Resblock\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2. from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of appending layers at the end of the shallow net, they used *deep residual learning framework*. So, there is an input, **x**, and two layers, **F**. Applying the layers **F** on **x** results in **F(x)**, and we add **x** to this, resulting in **F(x) + x**. Here, we can consider **F** as the additional layers we appended at the end of the shallow net in the previous approach. However, because we are doing **F(x) + x**, **x** acts as a stabilizer. It stabilizes **F(x)** if there is no improvement. **x** is called identity and stabilizes the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get into the code. We can define **F(x) + x** as a `ResBlock`. We define `_conv_block`, which has two convolutional layers. The first layer changes the input from `ni` to `nf` with stride one, and the second layer uses the given stride without activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n",
    "    return nn.Sequential(conv(ni, nf, ks, 1, act, norm, bias),\n",
    "                         conv(nf, nf, ks, stride, False, norm, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `ResBlock`, we use `nn.AvgPool2d` if stride is two and a convolutional layer with kernel size one to match the shape of **x** and **F(x)** when there is a stride and/or `ni` is different from `nf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n",
    "        super().__init__()\n",
    "        self.conv = _conv_block(ni, nf, ks, stride, act, norm, bias)\n",
    "        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "        self.eye = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=False)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.eye(self.pool(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=[8,16,32,64,128,256], norm=None):\n",
    "    layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n",
    "    return nn.Sequential(*layers, conv(nfs[-1],10, act=None, norm=False, bias=True),\n",
    "                         nn.Flatten()).to(def_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking into the input and output shapes from the layers, we can look at layers and their shapes quickly. By using the summary, it is more convenient to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes(hook, m, inp, outp):\n",
    "    print(m.__class__.__name__, inp[0].shape, outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResBlock torch.Size([1024, 1, 28, 28]) torch.Size([1024, 8, 28, 28])\n",
      "ResBlock torch.Size([1024, 8, 28, 28]) torch.Size([1024, 16, 14, 14])\n",
      "ResBlock torch.Size([1024, 16, 14, 14]) torch.Size([1024, 32, 7, 7])\n",
      "ResBlock torch.Size([1024, 32, 7, 7]) torch.Size([1024, 64, 4, 4])\n",
      "ResBlock torch.Size([1024, 64, 4, 4]) torch.Size([1024, 128, 2, 2])\n",
      "ResBlock torch.Size([1024, 128, 2, 2]) torch.Size([1024, 256, 1, 1])\n",
      "Sequential torch.Size([1024, 256, 1, 1]) torch.Size([1024, 10, 1, 1])\n",
      "Flatten torch.Size([1024, 10, 1, 1]) torch.Size([1024, 10])\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, cbs=[SingleBatchCB(), DeviceCB()])\n",
    "with Hooks(model, print_shapes) as h: learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can patch it into the `Learner` and use it as a class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def summary(self:Learner):\n",
    "    res = '|Module|Input|Output|Num params|\\n|--|--|--|--|\\n'\n",
    "    num = 0\n",
    "    def _f(hook, m, inp, outp):\n",
    "        nonlocal res, num\n",
    "        num_params = sum(o.numel() for o in m.parameters())\n",
    "        res += f'|{m.__class__.__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{num_params}|\\n'\n",
    "        num += num_params\n",
    "    with Hooks(self.model, _f) as hook: self.fit(1, train=False, cbs=[SingleBatchCB()])\n",
    "    print('Total number of params:', num)\n",
    "    if fc.IN_NOTEBOOK:\n",
    "        from IPython.display import Markdown\n",
    "        return Markdown(res)\n",
    "    else:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params: 1247362\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|Module|Input|Output|Num params|\n",
       "|--|--|--|--|\n",
       "|ResBlock|(1024, 1, 28, 28)|(1024, 8, 28, 28)|680|\n",
       "|ResBlock|(1024, 8, 28, 28)|(1024, 16, 14, 14)|3632|\n",
       "|ResBlock|(1024, 16, 14, 14)|(1024, 32, 7, 7)|14432|\n",
       "|ResBlock|(1024, 32, 7, 7)|(1024, 64, 4, 4)|57536|\n",
       "|ResBlock|(1024, 64, 4, 4)|(1024, 128, 2, 2)|229760|\n",
       "|ResBlock|(1024, 128, 2, 2)|(1024, 256, 1, 1)|918272|\n",
       "|Sequential|(1024, 256, 1, 1)|(1024, 10, 1, 1)|23050|\n",
       "|Flatten|(1024, 10, 1, 1)|(1024, 10)|0|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GlobalAvgPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model only works on images with 28 by 28 pixels. To use images with higher resolutions, we can use `GlobalAvgPool`. It simply averages the last two dimensions into one by one. We can then use flatten to remove these dimensions. Then, we can use a linear layer to create an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool(nn.Module):\n",
    "    def forward(self, x): return x.mean((-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=[8,16,32,64,128,256], norm=None):\n",
    "    layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n",
    "    return nn.Sequential(*layers, GlobalAvgPool(), nn.Flatten(), nn.Linear(nfs[-1], 10)).to(def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params: 1226882\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|Module|Input|Output|Num params|\n",
       "|--|--|--|--|\n",
       "|ResBlock|torch.Size([1024, 1, 28, 28])|torch.Size([1024, 8, 28, 28])|680|\n",
       "|ResBlock|torch.Size([1024, 8, 28, 28])|torch.Size([1024, 16, 14, 14])|3632|\n",
       "|ResBlock|torch.Size([1024, 16, 14, 14])|torch.Size([1024, 32, 7, 7])|14432|\n",
       "|ResBlock|torch.Size([1024, 32, 7, 7])|torch.Size([1024, 64, 4, 4])|57536|\n",
       "|ResBlock|torch.Size([1024, 64, 4, 4])|torch.Size([1024, 128, 2, 2])|229760|\n",
       "|ResBlock|torch.Size([1024, 128, 2, 2])|torch.Size([1024, 256, 1, 1])|918272|\n",
       "|GlobalAvgPool|torch.Size([1024, 256, 1, 1])|torch.Size([1024, 256])|0|\n",
       "|Flatten|torch.Size([1024, 256])|torch.Size([1024, 256])|0|\n",
       "|Linear|torch.Size([1024, 256])|torch.Size([1024, 10])|2570|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLearner(get_model(), dls, F.cross_entropy, lr=1).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add number of flops into the summary. Number of flops provides the number of operations. The way we calculate flops here is not very accurate, but it still tells us roughly how compute intensive the layer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flops(x, h, w):\n",
    "    if x.dim()<3: return x.numel()\n",
    "    if x.dim()==4: return x.numel()*h*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we multiply by height and width if dimension is 4? Because whe dimension is 4, it is a convolutional net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([8, 2, 3, 3]), 144), (torch.Size([8]), 8)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(o.shape, o.numel()) for o in conv(2, 8).parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([8, 2]), 16), (torch.Size([8]), 8)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(o.shape, o.numel()) for o in nn.Linear(2, 8).parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def summary(self:Learner):\n",
    "    res = '|Module|Input|Output|Num params|Flops|\\n|--|--|--|--|--|\\n'\n",
    "    n_params, n_flops = 0, 0\n",
    "    def _f(hook, m, inp, outp):\n",
    "        nonlocal res, n_params, n_flops\n",
    "        num_params = sum(o.numel() for o in m.parameters())\n",
    "        *_, h, w = outp.shape\n",
    "        num_flops = sum(_flops(o, h, w) for o in m.parameters())/1e6\n",
    "        n_params += num_params\n",
    "        n_flops += num_flops\n",
    "        res += f'|{m.__class__.__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{num_params}|{num_flops:.2f}|\\n'\n",
    "    with Hooks(self.model, _f) as hook: self.fit(1, train=False, cbs=[SingleBatchCB()])\n",
    "    print('Total number of params:', n_params)\n",
    "    print('Total number of flops:', n_flops)\n",
    "    if fc.IN_NOTEBOOK:\n",
    "        from IPython.display import Markdown\n",
    "        return Markdown(res)\n",
    "    else:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params: 1226882\n",
      "Total number of flops: 4.675826000000001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|Module|Input|Output|Num params|Flops|\n",
       "|--|--|--|--|--|\n",
       "|ResBlock|(1024, 1, 28, 28)|(1024, 8, 28, 28)|680|0.51|\n",
       "|ResBlock|(1024, 8, 28, 28)|(1024, 16, 14, 14)|3632|0.70|\n",
       "|ResBlock|(1024, 16, 14, 14)|(1024, 32, 7, 7)|14432|0.70|\n",
       "|ResBlock|(1024, 32, 7, 7)|(1024, 64, 4, 4)|57536|0.92|\n",
       "|ResBlock|(1024, 64, 4, 4)|(1024, 128, 2, 2)|229760|0.92|\n",
       "|ResBlock|(1024, 128, 2, 2)|(1024, 256, 1, 1)|918272|0.92|\n",
       "|GlobalAvgPool|(1024, 256, 1, 1)|(1024, 256)|0|0.00|\n",
       "|Flatten|(1024, 256)|(1024, 256)|0|0.00|\n",
       "|Linear|(1024, 256)|(1024, 10)|2570|0.00|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLearner(get_model(), dls, F.cross_entropy, lr=1).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog, we learned about Resnet. As we have seen from the code, it is straightforward. It is conceptually easy to understand why it works as well. We also learned about creating a summary with module names, input and output shapes, number of parameters, and number of flops. It allows us to look at the big picture of the model. It's also helpful when creating a model and debugging to look at the layers' shapes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
