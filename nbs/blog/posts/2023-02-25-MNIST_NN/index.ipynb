{
 "cells": [
  {
   "cell_type": "raw",
   "id": "769bc6dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"MNIST Neural Nets\"\n",
    "author: \"galopy\"\n",
    "date: \"February 25, 2023\"\n",
    "toc: true\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: galopyz/delicious-nbdev\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e320c",
   "metadata": {},
   "source": [
    "# MNIST Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5adc723",
   "metadata": {},
   "source": [
    "In this blog, we will use neural networks to train MNIST dataset. We start with using pytorch and integrate fastai in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574de78",
   "metadata": {},
   "source": [
    "This is part of Further Research from fastbook [chapter 4](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095738a3",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67111309",
   "metadata": {},
   "source": [
    "This part is the same as last blog where we created a baseline by calculating means of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ee13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fac27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matplotlib.rc('image', cmap='Greys')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb2f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='15687680' class='' max='15683414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.03% [15687680/15683414 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [Path('training'),Path('testing')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea0c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('training/7'),Path('training/8'),Path('training/5'),Path('training/6'),Path('training/9'),Path('training/3'),Path('training/2'),Path('training/0'),Path('training/4'),Path('training/1')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'training').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35a183",
   "metadata": {},
   "source": [
    "I looked at the number of files for each number, and they are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681a774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_counts = {n: len((path/'training'/str(n)).ls()) for n in range(10)}\n",
    "trn_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af56b33",
   "metadata": {},
   "source": [
    "Since these are paths for images we have to convert them into pytorch tensors. Pytorch provides us with [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) ability, which is very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ead0fe",
   "metadata": {},
   "source": [
    "First, we turn image paths into numbers and return as L, which is an upgraded version of list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images(path):\n",
    "    return L(PILImage.create(x) for x in path.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f129ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABAUlEQVR4nO2UIY6FMBCGy2QFDoMHAQmOI5QTQE9Awj24FJ4gaMIRCDVYgkSToU+QZVfR4cFmzftUxfTLP/2TMvbhfwmCoO97RPQ872QMLkmzLAvDcNu2e9l+IYRAxHVd67p2HOdk8kLSNE33Q9u2y7LcCrgTRZFSChGnaYrj+AEjY6wsS0TUWnPOjcNfRGlRFHs/WmvjMOlNkyQ5b+YdKef8eelVSG9qWRYAMMbGcZzn+Rmp1npvSUqplDLO/8n6Zqlt28f3QYlJwvf99RviFdL6AAAAXdcRpeaiXNfdW6qqiig10zTNsb4QgnLFvP4wDMdBSnkn3w9HUXmeP2P88AYv9qxy/m6noVIAAAAASUVORK5CYII=",
      "text/plain": [
       "PILImage mode=RGB size=28x28"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = create_images(path/'training'/'1')\n",
    "im1 = ones[0]\n",
    "im1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece46898",
   "metadata": {},
   "source": [
    "We need to turn them into tensors for speed. However, we have to check the shape of tensors to make sure they are broadcastable and the shape is one we're expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49307413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_imgs(imgs):\n",
    "    \"Convert a list of images into a tensor\"\n",
    "    return torch.stack([tensor(im) for im in imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d3a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_ones = create_tensor_imgs(ones)\n",
    "stacked_ones[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbeb9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6742, 28, 28, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_ones.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc35cca",
   "metadata": {},
   "source": [
    "Looking at the shape of an image, it is interesting to find out that its shape is [28, 28, 3] (y, x, and color values). I was expecting to see [28, 28] (y and x values) because it is a black and white image. So, let's see what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c97b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [221, 221, 221],\n",
       "        [251, 251, 251]],\n",
       "\n",
       "       [[ 64,  64,  64],\n",
       "        [236, 236, 236],\n",
       "        [251, 251, 251]],\n",
       "\n",
       "       [[127, 127, 127],\n",
       "        [251, 251, 251],\n",
       "        [251, 251, 251]],\n",
       "\n",
       "       [[127, 127, 127],\n",
       "        [251, 251, 251],\n",
       "        [251, 251, 251]],\n",
       "\n",
       "       [[128, 128, 128],\n",
       "        [253, 253, 253],\n",
       "        [253, 253, 253]]], dtype=uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(im1)[10:15, 12:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f3d895",
   "metadata": {},
   "source": [
    "So, it is just filled with the same numbers per each row. We just need one column, so we can simply take a mean of last rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a33b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., 221., 251.],\n",
       "       [ 64., 236., 251.],\n",
       "       [127., 251., 251.],\n",
       "       [127., 251., 251.],\n",
       "       [128., 253., 253.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(im1)[10:15, 12:15].mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54830d67",
   "metadata": {},
   "source": [
    "So, here is the updated version of `create_tensor_imgs` that calculates a mean so that we can get rid of the last rank of our tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d201a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_imgs(imgs):\n",
    "    \"Convert a list of images into a tensor\"\n",
    "    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d211f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_ones = create_tensor_imgs(ones)\n",
    "stacked_ones[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448fcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., 221., 251.],\n",
       "       [ 64., 236., 251.],\n",
       "       [127., 251., 251.],\n",
       "       [127., 251., 251.],\n",
       "       [128., 253., 253.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(stacked_ones[0])[10:15, 12:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585f711",
   "metadata": {},
   "source": [
    "Looks good. Now, we want those pixel values to be between 0 and 1 so that it is easier to train neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_imgs(imgs):\n",
    "    \"Convert a list of images into a tensor\"\n",
    "    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb8a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.8666667 , 0.9843137 ],\n",
       "       [0.2509804 , 0.9254902 , 0.9843137 ],\n",
       "       [0.49803922, 0.9843137 , 0.9843137 ],\n",
       "       [0.49803922, 0.9843137 , 0.9843137 ],\n",
       "       [0.5019608 , 0.99215686, 0.99215686]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_ones = create_tensor_imgs(ones)\n",
    "array(stacked_ones[0])[10:15, 12:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad742cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_nums(path):\n",
    "    \"Converts path into stacked tensors.\"\n",
    "    imgs = create_images(path)\n",
    "    return create_tensor_imgs(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_zeros = stacked_nums(path/'training'/'0')\n",
    "stacked_twos = stacked_nums(path/'training'/'2')\n",
    "stacked_threes = stacked_nums(path/'training'/'3')\n",
    "stacked_fours = stacked_nums(path/'training'/'4')\n",
    "stacked_fives = stacked_nums(path/'training'/'5')\n",
    "stacked_sixs = stacked_nums(path/'training'/'6')\n",
    "stacked_sevens = stacked_nums(path/'training'/'7')\n",
    "stacked_eights = stacked_nums(path/'training'/'8')\n",
    "stacked_nines = stacked_nums(path/'training'/'9')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b0853",
   "metadata": {},
   "source": [
    "We also need test data to check how we're doing. Without test set, we don't know whether we are overfitting or not. So, we do the same process we did for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08185fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1135, 28, 28]), torch.Size([1009, 28, 28]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_0_tens = stacked_nums(path/'testing'/'0')\n",
    "tst_1_tens = stacked_nums(path/'testing'/'1')\n",
    "tst_2_tens = stacked_nums(path/'testing'/'2')\n",
    "tst_3_tens = stacked_nums(path/'testing'/'3')\n",
    "tst_4_tens = stacked_nums(path/'testing'/'4')\n",
    "tst_5_tens = stacked_nums(path/'testing'/'5')\n",
    "tst_6_tens = stacked_nums(path/'testing'/'6')\n",
    "tst_7_tens = stacked_nums(path/'testing'/'7')\n",
    "tst_8_tens = stacked_nums(path/'testing'/'8')\n",
    "tst_9_tens = stacked_nums(path/'testing'/'9')\n",
    "\n",
    "tst_tens = L([tst_0_tens, tst_1_tens, tst_2_tens, tst_3_tens, tst_4_tens, \n",
    "              tst_5_tens, tst_6_tens, tst_7_tens, tst_8_tens, tst_9_tens])\n",
    "\n",
    "tst_counts = {n: len((path/'testing'/str(n)).ls()) for n in range(10)}\n",
    "\n",
    "tst_1_tens.shape, tst_9_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ede1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 980,\n",
       " 1: 1135,\n",
       " 2: 1032,\n",
       " 3: 1010,\n",
       " 4: 982,\n",
       " 5: 892,\n",
       " 6: 958,\n",
       " 7: 1028,\n",
       " 8: 974,\n",
       " 9: 1009}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8d896",
   "metadata": {},
   "source": [
    "Now, we stack all the training tensors together to create `trn_x`. And create `trn_y` with the counts. We zip those together to create a dataset. We also change the shape of tensors from [28, 28] to [784]. This shape is more useful for matrix multiplication we will do later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c94b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_x = torch.cat([stacked_zeros, stacked_ones, stacked_twos, \n",
    "                   stacked_threes, stacked_fours, stacked_fives, \n",
    "                   stacked_sixs, stacked_sevens, stacked_eights, \n",
    "                   stacked_nines]).view(-1, 28*28)\n",
    "\n",
    "trn_y = tensor(list(flatten([[x] * y for x, y in trn_counts.items()])))\n",
    "trn_dset = list(zip(trn_x, trn_y))\n",
    "\n",
    "trn_x.shape, trn_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bac8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:>, tensor(8))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(trn_x[50000].view(28,28)), trn_y[50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9f5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = trn_dset[0]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04944efd",
   "metadata": {},
   "source": [
    "Training data looks good, so let's work on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86af4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 784]), torch.Size([10000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_x = torch.cat([tst_0_tens, tst_1_tens, tst_2_tens, \n",
    "                     tst_3_tens, tst_4_tens, tst_5_tens,\n",
    "                     tst_6_tens, tst_7_tens, tst_8_tens,\n",
    "                     tst_9_tens]).view(-1, 28*28)\n",
    "\n",
    "tst_y = tensor(list(flatten([[x] * y for x, y in tst_counts.items()])))\n",
    "tst_dset = list(zip(tst_x, tst_y))\n",
    "\n",
    "tst_x.shape, tst_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc48691",
   "metadata": {},
   "source": [
    "## Using neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1d23c",
   "metadata": {},
   "source": [
    "In order to use neural network, we need to initialize parameters, such as weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28, 10))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0839d522",
   "metadata": {},
   "source": [
    "We are initializing `weights` parameter with size (28\\*28, 10) because `28*28` is the size of our input (reshaped images) and `10` is the size of our output. Why do we return 10 numbers as an output for each image? Because we're making predictions on each category (from 0 to 9). Out of all those 10 numbers, the biggest number is the model's guess. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19f12b",
   "metadata": {},
   "source": [
    "But couldn't we just output 1 number instead and pick a target based on the range of our output? For instance, we could use sigmoid function to get our output numbers to be between 0 and 1, and if it's between 0 and 0.1, it is 0, if it's between 0.1 and 0.2, it is 1, and so on. That's also a possible approach to this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db598f",
   "metadata": {},
   "source": [
    "With those parameters, we will perform matrix multiplications. There are multiple ways to calculate it. One way involves manually transposing a matrix and getting the sum over dimension 1, and another way involves using `@` operator, which does all of that for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688bcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n",
       "         2.4677, 16.0547], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x*weights.T).sum(dim=1) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf95ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n",
       "         2.4677, 16.0547], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(xb): return xb@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94e31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.0786,   6.5823,  12.8080,  ...,  -2.3099,   2.4677,  16.0547],\n",
       "        [ -4.0905,   1.3646,  -1.6264,  ...,  -5.0465,  -8.9219,   5.3742],\n",
       "        [ -4.6858,  -1.6842,   5.6141,  ...,  -7.0370,   7.6748,   1.3543],\n",
       "        ...,\n",
       "        [-15.4864,  -1.8742,  -6.2627,  ...,  -0.1359,  10.1261,  18.8826],\n",
       "        [-22.5967,   5.1747,  -6.5157,  ...,  -2.5263,  20.9854,  12.2597],\n",
       "        [-15.9537,  -2.7749,   3.1555,  ...,   5.3508,  11.7086,  13.8179]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear(trn_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887612a4",
   "metadata": {},
   "source": [
    "So, we know that these numbers represent \"socres\" for each category. Let's look at the first prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897fc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n",
       "         2.4677, 16.0547], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b628bc7",
   "metadata": {},
   "source": [
    "From these numbers, the biggest number is 16.0547, which is a prediction for 9. So, this predicted 9. We can use `torch.argmax` to calculate the maximum index easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a1a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc0ac0",
   "metadata": {},
   "source": [
    "For multiple predictions, we can specify the dimension to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d5db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 5, 5,  ..., 9, 8, 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de13d33",
   "metadata": {},
   "source": [
    "With predictions on categories, we can calculate accuracy by comparing it to the targets (`trn_y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5eb805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.0433)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds.argmax(dim=1) == trn_y).float().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a8b69",
   "metadata": {},
   "source": [
    "We have about 10% accuracy since we are using randomly initialized parameters to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(pred, targ):\n",
    "    return (pred.argmax(dim=1) == targ).float().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1a8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.0433)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(preds, trn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f6089",
   "metadata": {},
   "source": [
    "## Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71085456",
   "metadata": {},
   "source": [
    "In order to improve our accuracy, we have to tweak our weights and biases (parameters). To improve our parameters, we have to use a loss function. Then, we can calculate gradients and use them to update our parameters. Loss functions are just mathematical functions, but they have smooth shapes. This is important because gradients cannot be calculated on sharp edges. Let's take look at a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(fn, title=None, xmin=-4.1, xmax=4.1):\n",
    "    x = torch.linspace(xmin, xmax, 20)\n",
    "    plt.plot(x, fn(x))\n",
    "    if title: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a257ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(torch.nn.Sigmoid(), title='Sigmoid function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b4ccd",
   "metadata": {},
   "source": [
    "Sigmoid function has no sharp edge, so gradients can be found anywhere. This will also ensure that predictions will be between 0 and 1. We typically use sigmoid function when we try to predict boolean values. For instance, we could use this function for problems like \"is it a cat or dog?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194018f",
   "metadata": {},
   "source": [
    "Because we have multiple categories to predict, we use cross entropy loss. Cross entropy loss can be thought of as sigmoid designed for multiple categories. Cross entropy loss is composed of multiple functions. First, we use a softmax function ensures that all numbers are positive and add up to 1. Also, this function takes exponential of all values, so even the slight difference will be magnified. This will result in choosing one class with more confidence. Second, we take the log of the activations we got from softmax. Let's look at a log function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fb2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(torch.log, title=\"Log function\", xmin=-0.1, xmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2caf5c",
   "metadata": {},
   "source": [
    "In log function, we get a very big negative value if a number gets closer to 0, and reaches 0 when it gets 1. So, if our activation is close to 0 after softmax, it will be a big negative number after taking a log. This will result in a big gradient. However, if the number is close to 1, it is good enough. So, it will punish numbers that are far off, but forgiving for close numbers. But we still have problems. We still have 10 activation numbers and all of them are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8045415",
   "metadata": {},
   "source": [
    "The last function we will use is negative log likelihood, which picks one number from our 10 activations and multiply by -1 to make it a positive value. Although it has log in its name, it actually does not take the log function. We have to do it ourselves. Let's take a look at how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a9252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.6022e-09, 7.2222e-05, 3.6513e-02, 3.9309e-03, 2.0602e-11, 2.0636e-02,\n",
       "        2.5585e-04, 9.9276e-09, 1.1795e-06, 9.3859e-01],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_acts = torch.nn.Softmax(dim=1)(preds)\n",
    "sm_acts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32454382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19.1967,  -9.5358,  -3.3101,  -5.5389, -24.6057,  -3.8807,  -8.2709,\n",
       "        -18.4279, -13.6504,  -0.0634], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_acts = torch.log(sm_acts)\n",
    "log_acts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514945d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657c1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.1967, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.NLLLoss()(log_acts[0], trn_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3d5b6",
   "metadata": {},
   "source": [
    "Notice that we get the same values for the built in cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6540b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3907, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.NLLLoss()(log_acts, trn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8390c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3907, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()(preds, trn_y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adcc02",
   "metadata": {},
   "source": [
    "It's easier to use the built in function, and it's also faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.5 ms ± 627 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.nn.CrossEntropyLoss()(preds, trn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427a14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.nn.NLLLoss()(torch.log(torch.nn.Softmax(dim=1)(preds)), trn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1afc78",
   "metadata": {},
   "source": [
    "## Calculating gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4757aa",
   "metadata": {},
   "source": [
    "So, we calculated the loss. With loss, we can calculate the gradients by calling `backward()`. Then, we can take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020f721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-9.3132e-09]), tensor(-1.0490e-05))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.grad, weights.grad.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f5854",
   "metadata": {},
   "source": [
    "We can define a function it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7439e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    \"Calculate the gradients\"\n",
    "    preds = model(xb)\n",
    "    loss = torch.nn.CrossEntropyLoss()(preds, yb)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28, 10))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856ce3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.2164e-10), tensor(-9.3132e-09))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(trn_x, trn_y, linear)\n",
    "weights.grad.mean(), bias.grad.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e988a0",
   "metadata": {},
   "source": [
    "After calculating the gradients, we can update our parameters using the gradients. We can multiply parameter's gradient by any number, which is referred to as a learning rate. If it's big, it will train faster, and if it's slow, it will train slower. However, if it's too big, it cannot train. So, we can try couple numbers and see what works and what doensn't. After updating, we have to reset the gradients. If we don't, gradients will keep growing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23be31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.3039e-08]), tensor(-9.5367e-07))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = init_params((28*28, 10))\n",
    "bias = init_params(1)\n",
    "calc_grad(trn_x, trn_y, linear)\n",
    "bias.grad, weights.grad.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10b072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.6077e-08]), tensor(-1.9073e-06))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(trn_x, trn_y, linear)\n",
    "bias.grad, weights.grad.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cdb521",
   "metadata": {},
   "source": [
    "See how it grew? we have to use `zero_()` on the gradients. Training _ on functions means in place function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6750c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.data -= weights.grad * 10\n",
    "bias.data -= bias.grad * 10\n",
    "weights.grad.zero_()\n",
    "bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ec1f3",
   "metadata": {},
   "source": [
    "## First training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1b86f",
   "metadata": {},
   "source": [
    "If we put everything together, this is what it looks like. We will train with the whole data 20 times, which means we are using 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.9500)\n",
      "tensor(38.9100)\n",
      "tensor(40.2700)\n",
      "tensor(48.3800)\n",
      "tensor(42.6600)\n",
      "tensor(51.5200)\n",
      "tensor(45.4200)\n",
      "tensor(57.6000)\n",
      "tensor(60.3500)\n",
      "tensor(62.2100)\n",
      "tensor(67.2600)\n",
      "tensor(70.8800)\n",
      "tensor(75.0300)\n",
      "tensor(71.8100)\n",
      "tensor(76.1800)\n",
      "tensor(82.2500)\n",
      "tensor(85.1200)\n",
      "tensor(85.3800)\n",
      "tensor(85.1700)\n",
      "tensor(85.1200)\n"
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28, 10))\n",
    "bias = init_params(1)\n",
    "for _ in range(20):\n",
    "    calc_grad(trn_x, trn_y, linear)\n",
    "    weights.data -= weights.grad * 10\n",
    "    weights.grad.zero_()\n",
    "    bias.data -= bias.grad * 10\n",
    "    bias.grad.zero_()\n",
    "    pred = linear(tst_x)\n",
    "    print(acc(pred, tst_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01b067",
   "metadata": {},
   "source": [
    "We got over 80% accuracy, which is better tan our baseline using means of images from the last time, which had 66% accuracy. However, we can still make this better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2022e3",
   "metadata": {},
   "source": [
    "## Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148d78a",
   "metadata": {},
   "source": [
    "We updated our parameters after looking at the whole dataset. The MNIST dataset is very small, so it's not a problem, but dataset can get really big, and it would take a long time to train this way. Another approach to train is update parameters after each image. This is not a good idea either as our model learns about one image at a time, which can lead to overfitting. That's why we use small batches of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fdfad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(trn_dset, bs=256)\n",
    "xb, yb = first(dl)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4065e3",
   "metadata": {},
   "source": [
    "With fastai's DataLoader, we will have 256 items per batch. And we will do the same thing for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ada2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dl = DataLoader(tst_dset, bs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr=1.):\n",
    "    params = weights, bias\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb, yb, linear)\n",
    "        for p in params:\n",
    "            p.data -= p.grad * lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [acc(model(xb), yb) for xb, yb in tst_dl]\n",
    "    return round(tensor(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46f8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9961; 27.8516; 39.1016; 46.3672; 51.3281; 54.8535; 57.2656; 59.4922; 61.1328; 62.5098; 63.6328; 64.5605; 65.2637; 65.9766; 66.6699; 67.2754; 67.9004; 68.418; 68.9355; 69.3164; 69.6777; 70.0586; 70.3809; 70.625; 70.8789; 71.1328; 71.3379; 71.5039; 71.6602; 71.8262; "
     ]
    }
   ],
   "source": [
    "weights = init_params((28*28, 10))\n",
    "bias = init_params(1)\n",
    "for _ in range(30):\n",
    "    train(0.1)\n",
    "    print(validate_epoch(linear), end='; ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a078f",
   "metadata": {},
   "source": [
    "Accuracy is not as good as using the whole dataset, but let's keep trying more things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a3efd",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd8045",
   "metadata": {},
   "source": [
    "We can use pytorch's `torch.nn.Linear` instead of our `linear`. Good thing about using this is that we can get grab parameters easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3921421",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Linear(28*28, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb4f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]), torch.Size([10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = list(linear_model.parameters())\n",
    "w.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5ae18",
   "metadata": {},
   "source": [
    "We define a `BasicOptim` class that has `step` and `zero_grad` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cdb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad * self.lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3451b2",
   "metadata": {},
   "source": [
    "Optimizer can be used this way. And we can update our train function to use the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(list(linear_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bf0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6a679",
   "metadata": {},
   "source": [
    "It's good to define a function to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, model):\n",
    "    for _ in range(epochs):\n",
    "        train(model)\n",
    "        print(validate_epoch(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2266\n",
      "31.3184\n",
      "46.7578\n",
      "55.4785\n",
      "61.2988\n",
      "65.1953\n",
      "68.4375\n",
      "70.9473\n",
      "72.9297\n",
      "74.8047\n",
      "76.25\n",
      "77.5391\n",
      "78.6035\n",
      "79.6094\n",
      "80.3027\n",
      "80.9473\n",
      "81.6016\n",
      "82.2754\n",
      "82.8027\n",
      "83.2031\n"
     ]
    }
   ],
   "source": [
    "train_model(20, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efc903",
   "metadata": {},
   "source": [
    "We can also use fastai's SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28, 10)\n",
    "opt = SGD(list(linear_model.parameters()), .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff7e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2461\n",
      "32.4121\n",
      "47.0117\n",
      "55.4004\n",
      "61.1621\n",
      "65.166\n",
      "68.418\n",
      "70.8398\n",
      "72.9297\n",
      "75.1172\n",
      "76.377\n",
      "77.5488\n",
      "78.623\n",
      "79.4922\n",
      "80.3809\n",
      "81.0547\n",
      "81.582\n",
      "82.2363\n",
      "82.7148\n",
      "83.0664\n"
     ]
    }
   ],
   "source": [
    "train_model(20, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d68ead",
   "metadata": {},
   "source": [
    "Also, we can use fastai's learner. We just need `DataLoaders`, which combines training dataloaders with test dataloaders. By using `learn.fit`, we can train easily and get a nice output, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, tst_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85191763",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28, 10), loss_func=nn.CrossEntropyLoss(), opt_func=SGD, metrics=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af1715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.030682</td>\n",
       "      <td>2.442845</td>\n",
       "      <td>10.260000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.756178</td>\n",
       "      <td>1.755957</td>\n",
       "      <td>30.840000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.652057</td>\n",
       "      <td>1.387471</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.593682</td>\n",
       "      <td>1.172395</td>\n",
       "      <td>54.570000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.555542</td>\n",
       "      <td>1.030824</td>\n",
       "      <td>60.259998</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.528407</td>\n",
       "      <td>0.930011</td>\n",
       "      <td>64.510002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.507978</td>\n",
       "      <td>0.854310</td>\n",
       "      <td>67.820000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.491943</td>\n",
       "      <td>0.795262</td>\n",
       "      <td>70.309998</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.478946</td>\n",
       "      <td>0.747864</td>\n",
       "      <td>72.440002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.468138</td>\n",
       "      <td>0.708950</td>\n",
       "      <td>74.440002</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.458963</td>\n",
       "      <td>0.676414</td>\n",
       "      <td>75.860001</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.451041</td>\n",
       "      <td>0.648797</td>\n",
       "      <td>77.139999</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.444104</td>\n",
       "      <td>0.625053</td>\n",
       "      <td>78.199997</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.437957</td>\n",
       "      <td>0.604415</td>\n",
       "      <td>79.160004</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.432457</td>\n",
       "      <td>0.586306</td>\n",
       "      <td>80.010002</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.427493</td>\n",
       "      <td>0.570282</td>\n",
       "      <td>80.739998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.422980</td>\n",
       "      <td>0.555999</td>\n",
       "      <td>81.290001</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.418851</td>\n",
       "      <td>0.543184</td>\n",
       "      <td>82.040001</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.415051</td>\n",
       "      <td>0.531618</td>\n",
       "      <td>82.360001</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.411538</td>\n",
       "      <td>0.521124</td>\n",
       "      <td>82.720001</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(20, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdbe24",
   "metadata": {},
   "source": [
    "We can improve this accuracy with adding another linear layers and a relu layer. We create a deeper network as long as there's a relu layer in between each linear layer. Relu layer simply turns negative numbers to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28*28, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b76a6b",
   "metadata": {},
   "source": [
    "We made a network of layers. Since it is deeper, we have to reduce the learning rate, which results in slower training speed. However, we can train for more epochs. Let's try training for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7a533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.257359</td>\n",
       "      <td>2.186582</td>\n",
       "      <td>28.950001</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.927473</td>\n",
       "      <td>2.062460</td>\n",
       "      <td>16.990000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.586289</td>\n",
       "      <td>1.925866</td>\n",
       "      <td>19.389999</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.296686</td>\n",
       "      <td>1.757989</td>\n",
       "      <td>29.430000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.077161</td>\n",
       "      <td>1.585793</td>\n",
       "      <td>37.320000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.920138</td>\n",
       "      <td>1.439800</td>\n",
       "      <td>42.250000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.809401</td>\n",
       "      <td>1.322030</td>\n",
       "      <td>45.910000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.729531</td>\n",
       "      <td>1.226982</td>\n",
       "      <td>48.950001</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.669996</td>\n",
       "      <td>1.148582</td>\n",
       "      <td>52.099998</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.624211</td>\n",
       "      <td>1.082637</td>\n",
       "      <td>54.900002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.587927</td>\n",
       "      <td>1.026398</td>\n",
       "      <td>57.480000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.558507</td>\n",
       "      <td>0.977675</td>\n",
       "      <td>59.820000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.534146</td>\n",
       "      <td>0.934994</td>\n",
       "      <td>62.150002</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.513631</td>\n",
       "      <td>0.897281</td>\n",
       "      <td>63.990002</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>0.863748</td>\n",
       "      <td>65.589996</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.480910</td>\n",
       "      <td>0.833677</td>\n",
       "      <td>66.970001</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.467640</td>\n",
       "      <td>0.806602</td>\n",
       "      <td>68.199997</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.455922</td>\n",
       "      <td>0.782081</td>\n",
       "      <td>69.519997</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.445503</td>\n",
       "      <td>0.759768</td>\n",
       "      <td>70.489998</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.436185</td>\n",
       "      <td>0.739375</td>\n",
       "      <td>71.389999</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.427801</td>\n",
       "      <td>0.720681</td>\n",
       "      <td>72.199997</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.420199</td>\n",
       "      <td>0.703508</td>\n",
       "      <td>73.080002</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.413270</td>\n",
       "      <td>0.687696</td>\n",
       "      <td>73.760002</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.406932</td>\n",
       "      <td>0.673062</td>\n",
       "      <td>74.489998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.401142</td>\n",
       "      <td>0.659454</td>\n",
       "      <td>75.080002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.395792</td>\n",
       "      <td>0.646811</td>\n",
       "      <td>75.730003</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.390829</td>\n",
       "      <td>0.635046</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.386238</td>\n",
       "      <td>0.624030</td>\n",
       "      <td>76.760002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.381959</td>\n",
       "      <td>0.613709</td>\n",
       "      <td>77.419998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.377980</td>\n",
       "      <td>0.604007</td>\n",
       "      <td>77.910004</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.374245</td>\n",
       "      <td>0.594876</td>\n",
       "      <td>78.379997</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.370748</td>\n",
       "      <td>0.586283</td>\n",
       "      <td>78.849998</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.367451</td>\n",
       "      <td>0.578153</td>\n",
       "      <td>79.239998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.364345</td>\n",
       "      <td>0.570461</td>\n",
       "      <td>79.610001</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.361401</td>\n",
       "      <td>0.563176</td>\n",
       "      <td>79.919998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.358619</td>\n",
       "      <td>0.556240</td>\n",
       "      <td>80.260002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.355972</td>\n",
       "      <td>0.549619</td>\n",
       "      <td>80.610001</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.353456</td>\n",
       "      <td>0.543326</td>\n",
       "      <td>80.839996</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.351051</td>\n",
       "      <td>0.537354</td>\n",
       "      <td>81.120003</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.348749</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>81.330002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.346539</td>\n",
       "      <td>0.526188</td>\n",
       "      <td>81.589996</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.344433</td>\n",
       "      <td>0.520945</td>\n",
       "      <td>81.839996</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.342406</td>\n",
       "      <td>0.515914</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.340465</td>\n",
       "      <td>0.511089</td>\n",
       "      <td>82.190002</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.338599</td>\n",
       "      <td>0.506455</td>\n",
       "      <td>82.290001</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.336791</td>\n",
       "      <td>0.501979</td>\n",
       "      <td>82.480003</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.335052</td>\n",
       "      <td>0.497672</td>\n",
       "      <td>82.660004</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.333371</td>\n",
       "      <td>0.493508</td>\n",
       "      <td>82.809998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.331753</td>\n",
       "      <td>0.489475</td>\n",
       "      <td>83.040001</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.330158</td>\n",
       "      <td>0.485592</td>\n",
       "      <td>83.220001</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.328622</td>\n",
       "      <td>0.481835</td>\n",
       "      <td>83.430000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.327128</td>\n",
       "      <td>0.478212</td>\n",
       "      <td>83.589996</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.325685</td>\n",
       "      <td>0.474692</td>\n",
       "      <td>83.750000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.324260</td>\n",
       "      <td>0.471305</td>\n",
       "      <td>83.959999</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.322876</td>\n",
       "      <td>0.467993</td>\n",
       "      <td>84.110001</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.321540</td>\n",
       "      <td>0.464754</td>\n",
       "      <td>84.239998</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.320235</td>\n",
       "      <td>0.461603</td>\n",
       "      <td>84.370003</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.318949</td>\n",
       "      <td>0.458543</td>\n",
       "      <td>84.449997</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.317698</td>\n",
       "      <td>0.455552</td>\n",
       "      <td>84.589996</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.316479</td>\n",
       "      <td>0.452645</td>\n",
       "      <td>84.690002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.315262</td>\n",
       "      <td>0.449823</td>\n",
       "      <td>84.820000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.314078</td>\n",
       "      <td>0.447053</td>\n",
       "      <td>84.949997</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.312930</td>\n",
       "      <td>0.444345</td>\n",
       "      <td>85.080002</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.311791</td>\n",
       "      <td>0.441711</td>\n",
       "      <td>85.230003</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.310679</td>\n",
       "      <td>0.439148</td>\n",
       "      <td>85.300003</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.309597</td>\n",
       "      <td>0.436635</td>\n",
       "      <td>85.400002</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.308528</td>\n",
       "      <td>0.434166</td>\n",
       "      <td>85.540001</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.307462</td>\n",
       "      <td>0.431768</td>\n",
       "      <td>85.599998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.306423</td>\n",
       "      <td>0.429406</td>\n",
       "      <td>85.709999</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.305414</td>\n",
       "      <td>0.427090</td>\n",
       "      <td>85.900002</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.304411</td>\n",
       "      <td>0.424796</td>\n",
       "      <td>86.040001</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.303422</td>\n",
       "      <td>0.422561</td>\n",
       "      <td>86.129997</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.302441</td>\n",
       "      <td>0.420381</td>\n",
       "      <td>86.190002</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.301463</td>\n",
       "      <td>0.418240</td>\n",
       "      <td>86.239998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.300526</td>\n",
       "      <td>0.416106</td>\n",
       "      <td>86.339996</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.299587</td>\n",
       "      <td>0.414013</td>\n",
       "      <td>86.370003</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.298673</td>\n",
       "      <td>0.411971</td>\n",
       "      <td>86.470001</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.297748</td>\n",
       "      <td>0.409977</td>\n",
       "      <td>86.570000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.296841</td>\n",
       "      <td>0.407999</td>\n",
       "      <td>86.629997</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.295964</td>\n",
       "      <td>0.406036</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.295090</td>\n",
       "      <td>0.404103</td>\n",
       "      <td>86.889999</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.294211</td>\n",
       "      <td>0.402208</td>\n",
       "      <td>86.970001</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.293342</td>\n",
       "      <td>0.400348</td>\n",
       "      <td>87.059998</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.292497</td>\n",
       "      <td>0.398527</td>\n",
       "      <td>87.160004</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.291661</td>\n",
       "      <td>0.396721</td>\n",
       "      <td>87.199997</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.290825</td>\n",
       "      <td>0.394935</td>\n",
       "      <td>87.250000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.289988</td>\n",
       "      <td>0.393174</td>\n",
       "      <td>87.309998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.289170</td>\n",
       "      <td>0.391404</td>\n",
       "      <td>87.360001</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.288363</td>\n",
       "      <td>0.389678</td>\n",
       "      <td>87.400002</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.287559</td>\n",
       "      <td>0.387958</td>\n",
       "      <td>87.449997</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.286770</td>\n",
       "      <td>0.386272</td>\n",
       "      <td>87.529999</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.285985</td>\n",
       "      <td>0.384583</td>\n",
       "      <td>87.550003</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.285202</td>\n",
       "      <td>0.382913</td>\n",
       "      <td>87.599998</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.284413</td>\n",
       "      <td>0.381289</td>\n",
       "      <td>87.699997</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.283646</td>\n",
       "      <td>0.379661</td>\n",
       "      <td>87.739998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.282881</td>\n",
       "      <td>0.378050</td>\n",
       "      <td>87.779999</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.376432</td>\n",
       "      <td>87.809998</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.281396</td>\n",
       "      <td>0.374831</td>\n",
       "      <td>87.860001</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.280646</td>\n",
       "      <td>0.373255</td>\n",
       "      <td>87.919998</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.279895</td>\n",
       "      <td>0.371665</td>\n",
       "      <td>87.949997</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, net, loss_func=nn.CrossEntropyLoss(), \n",
    "                opt_func=SGD, metrics=acc)\n",
    "learn.fit(100, 0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36295f6",
   "metadata": {},
   "source": [
    "After 100 epochs, we reached over 87% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c546d",
   "metadata": {},
   "source": [
    "So, we completed the project. That was very fun. Thanks for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
