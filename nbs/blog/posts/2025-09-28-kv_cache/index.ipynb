{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6c0ebc2c",
   "metadata": {},
   "source": [
    "---\r\n",
    "title: \"HuggingFace KV cache\"\r\n",
    "author: \"galopy\"\r\n",
    "date: \"September 28, 2025\"\r\n",
    "toc: true\r\n",
    "skip_showdoc: true\r\n",
    "skip_exec: true\r\n",
    "comments:\r\n",
    "  utterances:\r\n",
    "    repo: galopyz/delicious-nbdev\r\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05969f37",
   "metadata": {},
   "source": [
    "# HuggingFace KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe153dc2",
   "metadata": {},
   "source": [
    "In this notebook, we will learn about what KV cache is and try out different kinds of HuggingFace KV cache, such as dynamic and static. We will only look at decoder models. To follow along, use [colab notebook](https://colab.research.google.com/drive/1K51XGOwjvldzeMeZfDJrEv7o7aHDV_qa?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0e66f",
   "metadata": {},
   "source": [
    "## Brief look at the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e999cd",
   "metadata": {},
   "source": [
    "To understand what KV cache is and why it is useful, we have to understand what the transformer does in a large languge model. Transformer block consists of self attention and feed forward network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a093b",
   "metadata": {},
   "source": [
    "Let's take a look at the transformer in a big picture. In decoder models, the goal is to predict the next word (or a token). The self attention highlights which words are important by doing weighted sum. For instance, we have a sentence, \"I was thirsty, so I\", and we want to predict the next word. It makes sense to pay attention to the word, \"thirsty\". That's what the attention does. And the feed forward network figures out the next words with these clues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1382be6",
   "metadata": {},
   "source": [
    "![kv_cache](kv_cache.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c97eee",
   "metadata": {},
   "source": [
    "Inside of the self attention, we calculate query, key, and value for each token. So, from the example sentence we had, \"I was thirsty,\", each word has a query, key, and value.\n",
    "\n",
    "- Query tells which word to predict. If we use query for \"thirsty,\", it is like asking what is the next word after \"thirsty\"? It is easy to think of a query as a one word. If we used a differnt word, like \"I\", it is like asking what comes after \"I\"?\n",
    "\n",
    "- Keys tells which words to focus more respect to query. We can think of keys as a sentence with words \"thisty\" and other words that come before it, \"I was thirsty,\". To predict the next word after \"thirsty\", it is important to pay attention to say \"I\". With query and keys, we gained information about which words to focus.\n",
    "\n",
    "- Value provides an updated sentence with highlighted words. Using query and keys, we know which ones to focus from the sentence, but we need to apply this information into the sentence itself. Think of values as the whole sentence. And with query + keys + values, we have a highlighted sentence.\n",
    "\n",
    "Let's assume the model generated \"so\" as an output.\n",
    "\n",
    "You see how we needed a query for one word we want to make prediction of, but needed keys and values for the whole sentence? For instance, we fed the model with \"I was thirsty,\" and its output was \"so\". To generate \"so\", it calculated keys and values for \"I was thirsty,\". Now we have \"I was thirsty, so\" as an input to the model. The model needs \"so\" as a query, \"I was thirsty, so\" as keys and values to predict the next word. But we calculated keys and values for \"I was thirsty,\" already. It would be a waste to calculate it again (which grows quadratically).\n",
    "\n",
    "By saving the keys and values for the previous words, we can compute the keys and values with linear time complexity. However, it consumes more memory because we have to save those values.This is KV cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b50de",
   "metadata": {},
   "source": [
    "There are two ways to use KV cache: dynamic and static. In dynamic cache, cache grows with each generation. However, static cache has a fixed cache.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c345b",
   "metadata": {},
   "source": [
    "I didn't go into details when explaining the transformer in LLMs. If you would like to learn more, I included resources to learn more about them in the conclusion section at the end.\n",
    "\n",
    "Also, Hugging Face supports more sophisticated techniques such as `Cache offloading`, `Quantized cache`, `Encoder-decoder cache`, and `Model-specific cache`. Please take a look at [KV cache strategies](https://huggingface.co/docs/transformers/v4.56.2/kv_cache) from HuggingFace documentations for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1e4a7",
   "metadata": {},
   "source": [
    "## KV Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5301e2",
   "metadata": {},
   "source": [
    "Now, let's dive into the code. We first import libraries and setup tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, DynamicCache, StaticCache\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "num_new_tokens = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tks(model, input_ids, num_new_tokens=200, **kwargs):\n",
    "    \"Generate text and print time and tokens/sec\"\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=num_new_tokens, **kwargs)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "    print(f\"{int(num_new_tokens/total_time)} tokens/sec\")\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_giga_bytes(bytes): return f'{bytes / 1024 / 1024 / 1024:.2f} GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa3fb1",
   "metadata": {},
   "source": [
    "KV cache speeds up the inference. HuggingFace offers dynamic cache and static cache.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fce765",
   "metadata": {},
   "source": [
    "Here's more info on [Caching](https://huggingface.co/docs/transformers/en/cache_explanation) and [KV cache strategies](https://huggingface.co/docs/transformers/en/kv_cache) from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833df55",
   "metadata": {},
   "source": [
    "I conducted some experiment with time and memory.Results cache runs with generating 200 tokens on SmolLM2-135M:\n",
    "\n",
    "|Cache|Hardware|Time (s)| Tokens per sec|Note|\n",
    "|-----|-----|----|--|--|\n",
    "|None|L4|6|32||\n",
    "|Dynamic|L4|6|32||\n",
    "|Static|L4|1|153 ||\n",
    "|None|T4|9|21||\n",
    "|Dynamic|T4|7.44|26||\n",
    "|Static|T4|9.35|21| /usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping|\n",
    "|None|A100|7.40|27||\n",
    "|Dynamic|A100|6.30|31||\n",
    "|Static|A100|1.39|143 ||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681fae5",
   "metadata": {},
   "source": [
    "### No cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf094e9",
   "metadata": {},
   "source": [
    "HuggingFace models use dynamic cache by default. By setting `use_cache=False`, we can generate without cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439acf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6a6d31c7c643bd9980a383473245ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7880a347274c82934c354d03ef35fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786ea14ca460456e848a54f23764f416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74908f4144934fc0becfa1e42a230fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986b061169454d45a19c98db4c5b6462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d3fc0996e540acb4ef72e63f237d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf489d01b0449ff91043b3d30c18840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639adf00e54c42cfb87babc20f593a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"use_cache\": false\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig(use_cache=False)\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da47919",
   "metadata": {},
   "source": [
    "To change the behavior of generation, we can either use `model.generation_config` with `GenerationConfig` or pass keywords to `model.generate`. Let's use the `GenerationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6448a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 8.73 sec\n",
      "22 tokens/sec\n",
      "The theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The theory of special relativity states \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n",
    "tks(model, input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f724d",
   "metadata": {},
   "source": [
    "Using the keyword `use_cache=False` to `model.generate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f974a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 8.03 sec\n",
      "24 tokens/sec\n",
      "The theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light\n"
     ]
    }
   ],
   "source": [
    "tks(model, input_ids, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88e96a",
   "metadata": {},
   "source": [
    "### Dynamic cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae67f1a",
   "metadata": {},
   "source": [
    "This is the default cache for HuggingFace models. KV cache is dynamically added each time in the loop. Therefore, this cannot be used with `torch.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf805a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 7.94 sec\n",
      "25 tokens/sec\n",
      "The theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tks(model, input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c523766",
   "metadata": {},
   "source": [
    "### Static cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43314ff",
   "metadata": {},
   "source": [
    "Static cache initializes a bulk of memory for kv cache. It takes more memory than dynamic cache. However, it can be compiled. In my experience, static cache was slower than dynamic cache on CPU and T4, but faster on L4 and A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc182468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"cache_implementation\": \"static\",\n",
       "  \"eos_token_id\": 0\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35f8d4",
   "metadata": {},
   "source": [
    "`model.forward` is compiled automatically with static cache. But it is also possible to compile with differen options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\")\n",
    "# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756f369",
   "metadata": {},
   "source": [
    "After compiling, first run takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7da0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "W0928 19:52:23.353000 791 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 124.21 sec\n",
      "1 tokens/sec\n",
      "The theory of special relativity states 2 things:\n",
      "\n",
      "(1) The speed of light is constant in all inertial frames of reference.\n",
      "\n",
      "(2) The speed of light is the same in all inertial frames of reference.\n",
      "\n",
      "The first statement is called the Lorentz transformation. The second is called the Lorentz contraction.\n",
      "\n",
      "The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n",
      "\n",
      "The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n",
      "\n",
      "The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n",
      "\n",
      "The Lorent\n"
     ]
    }
   ],
   "source": [
    "tks(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba886ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 9.15 sec\n",
      "21 tokens/sec\n",
      "The theory of special relativity states 2 things:\n",
      "\n",
      "(1) The speed of light is constant in all inertial frames of reference.\n",
      "\n",
      "(2) The speed of light is the same in all inertial frames of reference.\n",
      "\n",
      "The first statement is called the Lorentz transformation. The second is called the Lorentz contraction.\n",
      "\n",
      "The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n",
      "\n",
      "The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n",
      "\n",
      "The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n",
      "\n",
      "The Lorent\n"
     ]
    }
   ],
   "source": [
    "tks(model, input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4714b72",
   "metadata": {},
   "source": [
    "## Digging into details\n",
    "\n",
    "> https://huggingface.co/docs/transformers/v4.56.2/llm_tutorial_optimization#32-the-key-value-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd5016",
   "metadata": {},
   "source": [
    "We looked at how different cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71904524",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 7.69 sec\n",
      "26 tokens/sec\n",
      "The theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames.\n",
      "\n",
      "The speed of light is the same in all inertial frames\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The theory of special relativity states \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = model.generate(**input_ids, max_length=num_new_tokens)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "print(f\"{int(num_new_tokens/total_time)} tokens/sec\")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484bdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdpa'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0412c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.07 GB'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab8217",
   "metadata": {},
   "source": [
    "### Looking at the shape of cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dffd6a",
   "metadata": {},
   "source": [
    "#### No cache `input_ids`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbdd0",
   "metadata": {},
   "source": [
    "Before looking into the cache, let's look at the `input_ids` without cache. `input_ids` have shape `[batch_size, sequence_len]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d34e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  504,  3108,   282,  1767, 24581,  2496,   216]], device='cuda:0'),\n",
       " torch.Size([1, 7]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_ids torch.Size([1, 8])\n",
      "shape of input_ids torch.Size([1, 9])\n",
      "shape of input_ids torch.Size([1, 10])\n",
      "shape of input_ids torch.Size([1, 11])\n",
      "shape of input_ids torch.Size([1, 12])\n",
      "Generated text: \n",
      "200 years ago\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    next_logits = model(input_ids, use_cache=False)[\"logits\"][:, -1:]\n",
    "    next_token_id = torch.argmax(next_logits,dim=-1)\n",
    "    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "    print(\"shape of input_ids\", input_ids.shape)\n",
    "\n",
    "print(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387f4b5",
   "metadata": {},
   "source": [
    "Notice how the second dimension (sequence length) grows each iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084cf2c",
   "metadata": {},
   "source": [
    "#### Dynamic cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946783cd",
   "metadata": {},
   "source": [
    "\n",
    "And this is what the `input_ids` look like with dynamic cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3954bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 7, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 8, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 9, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 10, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 11, 64])\n",
      "Generated text: \n",
      "200 years ago\n"
     ]
    }
   ],
   "source": [
    "past_key_values = None # past_key_values is the key-value cache\n",
    "generated_tokens = []\n",
    "next_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "for _ in range(5):\n",
    "    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\n",
    "    next_logits = next_logits[:, -1:]\n",
    "    next_token_id = torch.argmax(next_logits, dim=-1)\n",
    "\n",
    "    print(\"shape of input_ids\", next_token_id.shape)\n",
    "    print(\"shape of key-value cache\", past_key_values[0][0].shape)  # shape of [batch_size, num_key_value_heads, sequence_length, head_dim]\n",
    "    generated_tokens.append(next_token_id.item())\n",
    "\n",
    "print(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91ffd6",
   "metadata": {},
   "source": [
    "Notice how we are only feeding one token at a time (`input_ids` has the shape of `[1, 1]` with `[batch_size, sequence_length]`), instead of the whole previous sequence. But we can see that the kv cache is growing dynamically with each iteration. The dyanmic cache has the shape of `[batch_size, num_key_value_heads, sequence_length, head_dim]`, and the `sequence_length` dimension is growing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f7207",
   "metadata": {},
   "source": [
    "If we look at the code in detail, `model` returns the `next_logits` and `past_key_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507b427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[12.4375,  1.0859,  1.1953,  ..., 10.2500, 11.6875,  8.3125],\n",
       "          [17.2500,  1.7422,  1.8438,  ..., 11.8125, 14.0000,  7.4688],\n",
       "          [ 4.8438, -8.8750, -8.8125,  ...,  0.0732,  3.1406, -1.2812],\n",
       "          ...,\n",
       "          [20.3750,  3.3125,  3.3906,  ..., 13.4375, 15.0625,  8.3125],\n",
       "          [15.0000, -1.4297, -1.3281,  ...,  3.3594,  9.3125,  7.4688],\n",
       "          [18.8750,  3.3906,  3.4219,  ...,  4.3750, 11.4375, 10.3750]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([1, 7, 49152]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values = None # past_key_values is the key-value cache\n",
    "generated_tokens = []\n",
    "next_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\n",
    "next_logits, next_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa95c0",
   "metadata": {},
   "source": [
    "`next_logits` has shape `[batch_size, sequence_length, vocab_size]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4bd67",
   "metadata": {},
   "source": [
    "If we take the softmax from `next_logits` for the last token on the last dimension (`vocab_size`), we get the probability distribution for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8560689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.4850e-05, 1.2221e-11, 1.2619e-11,  ..., 3.2742e-11, 3.8184e-08,\n",
       "         1.3213e-08]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(next_logits[:, -1, :], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a7dc1",
   "metadata": {},
   "source": [
    "However, we are using greedy decoding, and we only care about the most likely next token. Therefore, we just pick the one with the highest value using `torch.argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0bd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 49152])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_logits = next_logits[:, -1:]\n",
    "next_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b065112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[34]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_id = torch.argmax(next_logits, dim=-1)\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcab23",
   "metadata": {},
   "source": [
    "The `model` also returned `past_key_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff409c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82dc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875785a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 7, 64]), torch.Size([1, 3, 7, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0][0].shape, past_key_values[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533cd24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 7, 64]), torch.Size([1, 3, 7, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[20][0].shape, past_key_values[20][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5cb76",
   "metadata": {},
   "source": [
    "In the `model.config`, we have 30 `num_hidden_layers`, and 64 `head_dim`, 3 `num_key_value_heads`. And `past_key_values` has 30 `DynamicLayer`, and each layer contains a tuple of key and value cache ([0] for key and [1] for values). Each of those cache has a shape of `[batch_size, num_key_value_heads, sequence_length, head_dim]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a43fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"dtype\": \"bfloat16\",\n",
       "  \"eos_token_id\": 0,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 576,\n",
       "  \"initializer_range\": 0.041666666666666664,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"is_llama_config\": true,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 9,\n",
       "  \"num_hidden_layers\": 30,\n",
       "  \"num_key_value_heads\": 3,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_interleaved\": false,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 100000,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"4.56.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 49152\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d21e4",
   "metadata": {},
   "source": [
    "If we keep running the model with cache, the `sequence_length` dimension gets concatenated one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d8148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53447cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 8, 64]), torch.Size([1, 3, 8, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0][0].shape, past_key_values[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b8d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_ids torch.Size([1, 1])\n",
      "length of key-value cache 1\n",
      "Generated text: \n",
      "200 years ago\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of input_ids\", next_token_id.shape)\n",
    "print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, head_dim]\n",
    "generated_tokens.append(next_token_id.item())\n",
    "\n",
    "print(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b11998",
   "metadata": {},
   "source": [
    "Another way to use HuggingFace Cache is pass cache into the model directly. Here, we create `DynamicCache` and pass it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0892aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 7, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 8, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 9, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 10, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 11, 64])\n",
      "Generated text: \n",
      "['2', '0', '0', ' years', ' ago']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "past_key_values = DynamicCache(config=model.config)\n",
    "generated_tokens = []\n",
    "next_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "for _ in range(5):\n",
    "    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values).to_tuple()\n",
    "    next_logits = next_logits[:, -1:]\n",
    "    next_token_id = torch.argmax(next_logits, dim=-1)\n",
    "\n",
    "    print(\"shape of input_ids\", next_token_id.shape)\n",
    "    print(\"shape of key-value cache\", past_key_values[0][0].shape)\n",
    "    generated_tokens.append(next_token_id.item())\n",
    "\n",
    "print(f'Generated text: \\n{tokenizer.batch_decode(generated_tokens[-5:], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078abde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 11, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cb43a",
   "metadata": {},
   "source": [
    "### Static cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a203b",
   "metadata": {},
   "source": [
    "If we pass `StaticCache` into the `model`, it uses static cache. As we can see, the cache is allocated with length 1024, and it stays the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 1024, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 1024, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 1024, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 1024, 64])\n",
      "shape of input_ids torch.Size([1, 1])\n",
      "shape of key-value cache torch.Size([1, 3, 1024, 64])\n",
      "Generated text: \n",
      "200 years ago\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "past_key_values = StaticCache(config=model.config, max_cache_len=1024)\n",
    "generated_tokens = []\n",
    "next_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "for _ in range(5):\n",
    "    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values).to_tuple()\n",
    "    next_logits = next_logits[:, -1:]\n",
    "    next_token_id = torch.argmax(next_logits, dim=-1)\n",
    "\n",
    "    print(\"shape of input_ids\", next_token_id.shape)\n",
    "    print(\"shape of key-value cache\", past_key_values[0][0].shape)\n",
    "    generated_tokens.append(next_token_id.item())\n",
    "\n",
    "print(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e04107",
   "metadata": {},
   "source": [
    "Static cache also has the shape as the Dynamic cache with `[layer, key, batch, num_head]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4ed83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2188e+00,  1.6504e-01, -2.0801e-01,  ..., -3.4766e-01,\n",
       "          8.0859e-01, -2.5781e-01],\n",
       "        [-3.0469e-01,  2.8711e-01, -4.0625e-01,  ..., -7.8906e-01,\n",
       "          1.2578e+00, -7.7637e-02],\n",
       "        [ 1.0156e+00, -5.7422e-01, -1.1035e-01,  ...,  1.5000e+00,\n",
       "         -2.5625e+00,  2.5177e-03],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fdefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.2188, -0.3047,  1.0156,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>),\n",
       " torch.Size([1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0][0][0][0][:,0], past_key_values[0][0][0][0][:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5a78a",
   "metadata": {},
   "source": [
    "Looking at the first number from the first 20 sequence_length from the first layer, key, first batch, and the first head. Very mouthful lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96a928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2188, -0.3047,  1.0156,  1.7266,  1.4453, -1.1641,  0.6797, -1.3281,\n",
       "         0.9688,  0.4980, -1.3047,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_key_values[0][0][0][0][:20,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c0a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  504,  3108,   282,  1767, 24581,  2496,   216,    34,    32,    32,\n",
       "           929,  3156]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d870b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "The theory of special relativity states 200 years ago\n"
     ]
    }
   ],
   "source": [
    "print(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, :], skip_special_tokens=True)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faeb602",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a5608",
   "metadata": {},
   "source": [
    "We looked at what KV cache is, why it is helpful, and how to use it with Hugging Face models. It saves keys and values in memory so the model does not have to calculate over and over again for each token.\n",
    "\n",
    "We only looked at decoder models, but Hugging Face has other models as well. Hugging Face also supports other kinds of cache, such as `Cache offloading`, `Quantized cache`, `Encoder-decoder cache`, and `Model-specific cache`. But basic idea is the same.\n",
    "\n",
    "To learn more about thse, check [KV cache strategies](https://huggingface.co/docs/transformers/v4.56.2/kv_cache) from Hugging Face documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaffb16",
   "metadata": {},
   "source": [
    "I didn't go into details when explaining the transformer in LLMs. If you would like to learn more, you can learn more in a youtube video, [Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://www.youtube.com/watch?v=wjZofJX0v4M) by 3Blue1Brown and [illustraed-gpt2](https://jalammar.github.io/illustrated-gpt2/) by Jay Lammar. With cool visualizations, it is very easy to grasp concepts.\n",
    "\n",
    "If you want to learn more in detail with code, I recommend [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy and [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) by Sebastian Raschka. Karpathy makes good videos with code. I also recommend watching [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU) which includes awesome details about how to pretrain gpt2. LLMs-from-scratch is also great for learning LLMs as it starts from gpt2. I really love this repo as it includes more advanced concepts not used in gpt2, such as kv cache, group query attention, rotary embeddings, llama and qwen architectures, and so on. The advanced concepts are not explained in as much in depth as gpt2, but very awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15607415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
