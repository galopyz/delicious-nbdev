{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for colab environment\n",
    "!pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog, we will go through chapter 2 of \"Build a Large Language Model From Scratch\" by Sebastian Raschka. This chapter is about working with text. It goes over preparing text for LLMs, splitting text into word and subword tokens, byte pair encoding, sliding window for dataloader sampling, and converting tokens into embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cat_reading.jpg\" alt=\"Cat reading a book in a library.\" width=\"400\">\n",
    "Image generated from copilot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an outline:\n",
    "\n",
    "- Intorduction\n",
    "- Tokenizing text\n",
    "    - Preprocessing text to split words and special characters.\n",
    "    - Creating vocab\n",
    "    - Encoding and decoding text using the vocab.\n",
    "    - Adding special tokens for encoding/decoding (such as <|unk|>, <|endoftext|>)\n",
    "- Creating dataloader\n",
    "    - Parameters to play with: batch_size, max_length, stride, shuffle, drop_last, num_workers\n",
    "- What is an embedding?\n",
    "    - Talk about embedding briefly in LLMs.\n",
    "    - Why use embedding?\n",
    "    - Examples of using embeddings in other fields.\n",
    "    - Pros and cons of using embedding.\n",
    "    - Creating embedding\n",
    "    - Positional embedding\n",
    "- Byte pair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last blog, we went over an introduction to Large Language Models (LLMs). In this blog, we will go over preparing text data for the training. First, we tokenize the text into numbers. Second, we build the dataloader. Lastly, we turn it into embeddings. As a bonus, we will also go over byte pair encoding in the end. Materials for this blog are from chapter 2 of \"Build a Large Language Model From Scratch\" by Sebastian Raschka with some adjustments. And the images and code are from the book and the author's [github repo](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to tokenize text, but we will keep it simple. We will only split the text into words, punctuations, and special characters and then convert them into numbers. This is called encoding. On the other hand, decoding is converting the numbers back into the text. We will use a dictionary to map each token to a number, and this will become a vocabulary. We will also add some special tokens to the vocabulary, such as `<|unk|>` for unknown tokens. Then we can test our encoding and decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "Byte Pair Encoding\n",
    "\n",
    "In practice, texts are not tokenized by each word as we did here. This is only for demonstration purpose to keep it simple and easy to understand. One drawback from this technique is unknown words. There are so many vocabulary words, and training would cost so much resources. To solve this problem, texts can be tokenized by each alphabet. Problem with this is that individual alphabet does not carry enough information, and it would have to train more.\n",
    "\n",
    "This is where Byte Pair Encoding (BPE) comes in. This is in the middle ground between the two. We will go over BPE at the end of this blog.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preprocessing step, we will split the text into words, punctuations, and special characters. We will use the `re` module to split the text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', ' ', 'cat', ' ', 'sat', ' ', 'on', ' ', 'the', ' ', 'mat!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"The cat sat on the mat!\"\n",
    "res = re.split(r'(\\s)', text)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also split on special characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', ' ', 'cat', ' ', 'sat', ' ', 'on', ' ', 'the', ' ', 'mat', '!', '']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the white space using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'sat', 'on', 'the', 'mat', '!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "[o.strip() for o in res if o.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a bigger text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = '''The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'''\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'cat',\n",
       " 'sat',\n",
       " 'on',\n",
       " 'the',\n",
       " 'mat',\n",
       " '!',\n",
       " 'She',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'red',\n",
       " 'ball',\n",
       " 'rolling',\n",
       " 'by',\n",
       " ',']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "prep = [o.strip() for o in prep if o.strip()]\n",
    "prep[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocab\n",
    "\n",
    "Now that we have preprocessed the text, we can create a vocabulary. The vocabulary is a mapping from each token to a number. We will use a dictionary to store the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(prep))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " ',': 2,\n",
       " '.': 3,\n",
       " '123': 4,\n",
       " '?': 5,\n",
       " '@': 6,\n",
       " 'Main': 7,\n",
       " 'Meow': 8,\n",
       " 'She': 9,\n",
       " 'Street': 10,\n",
       " 'The': 11,\n",
       " 'a': 12,\n",
       " 'and': 13,\n",
       " 'at': 14,\n",
       " 'ball': 15,\n",
       " 'by': 16,\n",
       " 'cat': 17,\n",
       " 'chase': 18,\n",
       " 'it': 19,\n",
       " 'jumped': 20,\n",
       " 'looking': 21,\n",
       " 'mat': 22,\n",
       " 'now': 23,\n",
       " 'on': 24,\n",
       " 'red': 25,\n",
       " 'rolling': 26,\n",
       " 'sat': 27,\n",
       " 'saw': 28,\n",
       " 'she': 29,\n",
       " 'table': 30,\n",
       " 'that': 31,\n",
       " 'the': 32,\n",
       " 'to': 33,\n",
       " 'under': 34,\n",
       " 'up': 35,\n",
       " 'was': 36,\n",
       " 'wondered': 37}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to reverse the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '!',\n",
       " 1: '\"',\n",
       " 2: ',',\n",
       " 3: '.',\n",
       " 4: '123',\n",
       " 5: '?',\n",
       " 6: '@',\n",
       " 7: 'Main',\n",
       " 8: 'Meow',\n",
       " 9: 'She',\n",
       " 10: 'Street',\n",
       " 11: 'The',\n",
       " 12: 'a',\n",
       " 13: 'and',\n",
       " 14: 'at',\n",
       " 15: 'ball',\n",
       " 16: 'by',\n",
       " 17: 'cat',\n",
       " 18: 'chase',\n",
       " 19: 'it',\n",
       " 20: 'jumped',\n",
       " 21: 'looking',\n",
       " 22: 'mat',\n",
       " 23: 'now',\n",
       " 24: 'on',\n",
       " 25: 'red',\n",
       " 26: 'rolling',\n",
       " 27: 'sat',\n",
       " 28: 'saw',\n",
       " 29: 'she',\n",
       " 30: 'table',\n",
       " 31: 'that',\n",
       " 32: 'the',\n",
       " 33: 'to',\n",
       " 34: 'under',\n",
       " 35: 'up',\n",
       " 36: 'was',\n",
       " 37: 'wondered'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_vocab = {i:s for s,i in vocab.items()}\n",
    "rev_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the vocabulary, we can encode and decode text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [vocab[s] for s in prep]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we decode. We first turn it back into a list of tokens, then join them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'sat', 'on', 'the', 'mat', '!', 'She', 'saw', 'a']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strs = [rev_vocab[i] for i in tokens]\n",
    "strs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat ! She saw a red ball rolling by , and jumped up to chase it . \" Meow ? \" she wondered , looking at the ball that was now under the table @ 123 Main Street .'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strs = ' '.join(strs)\n",
    "strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are extra spaces in the decoded text. Let's remove them using `re.sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare it to the original text. It looks pretty good, except `it.\" Meow?\"`. It should be `it. \"Meow?\"`. But it's not a big deal. We can fix it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, here is `SimpleTokenizerV1` class that we can use to encode and decode text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        return [self.str_to_int[s] for s in preprocessed]\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "tokenizer.encode(raw_text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding special tokens\n",
    "\n",
    "There are some special tokens such as `<|unk|>` that we need to add to the vocabulary. We will add them to the vocabulary and update the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens in LLMs serve specific functional purposes and are typically added to the vocabulary with reserved IDs (usually at the beginning). Here are the most common ones:\n",
    "\n",
    "1. `[UNK]` or `<|unk|>` - Used for unknown tokens not in vocabulary\n",
    "2. `[PAD]` - Used to pad sequences to a fixed length in a batch\n",
    "3. `[BOS]` or `<|startoftext|>` - Marks the beginning of a sequence\n",
    "4. `[EOS]` or `<|endoftext|>` - Marks the end of a sequence\n",
    "5. `[SEP]` - Used to separate different segments of text (common in BERT)\n",
    "6. `[CLS]` - Special classification token (used in BERT-like models)\n",
    "7. `[MASK]` - Used for masked language modeling tasks\n",
    "\n",
    "These tokens are crucial because they:\n",
    "- Help models understand sequence boundaries\n",
    "- Enable batch processing of variable-length sequences\n",
    "- Support specific training objectives\n",
    "- Handle out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many special tokens. They help the model understand the sequence boundaries, handle out-of-vocabulary words, and support specific training objectives. However, GPT-2 only used `<|endoftext|>` because it could also be used for padding. This token is also used for separating documents, such as wikipedia articles. It signals the model that the article ended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer fails when it encounters a token that is not in the vocabulary. Let's add a special token `<|unk|>` to the vocabulary and update the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'wassup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwassup yo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wassup'"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(\"wassup yo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " ',': 2,\n",
       " '.': 3,\n",
       " '123': 4,\n",
       " '?': 5,\n",
       " '@': 6,\n",
       " 'Main': 7,\n",
       " 'Meow': 8,\n",
       " 'She': 9,\n",
       " 'Street': 10,\n",
       " 'The': 11,\n",
       " 'a': 12,\n",
       " 'and': 13,\n",
       " 'at': 14,\n",
       " 'ball': 15,\n",
       " 'by': 16,\n",
       " 'cat': 17,\n",
       " 'chase': 18,\n",
       " 'it': 19,\n",
       " 'jumped': 20,\n",
       " 'looking': 21,\n",
       " 'mat': 22,\n",
       " 'now': 23,\n",
       " 'on': 24,\n",
       " 'red': 25,\n",
       " 'rolling': 26,\n",
       " 'sat': 27,\n",
       " 'saw': 28,\n",
       " 'she': 29,\n",
       " 'table': 30,\n",
       " 'that': 31,\n",
       " 'the': 32,\n",
       " 'to': 33,\n",
       " 'under': 34,\n",
       " 'up': 35,\n",
       " 'was': 36,\n",
       " 'wondered': 37,\n",
       " '<|endoftext|>': 38,\n",
       " '<|unk|>': 39}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<|endoftext|>'] = len(vocab)\n",
    "vocab['<|unk|>'] = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can update our encoder to use special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        prep = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        prep = [item.strip() for item in prep if item.strip()]\n",
    "        prep = [item if item in self.str_to_int else \"<|unk|>\" for item in prep]\n",
    "        return [self.str_to_int[s] for s in prep]\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wassup yo, how's it going? <|endoftext|> The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Wassup yo, how's it going?\"\n",
    "text2 = \"The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \\\"Meow?\\\" she wondered\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 39, 2, 39, 39, 39, 19, 39, 5, 38]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> <|unk|>, <|unk|> <|unk|> <|unk|> it <|unk|>? <|endoftext|> The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. We can encode and decode without getting an error from the vocabulary. GPT-2 did not use the `<|unk|>` token. Instead, it used a byte pair encoding method to handle out-of-vocabulary words. We will go over byte pair encoding in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "Evil `<|unk|>` token\n",
    "\n",
    "Why would we use Byte Pair Encoding (BPE) when we could use `<|unk|>` token to encode? We're not getting any error anymore so the problem is solved, right? Actually, there is another problem. When training Large Language Models, if the model sees many unknown tokens in the training data, it doesn't learn very much. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataloader\n",
    "\n",
    "Now that we have tokenized the text, we can create a dataloader. Using dataloader is an easy way to turn the encoded text into batches of data. In each batch, we have a sequence of tokens for x and another for y. The x sequence is the input, and the y sequence is the output. The y sequence is the same as the x sequence, but shifted by one token. This is because we want the model to predict the next token given the previous tokens. The dataloader is also responsible for batching the data and shuffling it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(raw_text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the `context_size` as 4. This means x and y are 4 tokens long. This is only a toy example, but in GPT, context size is way bigger. For example, GPT-2 has a context size of 1024. This means that the model can see up to 1024 tokens in the past. This is why GPT-2 is so good at generating text. It can see the context of the text and generate text that is more coherent. However, longer context size means more memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [11, 17, 27, 24]\n",
      "y:      [17, 27, 24, 32]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = tokens[:context_size]\n",
    "y = tokens[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, this is what the model sees as x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] ----> 17\n",
      "[11, 17] ----> 27\n",
      "[11, 17, 27] ----> 24\n",
      "[11, 17, 27, 24] ----> 32\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    x = tokens[:i]\n",
    "    y = tokens[i]\n",
    "\n",
    "    print(x, \"---->\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more readability, decoded version is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ----> cat\n",
      "The cat ----> sat\n",
      "The cat sat ----> on\n",
      "The cat sat on ----> the\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    x = tokens[:i]\n",
    "    y = tokens[i]\n",
    "\n",
    "    print(tokenizer.decode(x), \"---->\", tokenizer.decode([y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pytorch dataset. As long as we have `__len__` and `__getitem__`, we can use it with pytorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1:\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self): return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "Sliding Window\n",
    "\n",
    "Sliding window is a common algorithm used in computer science. This is best understood as an example. This [stackoverflow answer](https://stackoverflow.com/questions/8269916/what-is-sliding-window-algorithm-examples) has diagrams, which are very easy to understand. It uses Javascript, but it is literally a range of values moving along like sliding a window in an array or a list.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `tiktoken` library to get an encoding from gpt2. We've pretty much looked at everything in this code except stride. `max_length` is the context size. `stride` is the number of tokens to skip when creating the next sequence. For example, if `max_length` is 4 and `stride` is 2, then the next sequence will start 2 tokens after the previous sequence. This is to avoid having the same sequence in the dataset multiple times. This is a common technique in NLP. It is called sliding window. It is also called sliding window attention. By avoiding the same sequence multiple times, we can reduce the size of the dataset. This is important because we want to use as much data as possible to train the model. It also reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = GPTDatasetV1(raw_text, tiktoken.get_encoding(\"gpt2\"), max_length=10, stride=5)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader returns x and y. Let's take a look at what `stride` does in a dataloader. Here is a simple example of `batch_size` of 1 and stride 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 464, 3797, 3332,  319]]), tensor([[3797, 3332,  319,  262]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at `batch_size` of 2. Both x and y are 2 sequences long. The first sequence is the same as the previous example. The second sequence is the next sequence in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 464, 3797, 3332,  319],\n",
       "         [3797, 3332,  319,  262]]),\n",
       " tensor([[3797, 3332,  319,  262],\n",
       "         [3332,  319,  262, 2603]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=1, shuffle=False)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we increase the `stride` to 2, the second sequence is 2 tokens after the first sequence. This is because we skipped 2 tokens when creating the second sequence. Instead of starting the second x with 3797, we start with 3332."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 464, 3797, 3332,  319],\n",
       "         [3332,  319,  262, 2603]]),\n",
       " tensor([[3797, 3332,  319,  262],\n",
       "         [ 319,  262, 2603,    0]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=2, shuffle=False)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the same `stride` and `max_length`, we can see that the second sequence is the same as the first sequence. This is because we skipped 4 tokens when creating the second sequence. Now, there is no overlap between the x sequences and y sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 464, 3797, 3332,  319],\n",
       "         [ 262, 2603,    0, 1375]]),\n",
       " tensor([[3797, 3332,  319,  262],\n",
       "         [2603,    0, 1375, 2497]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=False)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we also have `drop_last` parameter. This is to drop the last batch if it is smaller than `batch_size`. This is important during training because it can cause loss spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "Smooth Training\n",
    "\n",
    "When training, it is important to keep the loss go down smoothly. If the loss spikes up, it may not come down, and the model has to be trained again from the start. Using `drop_last` parameter when training helps. There are also other ways to keep it from spiking, such as using bigger batch sizes and using a high quality data. Data could have particularly noisy and unclean gibberish. If these are concentrated in one batch, loss goes up to spike, and the training is over. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Embeddings?\n",
    "\n",
    "An embedding is a way to represent words or phrases as vectors of numbers. These vectors capture the semantic meaning of the words, allowing us to perform mathematical operations on them. For example, we can calculate the distance between two words to see how similar they are. Embeddings are used in many NLP tasks, such as machine translation, text classification, and question answering. They are also used in recommendation systems, image recognition, and other machine learning tasks. Embeddings are a powerful tool for understanding and processing text data.\n",
    "\n",
    "For example, in this space:\n",
    "- \"Cat\" and \"dog\" might be close together because they're both pets\n",
    "- \"Run\" and \"sprint\" would be nearby as they're similar actions\n",
    "- \"Hot\" might be positioned opposite to \"cold\"\n",
    "- \"King\", \"queen\", \"prince\", and \"princess\" would form a cluster showing both their royal relationships and gender differences\n",
    "\n",
    "In modern LLMs like GPT-2, each token (word or subword) is represented by a vector of 768 numbers, while larger models like GPT-3 use even bigger vectors (2048 or more dimensions). These numbers aren't randomly assigned - they're learned during training to capture meaningful relationships between words.\n",
    "\n",
    "### Why Do We Need Embeddings?\n",
    "Traditional one-hot encoding represents each word as a vector of zeros with a single '1', where the vector length equals vocabulary size. For a 50,000-word vocabulary, each word requires a 50,000-dimensional vector! This approach has several major problems:\n",
    "\n",
    "1. Inefficiency:\n",
    "   - Massive storage requirements\n",
    "   - Sparse representations (mostly zeros)\n",
    "   - Computationally expensive to process\n",
    "\n",
    "2. No Semantic Information:\n",
    "   - \"cat\" and \"kitten\" are as different as \"cat\" and \"motorcycle\"\n",
    "   - No way to measure word similarity\n",
    "   - No capture of analogies or relationships\n",
    "\n",
    "Embeddings solve these problems by:\n",
    "1. Dense Representation:\n",
    "   - Using much smaller vectors (768 vs 50,000 dimensions)\n",
    "   - Every dimension carries meaningful information\n",
    "   - Efficient storage and computation\n",
    "\n",
    "2. Semantic Relationships:\n",
    "   - Similar words have similar vectors\n",
    "   - Enable amazing arithmetic: king - man + woman â‰ˆ queen\n",
    "   - Capture multiple types of relationships:\n",
    "     * Semantic (car/automobile)\n",
    "     * Syntactic (run/running)\n",
    "     * Conceptual (France/Paris :: Japan/Tokyo)\n",
    "\n",
    "3. Learning and Adaptation:\n",
    "   - Embeddings improve during model training\n",
    "   - Can capture domain-specific meanings\n",
    "   - Transfer learning from one task to another\n",
    "\n",
    "4. Mathematical Operations:\n",
    "   - Calculate similarity using cosine distance\n",
    "   - Find analogies through vector arithmetic\n",
    "   - Cluster related concepts together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Beyond Language Models\n",
    "\n",
    "The power of embeddings extends far beyond just processing text. The same fundamental concept - representing complex objects as dense vectors in high-dimensional space - has revolutionized many fields:\n",
    "\n",
    "1. Recommendation Systems:\n",
    "   - Netflix maps both users and movies into the same embedding space\n",
    "   - User embeddings capture viewing preferences and habits\n",
    "   - Movie embeddings represent genre, style, mood, and other features\n",
    "   - Similarity between vectors predicts what you might like to watch\n",
    "   - Even time-of-day and viewing context can be embedded\n",
    "\n",
    "2. Computer Vision:\n",
    "   - Images are embedded into high-dimensional spaces\n",
    "   - Similar images cluster together automatically\n",
    "   - Enables powerful features like:\n",
    "     * Face recognition\n",
    "     * Object detection\n",
    "     * Image similarity search\n",
    "     * Style transfer\n",
    "   - Transfer learning from pre-trained vision models\n",
    "\n",
    "3. Bioinformatics:\n",
    "   - Protein sequences represented as embeddings\n",
    "   - Captures complex biochemical properties\n",
    "   - Predicts protein folding and interactions\n",
    "   - Helps in drug discovery\n",
    "   - Enables rapid searching of similar compounds\n",
    "\n",
    "4. Audio Processing:\n",
    "   - Spotify embeds songs based on:\n",
    "     * Musical features (tempo, key, instruments)\n",
    "     * Listening patterns\n",
    "     * Cultural context\n",
    "     * User behavior\n",
    "   - Voice recognition systems use embeddings\n",
    "   - Sound classification and similarity detection\n",
    "\n",
    "5. Graph Networks:\n",
    "   - Social networks embed users and relationships\n",
    "   - Knowledge graphs embed concepts and connections\n",
    "   - Traffic networks embed locations and routes\n",
    "   - Fraud detection systems embed transaction patterns\n",
    "\n",
    "### Creating Embeddings in Practice\n",
    "\n",
    "Creating embeddings involves several key components and considerations:\n",
    "\n",
    "1. The Embedding Layer:\n",
    "```python\n",
    "embedding_dim = 768  # typical size\n",
    "vocab_size = 50257  # GPT-2 vocabulary size\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "```\n",
    "\n",
    "2. Initialization Strategies:\n",
    "   - Random initialization\n",
    "   - Pre-trained embeddings (Word2Vec, GloVe)\n",
    "   - Xavier/Glorot initialization\n",
    "   - Custom initialization based on domain knowledge\n",
    "\n",
    "3. Training Approaches:\n",
    "   - End-to-end with model\n",
    "   - Separate pre-training\n",
    "   - Fine-tuning existing embeddings\n",
    "   - Frozen pre-trained embeddings\n",
    "\n",
    "4. Advanced Techniques:\n",
    "   - Subword tokenization\n",
    "   - Contextual embeddings\n",
    "   - Multi-modal embeddings\n",
    "   - Hierarchical embeddings\n",
    "\n",
    "### The Position Problem\n",
    "\n",
    "Position information is crucial in language understanding, but transformers are inherently position-agnostic. Here's how we solve this:\n",
    "\n",
    "1. Learned Positional Embeddings:\n",
    "   - Model learns position representations\n",
    "   - Can capture common patterns\n",
    "   - More flexible but needs training\n",
    "   - Used in models like GPT\n",
    "\n",
    "2. Sinusoidal Embeddings:\n",
    "   - Fixed mathematical patterns\n",
    "   - Uses sine and cosine functions\n",
    "   - Different frequencies for different dimensions\n",
    "   - Original transformer approach\n",
    "   - No training required\n",
    "   - Can extrapolate to longer sequences\n",
    "\n",
    "3. Relative Positional Embeddings:\n",
    "   - Encode relative distances between tokens\n",
    "   - Better for certain tasks\n",
    "   - More computationally intensive\n",
    "   - Used in modern architectures like T5\n",
    "\n",
    "4. Hybrid Approaches:\n",
    "   - Combining different types\n",
    "   - Task-specific adaptations\n",
    "   - Novel architectural innovations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-offs and Challenges\n",
    "The use of embeddings, while powerful, comes with important considerations and challenges:\n",
    "\n",
    "1. Training Data Requirements:\n",
    "   - Need massive amounts of quality data\n",
    "   - Data must be representative and balanced\n",
    "   - Domain-specific data often required\n",
    "   - Poor quality data leads to poor embeddings\n",
    "   - Data cleaning and preprocessing crucial\n",
    "\n",
    "2. Computational Costs:\n",
    "   - Training embeddings is resource-intensive\n",
    "   - Large memory requirements\n",
    "   - GPU/TPU hardware often necessary\n",
    "   - Inference time can be significant\n",
    "   - Storage costs for large embedding tables\n",
    "\n",
    "3. Bias and Fairness:\n",
    "   - Embeddings inherit biases from training data\n",
    "   - Can amplify societal prejudices\n",
    "   - Gender, racial, and cultural biases common\n",
    "   - Debiasing techniques available but imperfect\n",
    "   - Ethical considerations in deployment\n",
    "\n",
    "4. Technical Challenges:\n",
    "   - Choosing optimal embedding dimension\n",
    "   - Handling out-of-vocabulary words\n",
    "   - Dealing with rare words/tokens\n",
    "   - Managing embedding table size\n",
    "   - Updating embeddings efficiently\n",
    "\n",
    "5. Performance Trade-offs:\n",
    "   - Accuracy vs. computation speed\n",
    "   - Memory usage vs. embedding size\n",
    "   - Training time vs. model quality\n",
    "   - Generalization vs. specialization\n",
    "   - Real-time requirements vs. model complexity\n",
    "\n",
    "## The Future of Embeddings\n",
    "\n",
    "1. Architectural Innovations:\n",
    "   - More efficient embedding architectures\n",
    "   - Sparse embedding techniques\n",
    "   - Dynamic embedding sizes\n",
    "   - Adaptive embedding strategies\n",
    "   - Novel initialization methods\n",
    "\n",
    "2. Multi-modal Developments:\n",
    "   - Cross-modal embeddings\n",
    "   - Universal embeddings across domains\n",
    "   - Joint learning of different modalities\n",
    "   - Transfer learning improvements\n",
    "   - Domain adaptation techniques\n",
    "\n",
    "3. Efficiency Improvements:\n",
    "   - Compression techniques\n",
    "   - Quantization methods\n",
    "   - Pruning strategies\n",
    "   - Distributed embedding systems\n",
    "   - Hardware-specific optimizations\n",
    "\n",
    "4. Ethical Considerations:\n",
    "   - Better debiasing techniques\n",
    "   - Fairness-aware embeddings\n",
    "   - Interpretable embeddings\n",
    "   - Privacy-preserving methods\n",
    "   - Robust evaluation metrics\n",
    "\n",
    "5. Emerging Applications:\n",
    "   - Quantum computing embeddings\n",
    "   - Neuromorphic hardware adaptation\n",
    "   - Edge device implementations\n",
    "   - Real-time embedding updates\n",
    "   - Federated learning approaches\n",
    "\n",
    "6. Research Directions:\n",
    "   - Theoretical understanding improvements\n",
    "   - Formal mathematical frameworks\n",
    "   - Stability and robustness studies\n",
    "   - Scaling laws investigation\n",
    "   - Novel training objectives\n",
    "\n",
    "The field of embeddings continues to be a crucial area of research and development in machine learning, with new breakthroughs and applications emerging regularly. As we better understand their properties and capabilities, we can expect to see even more innovative uses across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about embeddings, please refer to the following resources:\n",
    "- Google's tutorial on word embeddings, document search, and applications: https://github.com/google/generative-ai-docs/tree/main/site/en/gemini-api/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "\n",
    "Word Embeddings Size\n",
    "\n",
    "Word embeddings size with multiples of 64 have hardware optimization.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create embeddings with pytorch. We will use a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.tensor([2,1,0])\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 3\n",
    "output_dim = 4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "emb = torch.nn.Embedding(vocab_size, output_dim)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n",
       "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
       "        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding layer has weights defined by the vocab size and the output dimension. The weights are normally distributed with mean of 0 and standard deviation of 1. These weights are learnable parameters. With these embedding layer, we can convert the token ids into embeddings by simply calling the embedding layer with the token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3367, 0.1288, 0.2345, 0.2303]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First embedding\n",
    "emb(torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1229, -0.1863,  2.2082, -0.6380]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second embedding\n",
    "emb(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simply select using an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3367, 0.1288, 0.2345, 0.2303], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.weight[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can select multiple embeddings at once in whatever order we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1229, -0.1863,  2.2082, -0.6380],\n",
       "        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n",
       "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
       "        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "An older way to do this is using one hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually do one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n",
       "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
       "        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.nn.Parameter(emb.weight)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot(token_ids)\n",
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n",
       "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
       "        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot.float()@params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3367, -1.1229,  0.4617],\n",
       "        [ 0.1288, -0.1863,  0.2674],\n",
       "        [ 0.2345,  2.2082,  0.5349],\n",
       "        [ 0.2303, -0.6380,  0.8094]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = torch.nn.Linear(vocab_size, output_dim, bias=False)\n",
    "linear.weight = torch.nn.Parameter(emb.weight.T)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n",
       "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
       "        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(onehot.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.nn.Embedding` is the most efficient way to convert token ids into embeddings. It's faster and more memory-efficient than using one-hot encoding or a linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "\n",
    "Positional embeddings are added to the token embeddings to encode the position of the token in the sequence. This is because transformers do not have any inherent sense of order. There are two main types of positional embeddings: relative positional embeddings and absolute positional embeddings.\n",
    "\n",
    "- Relative positional embeddings\n",
    "\n",
    "Relative positional embeddings encode the distance between tokens. This is useful because the model can learn to pay attention to tokens that are close to each other. However, relative positional embeddings are not as efficient as absolute positional embeddings. \n",
    "\n",
    "- Absolute positional embeddings\n",
    "\n",
    "Absolute positional embeddings encode the absolute position of the token in the sequence. This is useful because the model can learn to pay attention to tokens that are in certain positions. However, absolute positional embeddings are not as flexible as relative positional embeddings. It can be difficult to change the size of the context length of the model because it was fixed during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, these embeddings are added to the token embeddings. \n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create absolute positional embeddings for simplicity. GPT-2 also used this. We have `token_ids` and `emb` from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get token embeddings again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n",
       "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
       "        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_emb = emb(token_ids)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional embedding is another embedding layer with the same output dimension as the token embedding layer. The input dimension is the context length. The context length is the maximum length of the sequence that the model can handle. In this case, we will use a context length of 3. And output size is 4. Since vocab size and output size are the same, we can use the same embedding layer for both token and positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb2 = torch.nn.Embedding(3, 4)\n",
    "emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7658, -0.7506,  1.3525,  0.6863],\n",
       "        [-0.3278,  0.7950,  0.2815,  0.0562],\n",
       "        [ 0.5227, -0.2384, -0.0499,  0.5263]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb = emb2(torch.arange(3))\n",
    "pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can get the input embedding for the model by adding the token embeddings and the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3042, -0.4833,  1.8875,  1.4957],\n",
       "        [-1.4506,  0.6086,  2.4897, -0.5818],\n",
       "        [ 0.8594, -0.1095,  0.1846,  0.7567]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_emb = token_emb + pos_emb\n",
    "inp_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about positional embeddings, please refer to the following resources:\n",
    "- A blog on Rotary Position Encoding (ROPE) by Akash Nain: https://aakashkumarnain.github.io/posts/ml_dl_concepts/rope\n",
    "- Reformer paper: https://arxiv.org/pdf/2104.09864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte pair encoding (BPE)\n",
    "\n",
    "Byte pair encoding (BPE) is a data compression technique that is used to create a vocabulary of subword units. It was a bit confusing for me to understand this because I didn't know about bytes, hexdecimal, ASCII, and UTF-8. We can just think of byte as a tiny thing that makes up a character. The algorithm is very simple. It works by iteratively merging the most frequent pair of bytes in the text. This process is repeated until the desired vocabulary size is reached. The resulting vocabulary consists of the most frequent subword units in the text. \n",
    "\n",
    "The book does not cover details of BPE, but [bpe-from-scratch](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/05_bpe-from-scratch) is included in the github. This version focuses on education purposes and skips some steps, such as converting the text into bytes. To learn more about bpe, I recommend [`minbpe`](https://github.com/karpathy/minbpe) by Karpathy. The code has many comments and is easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "What are bytes, hexadecimal, ASCII, and UTF-8? And what do they have to do with BPE?\n",
    "\n",
    "It is not necessary to know those concepts to understand how BPE works in a big picture. However, I had a lot of fun learning about these. And it gives a bit more in depth understanding of BPE and computers. \n",
    "\n",
    "Briefly, a byte is eight bits, and each bit is a number consists of 0 or 1. For instance, \"00000000\" and \"10101100\" are bytes. There are `2**8` or 256 ways of creatinga byte. Instead of writing eight characters long for each byte, we can use hexadecimal to write two characters for each byte. In simple terms, ASCII is an old way to convert or convert back a byte into a character and only has characters on english keyboard, such as english alphabet, numbers, +, -, etc. UTF-8 is modern way that includes characters from other languages and emojis. Using hexadecimal is useful because UTF-8 uses multiple bytes. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How BPE works\n",
    "\n",
    "Briefly, this is how to train BPE:\n",
    "\n",
    "1. Vocabulary is initialized with first 256 ASCII characters.\n",
    "2. Text is converted into bytes.\n",
    "3. Until vocabulary size is reached:\n",
    "    - Count the frequency of each pair of bytes in the text.\n",
    "    - Merge the most frequent pair of bytes into a single byte.\n",
    "    - Add the new byte to the vocabulary.\n",
    "\n",
    "That's it. BPE is a simple and effective way to create a vocabulary of subword units. It is used in many NLP models, including GPT-2, GPT-3, and BERT. I was planning on explaining BPE in more detail, but I think it is better to leave it as an exercise for the reader. Maybe I will write a blog on it in the future with some information about bytes, hexadecimal digits, ASCII, UTF-8, and such. Of course it is not necessary to understand BPE, but they are related and are fun to learn about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
