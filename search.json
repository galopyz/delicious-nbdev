[
  {
    "objectID": "blog/colab/bllm2_process_text.html",
    "href": "blog/colab/bllm2_process_text.html",
    "title": "delicious-nbdev",
    "section": "",
    "text": "# Required for colab environment\n!pip install -q tiktoken\nIn this blog, we will go through chapter 2 of “Build a Large Language Model From Scratch” by Sebastian Raschka. This chapter is about working with text. It goes over preparing text for LLMs, splitting text into word and subword tokens, byte pair encoding, sliding window for dataloader sampling, and converting tokens into embeddings.\nImage generated from copilot.\nHere is an outline:"
  },
  {
    "objectID": "blog/colab/bllm2_process_text.html#introduction",
    "href": "blog/colab/bllm2_process_text.html#introduction",
    "title": "delicious-nbdev",
    "section": "Introduction",
    "text": "Introduction\nIn the last blog, we went over an introduction to Large Language Models (LLMs). In this blog, we will go over preparing text data for the training. First, we tokenize the text into numbers. Second, we build the dataloader. Lastly, we turn it into embeddings. As a bonus, we will also go over byte pair encoding in the end. Materials for this blog are from chapter 2 of “Build a Large Language Model From Scratch” by Sebastian Raschka with some adjustments. And the images and code are from the book and the author’s github repo."
  },
  {
    "objectID": "blog/colab/bllm2_process_text.html#tokenizing-text",
    "href": "blog/colab/bllm2_process_text.html#tokenizing-text",
    "title": "delicious-nbdev",
    "section": "Tokenizing Text",
    "text": "Tokenizing Text\nThere are many ways to tokenize text, but we will keep it simple. We will only split the text into words, punctuations, and special characters and then convert them into numbers. This is called encoding. On the other hand, decoding is converting the numbers back into the text. We will use a dictionary to map each token to a number, and this will become a vocabulary. We will also add some special tokens to the vocabulary, such as &lt;|unk|&gt; for unknown tokens. Then we can test our encoding and decoding.\n\n\n\n\n\n\nNote\n\n\n\nByte Pair Encoding\nIn practice, texts are not tokenized by each word as we did here. This is only for demonstration purpose to keep it simple and easy to understand. One drawback from this technique is unknown words. There are so many vocabulary words, and training would cost so much resources. To solve this problem, texts can be tokenized by each alphabet. Problem with this is that individual alphabet does not carry enough information, and it would have to train more.\nThis is where Byte Pair Encoding (BPE) comes in. This is in the middle ground between the two. We will go over BPE at the end of this blog.\n\n\n\nPreprocessing text\nIn preprocessing step, we will split the text into words, punctuations, and special characters. We will use the re module to split the text into tokens.\n\nimport re\ntext = \"The cat sat on the mat!\"\nres = re.split(r'(\\s)', text)\nres\n\n['The', ' ', 'cat', ' ', 'sat', ' ', 'on', ' ', 'the', ' ', 'mat!']\n\n\nLet’s also split on special characters and punctuation.\n\nres = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nres\n\n['The', ' ', 'cat', ' ', 'sat', ' ', 'on', ' ', 'the', ' ', 'mat', '!', '']\n\n\nWe can remove the white space using list comprehension.\n\nres = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n[o.strip() for o in res if o.strip()]\n\n['The', 'cat', 'sat', 'on', 'the', 'mat', '!']\n\n\nLet’s use a bigger text.\n\nraw_text = '''The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'''\nraw_text\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\n\nprep = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\nprep = [o.strip() for o in prep if o.strip()]\nprep[:15]\n\n['The',\n 'cat',\n 'sat',\n 'on',\n 'the',\n 'mat',\n '!',\n 'She',\n 'saw',\n 'a',\n 'red',\n 'ball',\n 'rolling',\n 'by',\n ',']\n\n\n\n\nCreating vocab\nNow that we have preprocessed the text, we can create a vocabulary. The vocabulary is a mapping from each token to a number. We will use a dictionary to store the mapping.\n\nall_words = sorted(set(prep))\nvocab_size = len(all_words)\nvocab_size\n\n38\n\n\n\nvocab = {token:integer for integer,token in enumerate(all_words)}\nvocab\n\n{'!': 0,\n '\"': 1,\n ',': 2,\n '.': 3,\n '123': 4,\n '?': 5,\n '@': 6,\n 'Main': 7,\n 'Meow': 8,\n 'She': 9,\n 'Street': 10,\n 'The': 11,\n 'a': 12,\n 'and': 13,\n 'at': 14,\n 'ball': 15,\n 'by': 16,\n 'cat': 17,\n 'chase': 18,\n 'it': 19,\n 'jumped': 20,\n 'looking': 21,\n 'mat': 22,\n 'now': 23,\n 'on': 24,\n 'red': 25,\n 'rolling': 26,\n 'sat': 27,\n 'saw': 28,\n 'she': 29,\n 'table': 30,\n 'that': 31,\n 'the': 32,\n 'to': 33,\n 'under': 34,\n 'up': 35,\n 'was': 36,\n 'wondered': 37}\n\n\nWe also need a way to reverse the mapping.\n\nrev_vocab = {i:s for s,i in vocab.items()}\nrev_vocab\n\n{0: '!',\n 1: '\"',\n 2: ',',\n 3: '.',\n 4: '123',\n 5: '?',\n 6: '@',\n 7: 'Main',\n 8: 'Meow',\n 9: 'She',\n 10: 'Street',\n 11: 'The',\n 12: 'a',\n 13: 'and',\n 14: 'at',\n 15: 'ball',\n 16: 'by',\n 17: 'cat',\n 18: 'chase',\n 19: 'it',\n 20: 'jumped',\n 21: 'looking',\n 22: 'mat',\n 23: 'now',\n 24: 'on',\n 25: 'red',\n 26: 'rolling',\n 27: 'sat',\n 28: 'saw',\n 29: 'she',\n 30: 'table',\n 31: 'that',\n 32: 'the',\n 33: 'to',\n 34: 'under',\n 35: 'up',\n 36: 'was',\n 37: 'wondered'}\n\n\n\n\nEncoding and Decoding\nNow that we have the vocabulary, we can encode and decode text.\n\ntokens = [vocab[s] for s in prep]\ntokens[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\nAnd here is how we decode. We first turn it back into a list of tokens, then join them together.\n\nstrs = [rev_vocab[i] for i in tokens]\nstrs[:10]\n\n['The', 'cat', 'sat', 'on', 'the', 'mat', '!', 'She', 'saw', 'a']\n\n\n\nstrs = ' '.join(strs)\nstrs\n\n'The cat sat on the mat ! She saw a red ball rolling by , and jumped up to chase it . \" Meow ? \" she wondered , looking at the ball that was now under the table @ 123 Main Street .'\n\n\nThere are extra spaces in the decoded text. Let’s remove them using re.sub.\n\nre.sub(r'\\s+([,.?!\"()\\'])', r'\\1', strs)\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\n\nraw_text\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\nLet’s compare it to the original text. It looks pretty good, except it.\" Meow?\". It should be it. \"Meow?\". But it’s not a big deal. We can fix it later.\nPutting it all together, here is SimpleTokenizerV1 class that we can use to encode and decode text.\n\nclass SimpleTokenizerV1:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n    \n    def encode(self, text):\n        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        return [self.str_to_int[s] for s in preprocessed]\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\n\n\ntokenizer = SimpleTokenizerV1(vocab)\ntokenizer.encode(raw_text)[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\n\ntokens[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\n\ntokenizer.decode(tokenizer.encode(raw_text))\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\n\n\nAdding special tokens\nThere are some special tokens such as &lt;|unk|&gt; that we need to add to the vocabulary. We will add them to the vocabulary and update the tokenizer.\nSpecial tokens in LLMs serve specific functional purposes and are typically added to the vocabulary with reserved IDs (usually at the beginning). Here are the most common ones:\n\n[UNK] or &lt;|unk|&gt; - Used for unknown tokens not in vocabulary\n[PAD] - Used to pad sequences to a fixed length in a batch\n[BOS] or &lt;|startoftext|&gt; - Marks the beginning of a sequence\n[EOS] or &lt;|endoftext|&gt; - Marks the end of a sequence\n[SEP] - Used to separate different segments of text (common in BERT)\n[CLS] - Special classification token (used in BERT-like models)\n[MASK] - Used for masked language modeling tasks\n\nThese tokens are crucial because they: - Help models understand sequence boundaries - Enable batch processing of variable-length sequences - Support specific training objectives - Handle out-of-vocabulary words\nThere are many special tokens. They help the model understand the sequence boundaries, handle out-of-vocabulary words, and support specific training objectives. However, GPT-2 only used &lt;|endoftext|&gt; because it could also be used for padding. This token is also used for separating documents, such as wikipedia articles. It signals the model that the article ended.\nOur tokenizer fails when it encounters a token that is not in the vocabulary. Let’s add a special token &lt;|unk|&gt; to the vocabulary and update the tokenizer.\n\ntokenizer.encode(\"wassup yo\")\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 tokenizer.encode(\"wassup yo\")\n\nCell In[14], line 9, in SimpleTokenizerV1.encode(self, text)\n      7 preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n      8 preprocessed = [item.strip() for item in preprocessed if item.strip()]\n----&gt; 9 return [self.str_to_int[s] for s in preprocessed]\n\nCell In[14], line 9, in &lt;listcomp&gt;(.0)\n      7 preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n      8 preprocessed = [item.strip() for item in preprocessed if item.strip()]\n----&gt; 9 return [self.str_to_int[s] for s in preprocessed]\n\nKeyError: 'wassup'\n\n\n\n\nlen(vocab)\n\n38\n\n\n\nvocab['&lt;|endoftext|&gt;'] = len(vocab)\nvocab['&lt;|unk|&gt;'] = len(vocab)\nvocab\n\n{'!': 0,\n '\"': 1,\n ',': 2,\n '.': 3,\n '123': 4,\n '?': 5,\n '@': 6,\n 'Main': 7,\n 'Meow': 8,\n 'She': 9,\n 'Street': 10,\n 'The': 11,\n 'a': 12,\n 'and': 13,\n 'at': 14,\n 'ball': 15,\n 'by': 16,\n 'cat': 17,\n 'chase': 18,\n 'it': 19,\n 'jumped': 20,\n 'looking': 21,\n 'mat': 22,\n 'now': 23,\n 'on': 24,\n 'red': 25,\n 'rolling': 26,\n 'sat': 27,\n 'saw': 28,\n 'she': 29,\n 'table': 30,\n 'that': 31,\n 'the': 32,\n 'to': 33,\n 'under': 34,\n 'up': 35,\n 'was': 36,\n 'wondered': 37,\n '&lt;|endoftext|&gt;': 38,\n '&lt;|unk|&gt;': 39}\n\n\nNow, we can update our encoder to use special tokens.\n\nclass SimpleTokenizerV2:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n    \n    def encode(self, text):\n        prep = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n        prep = [item.strip() for item in prep if item.strip()]\n        prep = [item if item in self.str_to_int else \"&lt;|unk|&gt;\" for item in prep]\n        return [self.str_to_int[s] for s in prep]\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\n\n\ntokenizer = SimpleTokenizerV2(vocab)\n\ntext1 = \"Wassup yo, how's it going?\"\ntext2 = \"The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \\\"Meow?\\\" she wondered\"\ntext = \" &lt;|endoftext|&gt; \".join((text1, text2))\nprint(text)\n\nWassup yo, how's it going? &lt;|endoftext|&gt; The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered\n\n\n\ntokenizer.encode(text)[:10]\n\n[39, 39, 2, 39, 39, 39, 19, 39, 5, 38]\n\n\n\ntokenizer.decode(tokenizer.encode(text))\n\n'&lt;|unk|&gt; &lt;|unk|&gt;, &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; it &lt;|unk|&gt;? &lt;|endoftext|&gt; The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered'\n\n\nGreat. We can encode and decode without getting an error from the vocabulary. GPT-2 did not use the &lt;|unk|&gt; token. Instead, it used a byte pair encoding method to handle out-of-vocabulary words. We will go over byte pair encoding in the end.\n\n\n\n\n\n\nNote\n\n\n\nEvil &lt;|unk|&gt; token\nWhy would we use Byte Pair Encoding (BPE) when we could use &lt;|unk|&gt; token to encode? We’re not getting any error anymore so the problem is solved, right? Actually, there is another problem. When training Large Language Models, if the model sees many unknown tokens in the training data, it doesn’t learn very much."
  },
  {
    "objectID": "blog/colab/bllm2_process_text.html#creating-dataloader",
    "href": "blog/colab/bllm2_process_text.html#creating-dataloader",
    "title": "delicious-nbdev",
    "section": "Creating dataloader",
    "text": "Creating dataloader\nNow that we have tokenized the text, we can create a dataloader. Using dataloader is an easy way to turn the encoded text into batches of data. In each batch, we have a sequence of tokens for x and another for y. The x sequence is the input, and the y sequence is the output. The y sequence is the same as the x sequence, but shifted by one token. This is because we want the model to predict the next token given the previous tokens. The dataloader is also responsible for batching the data and shuffling it.\n\n\ntokens = tokenizer.encode(raw_text)\ntokens[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\nWe set the context_size as 4. This means x and y are 4 tokens long. This is only a toy example, but in GPT, context size is way bigger. For example, GPT-2 has a context size of 1024. This means that the model can see up to 1024 tokens in the past. This is why GPT-2 is so good at generating text. It can see the context of the text and generate text that is more coherent. However, longer context size means more memory usage.\n\ncontext_size = 4\n\nx = tokens[:context_size]\ny = tokens[1:context_size+1]\n\nprint(f\"x: {x}\")\nprint(f\"y:      {y}\")\n\nx: [11, 17, 27, 24]\ny:      [17, 27, 24, 32]\n\n\nWhen training, this is what the model sees as x and y:\n\nfor i in range(1, context_size+1):\n    x = tokens[:i]\n    y = tokens[i]\n\n    print(x, \"----&gt;\", y)\n\n[11] ----&gt; 17\n[11, 17] ----&gt; 27\n[11, 17, 27] ----&gt; 24\n[11, 17, 27, 24] ----&gt; 32\n\n\nFor more readability, decoded version is here:\n\nfor i in range(1, context_size+1):\n    x = tokens[:i]\n    y = tokens[i]\n\n    print(tokenizer.decode(x), \"----&gt;\", tokenizer.decode([y]))\n\nThe ----&gt; cat\nThe cat ----&gt; sat\nThe cat sat ----&gt; on\nThe cat sat on ----&gt; the\n\n\nLet’s create a pytorch dataset. As long as we have __len__ and __getitem__, we can use it with pytorch dataloader.\n\nclass GPTDatasetV1:\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        token_ids = tokenizer.encode(txt, allowed_special={\"&lt;|endoftext|&gt;\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self): return len(self.input_ids)\n\n    def __getitem__(self, idx): return self.input_ids[idx], self.target_ids[idx]\n\n\n\n\n\n\n\nNote\n\n\n\nSliding Window\nSliding window is a common algorithm used in computer science. This is best understood as an example. This stackoverflow answer has diagrams, which are very easy to understand. It uses Javascript, but it is literally a range of values moving along like sliding a window in an array or a list.\n\n\nWe will use tiktoken library to get an encoding from gpt2. We’ve pretty much looked at everything in this code except stride. max_length is the context size. stride is the number of tokens to skip when creating the next sequence. For example, if max_length is 4 and stride is 2, then the next sequence will start 2 tokens after the previous sequence. This is to avoid having the same sequence in the dataset multiple times. This is a common technique in NLP. It is called sliding window. It is also called sliding window attention. By avoiding the same sequence multiple times, we can reduce the size of the dataset. This is important because we want to use as much data as possible to train the model. It also reduces overfitting.\n\nimport tiktoken\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nds = GPTDatasetV1(raw_text, tiktoken.get_encoding(\"gpt2\"), max_length=10, stride=5)\nlen(ds)\n\n7\n\n\n\ndef create_dataloader_v1(txt, batch_size=4, max_length=256, \n                         stride=128, shuffle=True, drop_last=True,\n                         num_workers=0):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers\n    )\n\nDataloader returns x and y. Let’s take a look at what stride does in a dataloader. Here is a simple example of batch_size of 1 and stride 1.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319]]), tensor([[3797, 3332,  319,  262]])]\n\n\nLet’s look at batch_size of 2. Both x and y are 2 sequences long. The first sequence is the same as the previous example. The second sequence is the next sequence in the text.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=1, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319],\n         [3797, 3332,  319,  262]]),\n tensor([[3797, 3332,  319,  262],\n         [3332,  319,  262, 2603]])]\n\n\nWhen we increase the stride to 2, the second sequence is 2 tokens after the first sequence. This is because we skipped 2 tokens when creating the second sequence. Instead of starting the second x with 3797, we start with 3332.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=2, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319],\n         [3332,  319,  262, 2603]]),\n tensor([[3797, 3332,  319,  262],\n         [ 319,  262, 2603,    0]])]\n\n\nWhen we have the same stride and max_length, we can see that the second sequence is the same as the first sequence. This is because we skipped 4 tokens when creating the second sequence. Now, there is no overlap between the x sequences and y sequences.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319],\n         [ 262, 2603,    0, 1375]]),\n tensor([[3797, 3332,  319,  262],\n         [2603,    0, 1375, 2497]])]\n\n\nNote that we also have drop_last parameter. This is to drop the last batch if it is smaller than batch_size. This is important during training because it can cause loss spikes.\n\n\n\n\n\n\nNote\n\n\n\nSmooth Training\nWhen training, it is important to keep the loss go down smoothly. If the loss spikes up, it may not come down, and the model has to be trained again from the start. Using drop_last parameter when training helps. There are also other ways to keep it from spiking, such as using bigger batch sizes and using a high quality data. Data could have particularly noisy and unclean gibberish. If these are concentrated in one batch, loss goes up to spike, and the training is over."
  },
  {
    "objectID": "blog/colab/bllm2_process_text.html#what-are-embeddings",
    "href": "blog/colab/bllm2_process_text.html#what-are-embeddings",
    "title": "delicious-nbdev",
    "section": "What Are Embeddings?",
    "text": "What Are Embeddings?\nAn embedding is a way to represent words or phrases as vectors of numbers. These vectors capture the semantic meaning of the words, allowing us to perform mathematical operations on them. For example, we can calculate the distance between two words to see how similar they are. Embeddings are used in many NLP tasks, such as machine translation, text classification, and question answering. They are also used in recommendation systems, image recognition, and other machine learning tasks. Embeddings are a powerful tool for understanding and processing text data.\nFor example, in this space: - “Cat” and “dog” might be close together because they’re both pets - “Run” and “sprint” would be nearby as they’re similar actions - “Hot” might be positioned opposite to “cold” - “King”, “queen”, “prince”, and “princess” would form a cluster showing both their royal relationships and gender differences\nIn modern LLMs like GPT-2, each token (word or subword) is represented by a vector of 768 numbers, while larger models like GPT-3 use even bigger vectors (2048 or more dimensions). These numbers aren’t randomly assigned - they’re learned during training to capture meaningful relationships between words.\n\nWhy Do We Need Embeddings?\nTraditional one-hot encoding represents each word as a vector of zeros with a single ‘1’, where the vector length equals vocabulary size. For a 50,000-word vocabulary, each word requires a 50,000-dimensional vector! This approach has several major problems:\n\nInefficiency:\n\nMassive storage requirements\nSparse representations (mostly zeros)\nComputationally expensive to process\n\nNo Semantic Information:\n\n“cat” and “kitten” are as different as “cat” and “motorcycle”\nNo way to measure word similarity\nNo capture of analogies or relationships\n\n\nEmbeddings solve these problems by: 1. Dense Representation: - Using much smaller vectors (768 vs 50,000 dimensions) - Every dimension carries meaningful information - Efficient storage and computation\n\nSemantic Relationships:\n\nSimilar words have similar vectors\nEnable amazing arithmetic: king - man + woman ≈ queen\nCapture multiple types of relationships:\n\nSemantic (car/automobile)\nSyntactic (run/running)\nConceptual (France/Paris :: Japan/Tokyo)\n\n\nLearning and Adaptation:\n\nEmbeddings improve during model training\nCan capture domain-specific meanings\nTransfer learning from one task to another\n\nMathematical Operations:\n\nCalculate similarity using cosine distance\nFind analogies through vector arithmetic\nCluster related concepts together\n\n\n\n\nEmbeddings Beyond Language Models\nThe power of embeddings extends far beyond just processing text. The same fundamental concept - representing complex objects as dense vectors in high-dimensional space - has revolutionized many fields:\n\nRecommendation Systems:\n\nNetflix maps both users and movies into the same embedding space\nUser embeddings capture viewing preferences and habits\nMovie embeddings represent genre, style, mood, and other features\nSimilarity between vectors predicts what you might like to watch\nEven time-of-day and viewing context can be embedded\n\nComputer Vision:\n\nImages are embedded into high-dimensional spaces\nSimilar images cluster together automatically\nEnables powerful features like:\n\nFace recognition\nObject detection\nImage similarity search\nStyle transfer\n\nTransfer learning from pre-trained vision models\n\nBioinformatics:\n\nProtein sequences represented as embeddings\nCaptures complex biochemical properties\nPredicts protein folding and interactions\nHelps in drug discovery\nEnables rapid searching of similar compounds\n\nAudio Processing:\n\nSpotify embeds songs based on:\n\nMusical features (tempo, key, instruments)\nListening patterns\nCultural context\nUser behavior\n\nVoice recognition systems use embeddings\nSound classification and similarity detection\n\nGraph Networks:\n\nSocial networks embed users and relationships\nKnowledge graphs embed concepts and connections\nTraffic networks embed locations and routes\nFraud detection systems embed transaction patterns\n\n\n\n\nCreating Embeddings in Practice\nCreating embeddings involves several key components and considerations:\n\nThe Embedding Layer:\n\nembedding_dim = 768  # typical size\nvocab_size = 50257  # GPT-2 vocabulary size\nembedding_layer = nn.Embedding(vocab_size, embedding_dim)\n\nInitialization Strategies:\n\nRandom initialization\nPre-trained embeddings (Word2Vec, GloVe)\nXavier/Glorot initialization\nCustom initialization based on domain knowledge\n\nTraining Approaches:\n\nEnd-to-end with model\nSeparate pre-training\nFine-tuning existing embeddings\nFrozen pre-trained embeddings\n\nAdvanced Techniques:\n\nSubword tokenization\nContextual embeddings\nMulti-modal embeddings\nHierarchical embeddings\n\n\n\n\nThe Position Problem\nPosition information is crucial in language understanding, but transformers are inherently position-agnostic. Here’s how we solve this:\n\nLearned Positional Embeddings:\n\nModel learns position representations\nCan capture common patterns\nMore flexible but needs training\nUsed in models like GPT\n\nSinusoidal Embeddings:\n\nFixed mathematical patterns\nUses sine and cosine functions\nDifferent frequencies for different dimensions\nOriginal transformer approach\nNo training required\nCan extrapolate to longer sequences\n\nRelative Positional Embeddings:\n\nEncode relative distances between tokens\nBetter for certain tasks\nMore computationally intensive\nUsed in modern architectures like T5\n\nHybrid Approaches:\n\nCombining different types\nTask-specific adaptations\nNovel architectural innovations\n\n\n\n\nTrade-offs and Challenges\nThe use of embeddings, while powerful, comes with important considerations and challenges:\n\nTraining Data Requirements:\n\nNeed massive amounts of quality data\nData must be representative and balanced\nDomain-specific data often required\nPoor quality data leads to poor embeddings\nData cleaning and preprocessing crucial\n\nComputational Costs:\n\nTraining embeddings is resource-intensive\nLarge memory requirements\nGPU/TPU hardware often necessary\nInference time can be significant\nStorage costs for large embedding tables\n\nBias and Fairness:\n\nEmbeddings inherit biases from training data\nCan amplify societal prejudices\nGender, racial, and cultural biases common\nDebiasing techniques available but imperfect\nEthical considerations in deployment\n\nTechnical Challenges:\n\nChoosing optimal embedding dimension\nHandling out-of-vocabulary words\nDealing with rare words/tokens\nManaging embedding table size\nUpdating embeddings efficiently\n\nPerformance Trade-offs:\n\nAccuracy vs. computation speed\nMemory usage vs. embedding size\nTraining time vs. model quality\nGeneralization vs. specialization\nReal-time requirements vs. model complexity"
  },
  {
    "objectID": "blog/colab/bllm2_process_text.html#the-future-of-embeddings",
    "href": "blog/colab/bllm2_process_text.html#the-future-of-embeddings",
    "title": "delicious-nbdev",
    "section": "The Future of Embeddings",
    "text": "The Future of Embeddings\n\nArchitectural Innovations:\n\nMore efficient embedding architectures\nSparse embedding techniques\nDynamic embedding sizes\nAdaptive embedding strategies\nNovel initialization methods\n\nMulti-modal Developments:\n\nCross-modal embeddings\nUniversal embeddings across domains\nJoint learning of different modalities\nTransfer learning improvements\nDomain adaptation techniques\n\nEfficiency Improvements:\n\nCompression techniques\nQuantization methods\nPruning strategies\nDistributed embedding systems\nHardware-specific optimizations\n\nEthical Considerations:\n\nBetter debiasing techniques\nFairness-aware embeddings\nInterpretable embeddings\nPrivacy-preserving methods\nRobust evaluation metrics\n\nEmerging Applications:\n\nQuantum computing embeddings\nNeuromorphic hardware adaptation\nEdge device implementations\nReal-time embedding updates\nFederated learning approaches\n\nResearch Directions:\n\nTheoretical understanding improvements\nFormal mathematical frameworks\nStability and robustness studies\nScaling laws investigation\nNovel training objectives\n\n\nThe field of embeddings continues to be a crucial area of research and development in machine learning, with new breakthroughs and applications emerging regularly. As we better understand their properties and capabilities, we can expect to see even more innovative uses across various domains.\nTo learn more about embeddings, please refer to the following resources: - Google’s tutorial on word embeddings, document search, and applications: https://github.com/google/generative-ai-docs/tree/main/site/en/gemini-api/tutorials\n\n\n\n\n\n\nTip\n\n\n\nWord Embeddings Size\nWord embeddings size with multiples of 64 have hardware optimization.\n\n\n\nUsing Embeddings\nLet’s create embeddings with pytorch. We will use a simple example.\n\ntoken_ids = torch.tensor([2,1,0])\ntoken_ids\n\ntensor([2, 1, 0])\n\n\n\nvocab_size = 3\noutput_dim = 4\n\ntorch.manual_seed(42)\nemb = torch.nn.Embedding(vocab_size, output_dim)\nemb\n\nEmbedding(3, 4)\n\n\n\nemb.weight\n\nParameter containing:\ntensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)\n\n\nAn embedding layer has weights defined by the vocab size and the output dimension. The weights are normally distributed with mean of 0 and standard deviation of 1. These weights are learnable parameters. With these embedding layer, we can convert the token ids into embeddings by simply calling the embedding layer with the token ids.\n\n# First embedding\nemb(torch.tensor([0]))\n\ntensor([[0.3367, 0.1288, 0.2345, 0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n# Second embedding\nemb(torch.tensor([1]))\n\ntensor([[-1.1229, -0.1863,  2.2082, -0.6380]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\nWe can also simply select using an index.\n\nemb.weight[0]\n\ntensor([0.3367, 0.1288, 0.2345, 0.2303], grad_fn=&lt;SelectBackward0&gt;)\n\n\nOr we can select multiple embeddings at once in whatever order we want.\n\nemb(torch.tensor([1,0]))\n\ntensor([[-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\nemb(token_ids)\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\nOne-hot encoding\nAn older way to do this is using one hot encoding.\nWe can manually do one hot encoding\n\nparams = torch.nn.Parameter(emb.weight)\nparams\n\nParameter containing:\ntensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)\n\n\n\nonehot = torch.nn.functional.one_hot(token_ids)\nonehot\n\ntensor([[0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0]])\n\n\n\nonehot.float()@params\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;MmBackward0&gt;)\n\n\nWe can also use a linear layer\n\nlinear = torch.nn.Linear(vocab_size, output_dim, bias=False)\nlinear.weight = torch.nn.Parameter(emb.weight.T)\nlinear.weight\n\nParameter containing:\ntensor([[ 0.3367, -1.1229,  0.4617],\n        [ 0.1288, -0.1863,  0.2674],\n        [ 0.2345,  2.2082,  0.5349],\n        [ 0.2303, -0.6380,  0.8094]], requires_grad=True)\n\n\n\nlinear(onehot.float())\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;MmBackward0&gt;)\n\n\nUsing torch.nn.Embedding is the most efficient way to convert token ids into embeddings. It’s faster and more memory-efficient than using one-hot encoding or a linear layer.\n\n\n\nPositional Embeddings\nPositional embeddings are added to the token embeddings to encode the position of the token in the sequence. This is because transformers do not have any inherent sense of order. There are two main types of positional embeddings: relative positional embeddings and absolute positional embeddings.\n\nRelative positional embeddings\n\nRelative positional embeddings encode the distance between tokens. This is useful because the model can learn to pay attention to tokens that are close to each other. However, relative positional embeddings are not as efficient as absolute positional embeddings.\n\nAbsolute positional embeddings\n\nAbsolute positional embeddings encode the absolute position of the token in the sequence. This is useful because the model can learn to pay attention to tokens that are in certain positions. However, absolute positional embeddings are not as flexible as relative positional embeddings. It can be difficult to change the size of the context length of the model because it was fixed during the training.\nUsually, these embeddings are added to the token embeddings. \nLet’s create absolute positional embeddings for simplicity. GPT-2 also used this. We have token_ids and emb from before.\n\ntoken_ids\n\ntensor([2, 1, 0])\n\n\n\nemb\n\nEmbedding(3, 4)\n\n\nLet’s get token embeddings again.\n\ntoken_emb = emb(token_ids)\ntoken_emb\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\nPositional embedding is another embedding layer with the same output dimension as the token embedding layer. The input dimension is the context length. The context length is the maximum length of the sequence that the model can handle. In this case, we will use a context length of 3. And output size is 4. Since vocab size and output size are the same, we can use the same embedding layer for both token and positional embeddings.\n\nemb2 = torch.nn.Embedding(3, 4)\nemb2\n\nEmbedding(3, 4)\n\n\n\npos_emb = emb2(torch.arange(3))\npos_emb\n\ntensor([[-0.7658, -0.7506,  1.3525,  0.6863],\n        [-0.3278,  0.7950,  0.2815,  0.0562],\n        [ 0.5227, -0.2384, -0.0499,  0.5263]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\nFinally, we can get the input embedding for the model by adding the token embeddings and the positional embeddings.\n\ninp_emb = token_emb + pos_emb\ninp_emb\n\ntensor([[-0.3042, -0.4833,  1.8875,  1.4957],\n        [-1.4506,  0.6086,  2.4897, -0.5818],\n        [ 0.8594, -0.1095,  0.1846,  0.7567]], grad_fn=&lt;AddBackward0&gt;)\n\n\nTo learn more about positional embeddings, please refer to the following resources: - A blog on Rotary Position Encoding (ROPE) by Akash Nain: https://aakashkumarnain.github.io/posts/ml_dl_concepts/rope - Reformer paper: https://arxiv.org/pdf/2104.09864"
  },
  {
    "objectID": "blog/colab/bllm2_process_text.html#byte-pair-encoding-bpe",
    "href": "blog/colab/bllm2_process_text.html#byte-pair-encoding-bpe",
    "title": "delicious-nbdev",
    "section": "Byte pair encoding (BPE)",
    "text": "Byte pair encoding (BPE)\nByte pair encoding (BPE) is a data compression technique that is used to create a vocabulary of subword units. It was a bit confusing for me to understand this because I didn’t know about bytes, hexdecimal, ASCII, and UTF-8. We can just think of byte as a tiny thing that makes up a character. The algorithm is very simple. It works by iteratively merging the most frequent pair of bytes in the text. This process is repeated until the desired vocabulary size is reached. The resulting vocabulary consists of the most frequent subword units in the text.\nThe book does not cover details of BPE, but bpe-from-scratch is included in the github. This version focuses on education purposes and skips some steps, such as converting the text into bytes. To learn more about bpe, I recommend minbpe by Karpathy. The code has many comments and is easy to understand.\n\n\n\n\n\n\nNote\n\n\n\nWhat are bytes, hexadecimal, ASCII, and UTF-8? And what do they have to do with BPE?\nIt is not necessary to know those concepts to understand how BPE works in a big picture. However, I had a lot of fun learning about these. And it gives a bit more in depth understanding of BPE and computers.\nBriefly, a byte is eight bits, and each bit is a number consists of 0 or 1. For instance, “00000000” and “10101100” are bytes. There are 2**8 or 256 ways of creatinga byte. Instead of writing eight characters long for each byte, we can use hexadecimal to write two characters for each byte. In simple terms, ASCII is an old way to convert or convert back a byte into a character and only has characters on english keyboard, such as english alphabet, numbers, +, -, etc. UTF-8 is modern way that includes characters from other languages and emojis. Using hexadecimal is useful because UTF-8 uses multiple bytes.\n\n\n\nHow BPE works\nBriefly, this is how to train BPE:\n\nVocabulary is initialized with first 256 ASCII characters.\nText is converted into bytes.\nUntil vocabulary size is reached:\n\nCount the frequency of each pair of bytes in the text.\nMerge the most frequent pair of bytes into a single byte.\nAdd the new byte to the vocabulary.\n\n\nThat’s it. BPE is a simple and effective way to create a vocabulary of subword units. It is used in many NLP models, including GPT-2, GPT-3, and BERT. I was planning on explaining BPE in more detail, but I think it is better to leave it as an exercise for the reader. Maybe I will write a blog on it in the future with some information about bytes, hexadecimal digits, ASCII, UTF-8, and such. Of course it is not necessary to understand BPE, but they are related and are fun to learn about."
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "In this blog, we will use neural networks to train MNIST dataset. We start with using pytorch and integrate fastai in the end.\nThis is part of Further Research from fastbook chapter 4.\n\n\nThis part is the same as last blog where we created a baseline by calculating means of images.\n\nfrom fastai.vision.all import *\n\n\nmatplotlib.rc('image', cmap='Greys')\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator&gt;\n\n\n\npath = untar_data(URLs.MNIST)\nPath.BASE_PATH = path\npath.ls()\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00&lt;00:00]\n    \n    \n\n\n(#2) [Path('training'),Path('testing')]\n\n\n\n(path/'training').ls()\n\n(#10) [Path('training/7'),Path('training/8'),Path('training/5'),Path('training/6'),Path('training/9'),Path('training/3'),Path('training/2'),Path('training/0'),Path('training/4'),Path('training/1')]\n\n\nI looked at the number of files for each number, and they are not the same.\n\ntrn_counts = {n: len((path/'training'/str(n)).ls()) for n in range(10)}\ntrn_counts\n\n{0: 5923,\n 1: 6742,\n 2: 5958,\n 3: 6131,\n 4: 5842,\n 5: 5421,\n 6: 5918,\n 7: 6265,\n 8: 5851,\n 9: 5949}\n\n\nSince these are paths for images we have to convert them into pytorch tensors. Pytorch provides us with broadcasting ability, which is very fast.\nFirst, we turn image paths into numbers and return as L, which is an upgraded version of list.\n\ndef create_images(path):\n    return L(PILImage.create(x) for x in path.ls())\n\n\nones = create_images(path/'training'/'1')\nim1 = ones[0]\nim1\n\n\n\n\n\n\n\n\nWe need to turn them into tensors for speed. However, we have to check the shape of tensors to make sure they are broadcastable and the shape is one we’re expecting.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs])\n\n\nstacked_ones = create_tensor_imgs(ones)\nstacked_ones[0].shape\n\ntorch.Size([28, 28, 3])\n\n\n\nstacked_ones.shape\n\ntorch.Size([6742, 28, 28, 3])\n\n\nLooking at the shape of an image, it is interesting to find out that its shape is [28, 28, 3] (y, x, and color values). I was expecting to see [28, 28] (y and x values) because it is a black and white image. So, let’s see what’s going on here.\n\narray(im1)[10:15, 12:15]\n\narray([[[  0,   0,   0],\n        [221, 221, 221],\n        [251, 251, 251]],\n\n       [[ 64,  64,  64],\n        [236, 236, 236],\n        [251, 251, 251]],\n\n       [[127, 127, 127],\n        [251, 251, 251],\n        [251, 251, 251]],\n\n       [[127, 127, 127],\n        [251, 251, 251],\n        [251, 251, 251]],\n\n       [[128, 128, 128],\n        [253, 253, 253],\n        [253, 253, 253]]], dtype=uint8)\n\n\nSo, it is just filled with the same numbers per each row. We just need one column, so we can simply take a mean of last rank.\n\narray(im1)[10:15, 12:15].mean(-1)\n\narray([[  0., 221., 251.],\n       [ 64., 236., 251.],\n       [127., 251., 251.],\n       [127., 251., 251.],\n       [128., 253., 253.]])\n\n\nSo, here is the updated version of create_tensor_imgs that calculates a mean so that we can get rid of the last rank of our tensors.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)\n\n\nstacked_ones = create_tensor_imgs(ones)\nstacked_ones[0].shape\n\ntorch.Size([28, 28])\n\n\n\narray(stacked_ones[0])[10:15, 12:15]\n\narray([[  0., 221., 251.],\n       [ 64., 236., 251.],\n       [127., 251., 251.],\n       [127., 251., 251.],\n       [128., 253., 253.]], dtype=float32)\n\n\nLooks good. Now, we want those pixel values to be between 0 and 1 so that it is easier to train neural networks.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)/255\n\n\nstacked_ones = create_tensor_imgs(ones)\narray(stacked_ones[0])[10:15, 12:15]\n\narray([[0.        , 0.8666667 , 0.9843137 ],\n       [0.2509804 , 0.9254902 , 0.9843137 ],\n       [0.49803922, 0.9843137 , 0.9843137 ],\n       [0.49803922, 0.9843137 , 0.9843137 ],\n       [0.5019608 , 0.99215686, 0.99215686]], dtype=float32)\n\n\n\ndef stacked_nums(path):\n    \"Converts path into stacked tensors.\"\n    imgs = create_images(path)\n    return create_tensor_imgs(imgs)\n\n\nstacked_zeros = stacked_nums(path/'training'/'0')\nstacked_twos = stacked_nums(path/'training'/'2')\nstacked_threes = stacked_nums(path/'training'/'3')\nstacked_fours = stacked_nums(path/'training'/'4')\nstacked_fives = stacked_nums(path/'training'/'5')\nstacked_sixs = stacked_nums(path/'training'/'6')\nstacked_sevens = stacked_nums(path/'training'/'7')\nstacked_eights = stacked_nums(path/'training'/'8')\nstacked_nines = stacked_nums(path/'training'/'9')\n\nWe also need test data to check how we’re doing. Without test set, we don’t know whether we are overfitting or not. So, we do the same process we did for test data.\n\ntst_0_tens = stacked_nums(path/'testing'/'0')\ntst_1_tens = stacked_nums(path/'testing'/'1')\ntst_2_tens = stacked_nums(path/'testing'/'2')\ntst_3_tens = stacked_nums(path/'testing'/'3')\ntst_4_tens = stacked_nums(path/'testing'/'4')\ntst_5_tens = stacked_nums(path/'testing'/'5')\ntst_6_tens = stacked_nums(path/'testing'/'6')\ntst_7_tens = stacked_nums(path/'testing'/'7')\ntst_8_tens = stacked_nums(path/'testing'/'8')\ntst_9_tens = stacked_nums(path/'testing'/'9')\n\ntst_tens = L([tst_0_tens, tst_1_tens, tst_2_tens, tst_3_tens, tst_4_tens, \n              tst_5_tens, tst_6_tens, tst_7_tens, tst_8_tens, tst_9_tens])\n\ntst_counts = {n: len((path/'testing'/str(n)).ls()) for n in range(10)}\n\ntst_1_tens.shape, tst_9_tens.shape\n\n(torch.Size([1135, 28, 28]), torch.Size([1009, 28, 28]))\n\n\n\ntst_counts\n\n{0: 980,\n 1: 1135,\n 2: 1032,\n 3: 1010,\n 4: 982,\n 5: 892,\n 6: 958,\n 7: 1028,\n 8: 974,\n 9: 1009}\n\n\nNow, we stack all the training tensors together to create trn_x. And create trn_y with the counts. We zip those together to create a dataset. We also change the shape of tensors from [28, 28] to [784]. This shape is more useful for matrix multiplication we will do later.\n\ntrn_x = torch.cat([stacked_zeros, stacked_ones, stacked_twos, \n                   stacked_threes, stacked_fours, stacked_fives, \n                   stacked_sixs, stacked_sevens, stacked_eights, \n                   stacked_nines]).view(-1, 28*28)\n\ntrn_y = tensor(list(flatten([[x] * y for x, y in trn_counts.items()])))\ntrn_dset = list(zip(trn_x, trn_y))\n\ntrn_x.shape, trn_y.shape\n\n(torch.Size([60000, 784]), torch.Size([60000]))\n\n\n\nshow_image(trn_x[50000].view(28,28)), trn_y[50000]\n\n\n\n\n\n\n\n\n\nx, y = trn_dset[0]\nx.shape, y.shape\n\n(torch.Size([784]), torch.Size([]))\n\n\nTraining data looks good, so let’s work on the test data.\n\ntst_x = torch.cat([tst_0_tens, tst_1_tens, tst_2_tens, \n                     tst_3_tens, tst_4_tens, tst_5_tens,\n                     tst_6_tens, tst_7_tens, tst_8_tens,\n                     tst_9_tens]).view(-1, 28*28)\n\ntst_y = tensor(list(flatten([[x] * y for x, y in tst_counts.items()])))\ntst_dset = list(zip(tst_x, tst_y))\n\ntst_x.shape, tst_y.shape\n\n(torch.Size([10000, 784]), torch.Size([10000]))\n\n\n\n\n\nIn order to use neural network, we need to initialize parameters, such as weights and bias.\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\n\nWe are initializing weights parameter with size (28*28, 10) because 28*28 is the size of our input (reshaped images) and 10 is the size of our output. Why do we return 10 numbers as an output for each image? Because we’re making predictions on each category (from 0 to 9). Out of all those 10 numbers, the biggest number is the model’s guess.\nBut couldn’t we just output 1 number instead and pick a target based on the range of our output? For instance, we could use sigmoid function to get our output numbers to be between 0 and 1, and if it’s between 0 and 0.1, it is 0, if it’s between 0.1 and 0.2, it is 1, and so on. That’s also a possible approach to this problem.\nWith those parameters, we will perform matrix multiplications. There are multiple ways to calculate it. One way involves manually transposing a matrix and getting the sum over dimension 1, and another way involves using @ operator, which does all of that for us.\n\n(x*weights.T).sum(dim=1) + bias\n\ntensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n         2.4677, 16.0547], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nx@weights + bias\n\ntensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n         2.4677, 16.0547], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ndef linear(xb): return xb@weights + bias\n\n\npreds = linear(trn_x)\npreds\n\ntensor([[ -3.0786,   6.5823,  12.8080,  ...,  -2.3099,   2.4677,  16.0547],\n        [ -4.0905,   1.3646,  -1.6264,  ...,  -5.0465,  -8.9219,   5.3742],\n        [ -4.6858,  -1.6842,   5.6141,  ...,  -7.0370,   7.6748,   1.3543],\n        ...,\n        [-15.4864,  -1.8742,  -6.2627,  ...,  -0.1359,  10.1261,  18.8826],\n        [-22.5967,   5.1747,  -6.5157,  ...,  -2.5263,  20.9854,  12.2597],\n        [-15.9537,  -2.7749,   3.1555,  ...,   5.3508,  11.7086,  13.8179]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\nSo, we know that these numbers represent “socres” for each category. Let’s look at the first prediction.\n\npreds[0]\n\ntensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n         2.4677, 16.0547], grad_fn=&lt;SelectBackward0&gt;)\n\n\nFrom these numbers, the biggest number is 16.0547, which is a prediction for 9. So, this predicted 9. We can use torch.argmax to calculate the maximum index easily.\n\npreds[0].argmax()\n\ntensor(9)\n\n\nFor multiple predictions, we can specify the dimension to be 1.\n\npreds.argmax(dim=1)\n\ntensor([9, 5, 5,  ..., 9, 8, 9])\n\n\nWith predictions on categories, we can calculate accuracy by comparing it to the targets (trn_y).\n\n(preds.argmax(dim=1) == trn_y).float().mean() * 100\n\ntensor(12.0433)\n\n\nWe have about 10% accuracy since we are using randomly initialized parameters to predict.\n\ndef acc(pred, targ):\n    return (pred.argmax(dim=1) == targ).float().mean() * 100\n\n\nacc(preds, trn_y)\n\ntensor(12.0433)\n\n\n\n\n\nIn order to improve our accuracy, we have to tweak our weights and biases (parameters). To improve our parameters, we have to use a loss function. Then, we can calculate gradients and use them to update our parameters. Loss functions are just mathematical functions, but they have smooth shapes. This is important because gradients cannot be calculated on sharp edges. Let’s take look at a sigmoid function.\n\ndef plot_function(fn, title=None, xmin=-4.1, xmax=4.1):\n    x = torch.linspace(xmin, xmax, 20)\n    plt.plot(x, fn(x))\n    if title: plt.title(title)\n\n\nplot_function(torch.nn.Sigmoid(), title='Sigmoid function')\n\n\n\n\n\n\n\n\nSigmoid function has no sharp edge, so gradients can be found anywhere. This will also ensure that predictions will be between 0 and 1. We typically use sigmoid function when we try to predict boolean values. For instance, we could use this function for problems like “is it a cat or dog?”\nBecause we have multiple categories to predict, we use cross entropy loss. Cross entropy loss can be thought of as sigmoid designed for multiple categories. Cross entropy loss is composed of multiple functions. First, we use a softmax function ensures that all numbers are positive and add up to 1. Also, this function takes exponential of all values, so even the slight difference will be magnified. This will result in choosing one class with more confidence. Second, we take the log of the activations we got from softmax. Let’s look at a log function.\n\nplot_function(torch.log, title=\"Log function\", xmin=-0.1, xmax=1)\n\n\n\n\n\n\n\n\nIn log function, we get a very big negative value if a number gets closer to 0, and reaches 0 when it gets 1. So, if our activation is close to 0 after softmax, it will be a big negative number after taking a log. This will result in a big gradient. However, if the number is close to 1, it is good enough. So, it will punish numbers that are far off, but forgiving for close numbers. But we still have problems. We still have 10 activation numbers and all of them are negative.\nThe last function we will use is negative log likelihood, which picks one number from our 10 activations and multiply by -1 to make it a positive value. Although it has log in its name, it actually does not take the log function. We have to do it ourselves. Let’s take a look at how it looks like.\n\nsm_acts = torch.nn.Softmax(dim=1)(preds)\nsm_acts[0]\n\ntensor([4.6022e-09, 7.2222e-05, 3.6513e-02, 3.9309e-03, 2.0602e-11, 2.0636e-02,\n        2.5585e-04, 9.9276e-09, 1.1795e-06, 9.3859e-01],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nlog_acts = torch.log(sm_acts)\nlog_acts[0]\n\ntensor([-19.1967,  -9.5358,  -3.3101,  -5.5389, -24.6057,  -3.8807,  -8.2709,\n        -18.4279, -13.6504,  -0.0634], grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ntrn_y[0]\n\ntensor(0)\n\n\n\ntorch.nn.NLLLoss()(log_acts[0], trn_y[0])\n\ntensor(19.1967, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nNotice that we get the same values for the built in cross entropy loss function.\n\ntorch.nn.NLLLoss()(log_acts, trn_y)\n\ntensor(14.3907, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nloss = torch.nn.CrossEntropyLoss()(preds, trn_y)\nloss\n\ntensor(14.3907, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nIt’s easier to use the built in function, and it’s also faster.\n\n\n\n15.5 ms ± 627 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n21.7 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\nSo, we calculated the loss. With loss, we can calculate the gradients by calling backward(). Then, we can take a look at them.\n\nloss.backward()\n\n\nbias.grad, weights.grad.sum()\n\n(tensor([-9.3132e-09]), tensor(-1.0490e-05))\n\n\nWe can define a function it.\n\ndef calc_grad(xb, yb, model):\n    \"Calculate the gradients\"\n    preds = model(xb)\n    loss = torch.nn.CrossEntropyLoss()(preds, yb)\n    loss.backward()\n    return loss\n\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\n\n\ncalc_grad(trn_x, trn_y, linear)\nweights.grad.mean(), bias.grad.mean()\n\n(tensor(-1.2164e-10), tensor(-9.3132e-09))\n\n\nAfter calculating the gradients, we can update our parameters using the gradients. We can multiply parameter’s gradient by any number, which is referred to as a learning rate. If it’s big, it will train faster, and if it’s slow, it will train slower. However, if it’s too big, it cannot train. So, we can try couple numbers and see what works and what doensn’t. After updating, we have to reset the gradients. If we don’t, gradients will keep growing.\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\ncalc_grad(trn_x, trn_y, linear)\nbias.grad, weights.grad.sum()\n\n(tensor([-1.3039e-08]), tensor(-9.5367e-07))\n\n\n\ncalc_grad(trn_x, trn_y, linear)\nbias.grad, weights.grad.sum()\n\n(tensor([-2.6077e-08]), tensor(-1.9073e-06))\n\n\nSee how it grew? we have to use zero_() on the gradients. Training _ on functions means in place function.\n\nweights.data -= weights.grad * 10\nbias.data -= bias.grad * 10\nweights.grad.zero_()\nbias.grad.zero_()\n\ntensor([0.])\n\n\n\n\n\nIf we put everything together, this is what it looks like. We will train with the whole data 20 times, which means we are using 20 epochs.\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\nfor _ in range(20):\n    calc_grad(trn_x, trn_y, linear)\n    weights.data -= weights.grad * 10\n    weights.grad.zero_()\n    bias.data -= bias.grad * 10\n    bias.grad.zero_()\n    pred = linear(tst_x)\n    print(acc(pred, tst_y))\n\ntensor(20.9500)\ntensor(38.9100)\ntensor(40.2700)\ntensor(48.3800)\ntensor(42.6600)\ntensor(51.5200)\ntensor(45.4200)\ntensor(57.6000)\ntensor(60.3500)\ntensor(62.2100)\ntensor(67.2600)\ntensor(70.8800)\ntensor(75.0300)\ntensor(71.8100)\ntensor(76.1800)\ntensor(82.2500)\ntensor(85.1200)\ntensor(85.3800)\ntensor(85.1700)\ntensor(85.1200)\n\n\nWe got over 80% accuracy, which is better tan our baseline using means of images from the last time, which had 66% accuracy. However, we can still make this better.\n\n\n\nWe updated our parameters after looking at the whole dataset. The MNIST dataset is very small, so it’s not a problem, but dataset can get really big, and it would take a long time to train this way. Another approach to train is update parameters after each image. This is not a good idea either as our model learns about one image at a time, which can lead to overfitting. That’s why we use small batches of data.\n\ndl = DataLoader(trn_dset, bs=256)\nxb, yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256]))\n\n\nWith fastai’s DataLoader, we will have 256 items per batch. And we will do the same thing for the test set.\n\ntst_dl = DataLoader(tst_dset, bs=256)\n\n\ndef train(lr=1.):\n    params = weights, bias\n    for xb, yb in dl:\n        calc_grad(xb, yb, linear)\n        for p in params:\n            p.data -= p.grad * lr\n            p.grad.zero_()\n\n\ndef validate_epoch(model):\n    accs = [acc(model(xb), yb) for xb, yb in tst_dl]\n    return round(tensor(accs).mean().item(), 4)\n\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\nfor _ in range(30):\n    train(0.1)\n    print(validate_epoch(linear), end='; ')\n\n15.9961; 27.8516; 39.1016; 46.3672; 51.3281; 54.8535; 57.2656; 59.4922; 61.1328; 62.5098; 63.6328; 64.5605; 65.2637; 65.9766; 66.6699; 67.2754; 67.9004; 68.418; 68.9355; 69.3164; 69.6777; 70.0586; 70.3809; 70.625; 70.8789; 71.1328; 71.3379; 71.5039; 71.6602; 71.8262; \n\n\nAccuracy is not as good as using the whole dataset, but let’s keep trying more things.\n\n\n\nWe can use pytorch’s torch.nn.Linear instead of our linear. Good thing about using this is that we can get grab parameters easily.\n\nlinear_model = torch.nn.Linear(28*28, 10)\n\n\nw, b = list(linear_model.parameters())\nw.shape, b.shape\n\n(torch.Size([10, 784]), torch.Size([10]))\n\n\nWe define a BasicOptim class that has step and zero_grad methods.\n\nclass BasicOptim:\n    def __init__(self, params, lr=0.01):\n        self.params = params\n        self.lr = lr\n    \n    def step(self):\n        for p in self.params:\n            p.data -= p.grad * self.lr\n    \n    def zero_grad(self):\n        for p in self.params:\n            p.grad.zero_()\n\nOptimizer can be used this way. And we can update our train function to use the optimizer.\n\nopt = BasicOptim(list(linear_model.parameters()))\n\n\ndef train(model):\n    for xb, yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\nIt’s good to define a function to train a model.\n\ndef train_model(epochs, model):\n    for _ in range(epochs):\n        train(model)\n        print(validate_epoch(model))\n\n\ntrain_model(20, linear_model)\n\n12.2266\n31.3184\n46.7578\n55.4785\n61.2988\n65.1953\n68.4375\n70.9473\n72.9297\n74.8047\n76.25\n77.5391\n78.6035\n79.6094\n80.3027\n80.9473\n81.6016\n82.2754\n82.8027\n83.2031\n\n\nWe can also use fastai’s SGD.\n\nlinear_model = nn.Linear(28*28, 10)\nopt = SGD(list(linear_model.parameters()), .01)\n\n\ntrain_model(20, linear_model)\n\n12.2461\n32.4121\n47.0117\n55.4004\n61.1621\n65.166\n68.418\n70.8398\n72.9297\n75.1172\n76.377\n77.5488\n78.623\n79.4922\n80.3809\n81.0547\n81.582\n82.2363\n82.7148\n83.0664\n\n\nAlso, we can use fastai’s learner. We just need DataLoaders, which combines training dataloaders with test dataloaders. By using learn.fit, we can train easily and get a nice output, too.\n\ndls = DataLoaders(dl, tst_dl)\n\n\nlearn = Learner(dls, nn.Linear(28*28, 10), loss_func=nn.CrossEntropyLoss(), opt_func=SGD, metrics=acc)\n\n\nlearn.fit(20, 0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc\ntime\n\n\n\n\n0\n1.030682\n2.442845\n10.260000\n00:03\n\n\n1\n0.756178\n1.755957\n30.840000\n00:04\n\n\n2\n0.652057\n1.387471\n45.750000\n00:04\n\n\n3\n0.593682\n1.172395\n54.570000\n00:03\n\n\n4\n0.555542\n1.030824\n60.259998\n00:06\n\n\n5\n0.528407\n0.930011\n64.510002\n00:05\n\n\n6\n0.507978\n0.854310\n67.820000\n00:06\n\n\n7\n0.491943\n0.795262\n70.309998\n00:03\n\n\n8\n0.478946\n0.747864\n72.440002\n00:05\n\n\n9\n0.468138\n0.708950\n74.440002\n00:03\n\n\n10\n0.458963\n0.676414\n75.860001\n00:06\n\n\n11\n0.451041\n0.648797\n77.139999\n00:03\n\n\n12\n0.444104\n0.625053\n78.199997\n00:05\n\n\n13\n0.437957\n0.604415\n79.160004\n00:06\n\n\n14\n0.432457\n0.586306\n80.010002\n00:04\n\n\n15\n0.427493\n0.570282\n80.739998\n00:05\n\n\n16\n0.422980\n0.555999\n81.290001\n00:03\n\n\n17\n0.418851\n0.543184\n82.040001\n00:05\n\n\n18\n0.415051\n0.531618\n82.360001\n00:05\n\n\n19\n0.411538\n0.521124\n82.720001\n00:04\n\n\n\n\n\nWe can improve this accuracy with adding another linear layers and a relu layer. We create a deeper network as long as there’s a relu layer in between each linear layer. Relu layer simply turns negative numbers to 0.\n\nnet = nn.Sequential(\n    nn.Linear(28*28, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10)\n)\n\nWe made a network of layers. Since it is deeper, we have to reduce the learning rate, which results in slower training speed. However, we can train for more epochs. Let’s try training for 100 epochs.\n\nlearn = Learner(dls, net, loss_func=nn.CrossEntropyLoss(), \n                opt_func=SGD, metrics=acc)\nlearn.fit(100, 0.004)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc\ntime\n\n\n\n\n0\n2.257359\n2.186582\n28.950001\n00:04\n\n\n1\n1.927473\n2.062460\n16.990000\n00:05\n\n\n2\n1.586289\n1.925866\n19.389999\n00:07\n\n\n3\n1.296686\n1.757989\n29.430000\n00:04\n\n\n4\n1.077161\n1.585793\n37.320000\n00:06\n\n\n5\n0.920138\n1.439800\n42.250000\n00:07\n\n\n6\n0.809401\n1.322030\n45.910000\n00:08\n\n\n7\n0.729531\n1.226982\n48.950001\n00:04\n\n\n8\n0.669996\n1.148582\n52.099998\n00:06\n\n\n9\n0.624211\n1.082637\n54.900002\n00:05\n\n\n10\n0.587927\n1.026398\n57.480000\n00:05\n\n\n11\n0.558507\n0.977675\n59.820000\n00:08\n\n\n12\n0.534146\n0.934994\n62.150002\n00:07\n\n\n13\n0.513631\n0.897281\n63.990002\n00:07\n\n\n14\n0.496094\n0.863748\n65.589996\n00:07\n\n\n15\n0.480910\n0.833677\n66.970001\n00:08\n\n\n16\n0.467640\n0.806602\n68.199997\n00:11\n\n\n17\n0.455922\n0.782081\n69.519997\n00:09\n\n\n18\n0.445503\n0.759768\n70.489998\n00:07\n\n\n19\n0.436185\n0.739375\n71.389999\n00:07\n\n\n20\n0.427801\n0.720681\n72.199997\n00:05\n\n\n21\n0.420199\n0.703508\n73.080002\n00:07\n\n\n22\n0.413270\n0.687696\n73.760002\n00:06\n\n\n23\n0.406932\n0.673062\n74.489998\n00:05\n\n\n24\n0.401142\n0.659454\n75.080002\n00:05\n\n\n25\n0.395792\n0.646811\n75.730003\n00:05\n\n\n26\n0.390829\n0.635046\n76.250000\n00:05\n\n\n27\n0.386238\n0.624030\n76.760002\n00:05\n\n\n28\n0.381959\n0.613709\n77.419998\n00:05\n\n\n29\n0.377980\n0.604007\n77.910004\n00:09\n\n\n30\n0.374245\n0.594876\n78.379997\n00:07\n\n\n31\n0.370748\n0.586283\n78.849998\n00:07\n\n\n32\n0.367451\n0.578153\n79.239998\n00:05\n\n\n33\n0.364345\n0.570461\n79.610001\n00:06\n\n\n34\n0.361401\n0.563176\n79.919998\n00:05\n\n\n35\n0.358619\n0.556240\n80.260002\n00:05\n\n\n36\n0.355972\n0.549619\n80.610001\n00:05\n\n\n37\n0.353456\n0.543326\n80.839996\n00:05\n\n\n38\n0.351051\n0.537354\n81.120003\n00:05\n\n\n39\n0.348749\n0.531646\n81.330002\n00:05\n\n\n40\n0.346539\n0.526188\n81.589996\n00:05\n\n\n41\n0.344433\n0.520945\n81.839996\n00:09\n\n\n42\n0.342406\n0.515914\n82.000000\n00:04\n\n\n43\n0.340465\n0.511089\n82.190002\n00:07\n\n\n44\n0.338599\n0.506455\n82.290001\n00:06\n\n\n45\n0.336791\n0.501979\n82.480003\n00:05\n\n\n46\n0.335052\n0.497672\n82.660004\n00:05\n\n\n47\n0.333371\n0.493508\n82.809998\n00:05\n\n\n48\n0.331753\n0.489475\n83.040001\n00:06\n\n\n49\n0.330158\n0.485592\n83.220001\n00:05\n\n\n50\n0.328622\n0.481835\n83.430000\n00:06\n\n\n51\n0.327128\n0.478212\n83.589996\n00:10\n\n\n52\n0.325685\n0.474692\n83.750000\n00:08\n\n\n53\n0.324260\n0.471305\n83.959999\n00:05\n\n\n54\n0.322876\n0.467993\n84.110001\n00:04\n\n\n55\n0.321540\n0.464754\n84.239998\n00:06\n\n\n56\n0.320235\n0.461603\n84.370003\n00:07\n\n\n57\n0.318949\n0.458543\n84.449997\n00:05\n\n\n58\n0.317698\n0.455552\n84.589996\n00:07\n\n\n59\n0.316479\n0.452645\n84.690002\n00:05\n\n\n60\n0.315262\n0.449823\n84.820000\n00:07\n\n\n61\n0.314078\n0.447053\n84.949997\n00:05\n\n\n62\n0.312930\n0.444345\n85.080002\n00:10\n\n\n63\n0.311791\n0.441711\n85.230003\n00:05\n\n\n64\n0.310679\n0.439148\n85.300003\n00:05\n\n\n65\n0.309597\n0.436635\n85.400002\n00:10\n\n\n66\n0.308528\n0.434166\n85.540001\n00:08\n\n\n67\n0.307462\n0.431768\n85.599998\n00:05\n\n\n68\n0.306423\n0.429406\n85.709999\n00:07\n\n\n69\n0.305414\n0.427090\n85.900002\n00:09\n\n\n70\n0.304411\n0.424796\n86.040001\n00:08\n\n\n71\n0.303422\n0.422561\n86.129997\n00:07\n\n\n72\n0.302441\n0.420381\n86.190002\n00:05\n\n\n73\n0.301463\n0.418240\n86.239998\n00:05\n\n\n74\n0.300526\n0.416106\n86.339996\n00:05\n\n\n75\n0.299587\n0.414013\n86.370003\n00:07\n\n\n76\n0.298673\n0.411971\n86.470001\n00:05\n\n\n77\n0.297748\n0.409977\n86.570000\n00:11\n\n\n78\n0.296841\n0.407999\n86.629997\n00:05\n\n\n79\n0.295964\n0.406036\n86.750000\n00:05\n\n\n80\n0.295090\n0.404103\n86.889999\n00:06\n\n\n81\n0.294211\n0.402208\n86.970001\n00:07\n\n\n82\n0.293342\n0.400348\n87.059998\n00:11\n\n\n83\n0.292497\n0.398527\n87.160004\n00:07\n\n\n84\n0.291661\n0.396721\n87.199997\n00:10\n\n\n85\n0.290825\n0.394935\n87.250000\n00:05\n\n\n86\n0.289988\n0.393174\n87.309998\n00:05\n\n\n87\n0.289170\n0.391404\n87.360001\n00:07\n\n\n88\n0.288363\n0.389678\n87.400002\n00:07\n\n\n89\n0.287559\n0.387958\n87.449997\n00:05\n\n\n90\n0.286770\n0.386272\n87.529999\n00:07\n\n\n91\n0.285985\n0.384583\n87.550003\n00:04\n\n\n92\n0.285202\n0.382913\n87.599998\n00:04\n\n\n93\n0.284413\n0.381289\n87.699997\n00:06\n\n\n94\n0.283646\n0.379661\n87.739998\n00:05\n\n\n95\n0.282881\n0.378050\n87.779999\n00:07\n\n\n96\n0.282145\n0.376432\n87.809998\n00:05\n\n\n97\n0.281396\n0.374831\n87.860001\n00:06\n\n\n98\n0.280646\n0.373255\n87.919998\n00:04\n\n\n99\n0.279895\n0.371665\n87.949997\n00:05\n\n\n\n\n\nAfter 100 epochs, we reached over 87% accuracy!\nSo, we completed the project. That was very fun. Thanks for reading.",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html#preparing-data",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html#preparing-data",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "This part is the same as last blog where we created a baseline by calculating means of images.\n\nfrom fastai.vision.all import *\n\n\nmatplotlib.rc('image', cmap='Greys')\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator&gt;\n\n\n\npath = untar_data(URLs.MNIST)\nPath.BASE_PATH = path\npath.ls()\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00&lt;00:00]\n    \n    \n\n\n(#2) [Path('training'),Path('testing')]\n\n\n\n(path/'training').ls()\n\n(#10) [Path('training/7'),Path('training/8'),Path('training/5'),Path('training/6'),Path('training/9'),Path('training/3'),Path('training/2'),Path('training/0'),Path('training/4'),Path('training/1')]\n\n\nI looked at the number of files for each number, and they are not the same.\n\ntrn_counts = {n: len((path/'training'/str(n)).ls()) for n in range(10)}\ntrn_counts\n\n{0: 5923,\n 1: 6742,\n 2: 5958,\n 3: 6131,\n 4: 5842,\n 5: 5421,\n 6: 5918,\n 7: 6265,\n 8: 5851,\n 9: 5949}\n\n\nSince these are paths for images we have to convert them into pytorch tensors. Pytorch provides us with broadcasting ability, which is very fast.\nFirst, we turn image paths into numbers and return as L, which is an upgraded version of list.\n\ndef create_images(path):\n    return L(PILImage.create(x) for x in path.ls())\n\n\nones = create_images(path/'training'/'1')\nim1 = ones[0]\nim1\n\n\n\n\n\n\n\n\nWe need to turn them into tensors for speed. However, we have to check the shape of tensors to make sure they are broadcastable and the shape is one we’re expecting.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs])\n\n\nstacked_ones = create_tensor_imgs(ones)\nstacked_ones[0].shape\n\ntorch.Size([28, 28, 3])\n\n\n\nstacked_ones.shape\n\ntorch.Size([6742, 28, 28, 3])\n\n\nLooking at the shape of an image, it is interesting to find out that its shape is [28, 28, 3] (y, x, and color values). I was expecting to see [28, 28] (y and x values) because it is a black and white image. So, let’s see what’s going on here.\n\narray(im1)[10:15, 12:15]\n\narray([[[  0,   0,   0],\n        [221, 221, 221],\n        [251, 251, 251]],\n\n       [[ 64,  64,  64],\n        [236, 236, 236],\n        [251, 251, 251]],\n\n       [[127, 127, 127],\n        [251, 251, 251],\n        [251, 251, 251]],\n\n       [[127, 127, 127],\n        [251, 251, 251],\n        [251, 251, 251]],\n\n       [[128, 128, 128],\n        [253, 253, 253],\n        [253, 253, 253]]], dtype=uint8)\n\n\nSo, it is just filled with the same numbers per each row. We just need one column, so we can simply take a mean of last rank.\n\narray(im1)[10:15, 12:15].mean(-1)\n\narray([[  0., 221., 251.],\n       [ 64., 236., 251.],\n       [127., 251., 251.],\n       [127., 251., 251.],\n       [128., 253., 253.]])\n\n\nSo, here is the updated version of create_tensor_imgs that calculates a mean so that we can get rid of the last rank of our tensors.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)\n\n\nstacked_ones = create_tensor_imgs(ones)\nstacked_ones[0].shape\n\ntorch.Size([28, 28])\n\n\n\narray(stacked_ones[0])[10:15, 12:15]\n\narray([[  0., 221., 251.],\n       [ 64., 236., 251.],\n       [127., 251., 251.],\n       [127., 251., 251.],\n       [128., 253., 253.]], dtype=float32)\n\n\nLooks good. Now, we want those pixel values to be between 0 and 1 so that it is easier to train neural networks.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)/255\n\n\nstacked_ones = create_tensor_imgs(ones)\narray(stacked_ones[0])[10:15, 12:15]\n\narray([[0.        , 0.8666667 , 0.9843137 ],\n       [0.2509804 , 0.9254902 , 0.9843137 ],\n       [0.49803922, 0.9843137 , 0.9843137 ],\n       [0.49803922, 0.9843137 , 0.9843137 ],\n       [0.5019608 , 0.99215686, 0.99215686]], dtype=float32)\n\n\n\ndef stacked_nums(path):\n    \"Converts path into stacked tensors.\"\n    imgs = create_images(path)\n    return create_tensor_imgs(imgs)\n\n\nstacked_zeros = stacked_nums(path/'training'/'0')\nstacked_twos = stacked_nums(path/'training'/'2')\nstacked_threes = stacked_nums(path/'training'/'3')\nstacked_fours = stacked_nums(path/'training'/'4')\nstacked_fives = stacked_nums(path/'training'/'5')\nstacked_sixs = stacked_nums(path/'training'/'6')\nstacked_sevens = stacked_nums(path/'training'/'7')\nstacked_eights = stacked_nums(path/'training'/'8')\nstacked_nines = stacked_nums(path/'training'/'9')\n\nWe also need test data to check how we’re doing. Without test set, we don’t know whether we are overfitting or not. So, we do the same process we did for test data.\n\ntst_0_tens = stacked_nums(path/'testing'/'0')\ntst_1_tens = stacked_nums(path/'testing'/'1')\ntst_2_tens = stacked_nums(path/'testing'/'2')\ntst_3_tens = stacked_nums(path/'testing'/'3')\ntst_4_tens = stacked_nums(path/'testing'/'4')\ntst_5_tens = stacked_nums(path/'testing'/'5')\ntst_6_tens = stacked_nums(path/'testing'/'6')\ntst_7_tens = stacked_nums(path/'testing'/'7')\ntst_8_tens = stacked_nums(path/'testing'/'8')\ntst_9_tens = stacked_nums(path/'testing'/'9')\n\ntst_tens = L([tst_0_tens, tst_1_tens, tst_2_tens, tst_3_tens, tst_4_tens, \n              tst_5_tens, tst_6_tens, tst_7_tens, tst_8_tens, tst_9_tens])\n\ntst_counts = {n: len((path/'testing'/str(n)).ls()) for n in range(10)}\n\ntst_1_tens.shape, tst_9_tens.shape\n\n(torch.Size([1135, 28, 28]), torch.Size([1009, 28, 28]))\n\n\n\ntst_counts\n\n{0: 980,\n 1: 1135,\n 2: 1032,\n 3: 1010,\n 4: 982,\n 5: 892,\n 6: 958,\n 7: 1028,\n 8: 974,\n 9: 1009}\n\n\nNow, we stack all the training tensors together to create trn_x. And create trn_y with the counts. We zip those together to create a dataset. We also change the shape of tensors from [28, 28] to [784]. This shape is more useful for matrix multiplication we will do later.\n\ntrn_x = torch.cat([stacked_zeros, stacked_ones, stacked_twos, \n                   stacked_threes, stacked_fours, stacked_fives, \n                   stacked_sixs, stacked_sevens, stacked_eights, \n                   stacked_nines]).view(-1, 28*28)\n\ntrn_y = tensor(list(flatten([[x] * y for x, y in trn_counts.items()])))\ntrn_dset = list(zip(trn_x, trn_y))\n\ntrn_x.shape, trn_y.shape\n\n(torch.Size([60000, 784]), torch.Size([60000]))\n\n\n\nshow_image(trn_x[50000].view(28,28)), trn_y[50000]\n\n\n\n\n\n\n\n\n\nx, y = trn_dset[0]\nx.shape, y.shape\n\n(torch.Size([784]), torch.Size([]))\n\n\nTraining data looks good, so let’s work on the test data.\n\ntst_x = torch.cat([tst_0_tens, tst_1_tens, tst_2_tens, \n                     tst_3_tens, tst_4_tens, tst_5_tens,\n                     tst_6_tens, tst_7_tens, tst_8_tens,\n                     tst_9_tens]).view(-1, 28*28)\n\ntst_y = tensor(list(flatten([[x] * y for x, y in tst_counts.items()])))\ntst_dset = list(zip(tst_x, tst_y))\n\ntst_x.shape, tst_y.shape\n\n(torch.Size([10000, 784]), torch.Size([10000]))",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html#using-neural-network",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html#using-neural-network",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "In order to use neural network, we need to initialize parameters, such as weights and bias.\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\n\nWe are initializing weights parameter with size (28*28, 10) because 28*28 is the size of our input (reshaped images) and 10 is the size of our output. Why do we return 10 numbers as an output for each image? Because we’re making predictions on each category (from 0 to 9). Out of all those 10 numbers, the biggest number is the model’s guess.\nBut couldn’t we just output 1 number instead and pick a target based on the range of our output? For instance, we could use sigmoid function to get our output numbers to be between 0 and 1, and if it’s between 0 and 0.1, it is 0, if it’s between 0.1 and 0.2, it is 1, and so on. That’s also a possible approach to this problem.\nWith those parameters, we will perform matrix multiplications. There are multiple ways to calculate it. One way involves manually transposing a matrix and getting the sum over dimension 1, and another way involves using @ operator, which does all of that for us.\n\n(x*weights.T).sum(dim=1) + bias\n\ntensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n         2.4677, 16.0547], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nx@weights + bias\n\ntensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n         2.4677, 16.0547], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ndef linear(xb): return xb@weights + bias\n\n\npreds = linear(trn_x)\npreds\n\ntensor([[ -3.0786,   6.5823,  12.8080,  ...,  -2.3099,   2.4677,  16.0547],\n        [ -4.0905,   1.3646,  -1.6264,  ...,  -5.0465,  -8.9219,   5.3742],\n        [ -4.6858,  -1.6842,   5.6141,  ...,  -7.0370,   7.6748,   1.3543],\n        ...,\n        [-15.4864,  -1.8742,  -6.2627,  ...,  -0.1359,  10.1261,  18.8826],\n        [-22.5967,   5.1747,  -6.5157,  ...,  -2.5263,  20.9854,  12.2597],\n        [-15.9537,  -2.7749,   3.1555,  ...,   5.3508,  11.7086,  13.8179]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\nSo, we know that these numbers represent “socres” for each category. Let’s look at the first prediction.\n\npreds[0]\n\ntensor([-3.0786,  6.5823, 12.8080, 10.5792, -8.4876, 12.2374,  7.8472, -2.3099,\n         2.4677, 16.0547], grad_fn=&lt;SelectBackward0&gt;)\n\n\nFrom these numbers, the biggest number is 16.0547, which is a prediction for 9. So, this predicted 9. We can use torch.argmax to calculate the maximum index easily.\n\npreds[0].argmax()\n\ntensor(9)\n\n\nFor multiple predictions, we can specify the dimension to be 1.\n\npreds.argmax(dim=1)\n\ntensor([9, 5, 5,  ..., 9, 8, 9])\n\n\nWith predictions on categories, we can calculate accuracy by comparing it to the targets (trn_y).\n\n(preds.argmax(dim=1) == trn_y).float().mean() * 100\n\ntensor(12.0433)\n\n\nWe have about 10% accuracy since we are using randomly initialized parameters to predict.\n\ndef acc(pred, targ):\n    return (pred.argmax(dim=1) == targ).float().mean() * 100\n\n\nacc(preds, trn_y)\n\ntensor(12.0433)",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html#cross-entropy-loss",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html#cross-entropy-loss",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "In order to improve our accuracy, we have to tweak our weights and biases (parameters). To improve our parameters, we have to use a loss function. Then, we can calculate gradients and use them to update our parameters. Loss functions are just mathematical functions, but they have smooth shapes. This is important because gradients cannot be calculated on sharp edges. Let’s take look at a sigmoid function.\n\ndef plot_function(fn, title=None, xmin=-4.1, xmax=4.1):\n    x = torch.linspace(xmin, xmax, 20)\n    plt.plot(x, fn(x))\n    if title: plt.title(title)\n\n\nplot_function(torch.nn.Sigmoid(), title='Sigmoid function')\n\n\n\n\n\n\n\n\nSigmoid function has no sharp edge, so gradients can be found anywhere. This will also ensure that predictions will be between 0 and 1. We typically use sigmoid function when we try to predict boolean values. For instance, we could use this function for problems like “is it a cat or dog?”\nBecause we have multiple categories to predict, we use cross entropy loss. Cross entropy loss can be thought of as sigmoid designed for multiple categories. Cross entropy loss is composed of multiple functions. First, we use a softmax function ensures that all numbers are positive and add up to 1. Also, this function takes exponential of all values, so even the slight difference will be magnified. This will result in choosing one class with more confidence. Second, we take the log of the activations we got from softmax. Let’s look at a log function.\n\nplot_function(torch.log, title=\"Log function\", xmin=-0.1, xmax=1)\n\n\n\n\n\n\n\n\nIn log function, we get a very big negative value if a number gets closer to 0, and reaches 0 when it gets 1. So, if our activation is close to 0 after softmax, it will be a big negative number after taking a log. This will result in a big gradient. However, if the number is close to 1, it is good enough. So, it will punish numbers that are far off, but forgiving for close numbers. But we still have problems. We still have 10 activation numbers and all of them are negative.\nThe last function we will use is negative log likelihood, which picks one number from our 10 activations and multiply by -1 to make it a positive value. Although it has log in its name, it actually does not take the log function. We have to do it ourselves. Let’s take a look at how it looks like.\n\nsm_acts = torch.nn.Softmax(dim=1)(preds)\nsm_acts[0]\n\ntensor([4.6022e-09, 7.2222e-05, 3.6513e-02, 3.9309e-03, 2.0602e-11, 2.0636e-02,\n        2.5585e-04, 9.9276e-09, 1.1795e-06, 9.3859e-01],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nlog_acts = torch.log(sm_acts)\nlog_acts[0]\n\ntensor([-19.1967,  -9.5358,  -3.3101,  -5.5389, -24.6057,  -3.8807,  -8.2709,\n        -18.4279, -13.6504,  -0.0634], grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ntrn_y[0]\n\ntensor(0)\n\n\n\ntorch.nn.NLLLoss()(log_acts[0], trn_y[0])\n\ntensor(19.1967, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nNotice that we get the same values for the built in cross entropy loss function.\n\ntorch.nn.NLLLoss()(log_acts, trn_y)\n\ntensor(14.3907, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nloss = torch.nn.CrossEntropyLoss()(preds, trn_y)\nloss\n\ntensor(14.3907, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nIt’s easier to use the built in function, and it’s also faster.\n\n\n\n15.5 ms ± 627 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n21.7 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html#calculating-gradients",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html#calculating-gradients",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "So, we calculated the loss. With loss, we can calculate the gradients by calling backward(). Then, we can take a look at them.\n\nloss.backward()\n\n\nbias.grad, weights.grad.sum()\n\n(tensor([-9.3132e-09]), tensor(-1.0490e-05))\n\n\nWe can define a function it.\n\ndef calc_grad(xb, yb, model):\n    \"Calculate the gradients\"\n    preds = model(xb)\n    loss = torch.nn.CrossEntropyLoss()(preds, yb)\n    loss.backward()\n    return loss\n\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\n\n\ncalc_grad(trn_x, trn_y, linear)\nweights.grad.mean(), bias.grad.mean()\n\n(tensor(-1.2164e-10), tensor(-9.3132e-09))\n\n\nAfter calculating the gradients, we can update our parameters using the gradients. We can multiply parameter’s gradient by any number, which is referred to as a learning rate. If it’s big, it will train faster, and if it’s slow, it will train slower. However, if it’s too big, it cannot train. So, we can try couple numbers and see what works and what doensn’t. After updating, we have to reset the gradients. If we don’t, gradients will keep growing.\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\ncalc_grad(trn_x, trn_y, linear)\nbias.grad, weights.grad.sum()\n\n(tensor([-1.3039e-08]), tensor(-9.5367e-07))\n\n\n\ncalc_grad(trn_x, trn_y, linear)\nbias.grad, weights.grad.sum()\n\n(tensor([-2.6077e-08]), tensor(-1.9073e-06))\n\n\nSee how it grew? we have to use zero_() on the gradients. Training _ on functions means in place function.\n\nweights.data -= weights.grad * 10\nbias.data -= bias.grad * 10\nweights.grad.zero_()\nbias.grad.zero_()\n\ntensor([0.])",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html#first-training",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html#first-training",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "If we put everything together, this is what it looks like. We will train with the whole data 20 times, which means we are using 20 epochs.\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\nfor _ in range(20):\n    calc_grad(trn_x, trn_y, linear)\n    weights.data -= weights.grad * 10\n    weights.grad.zero_()\n    bias.data -= bias.grad * 10\n    bias.grad.zero_()\n    pred = linear(tst_x)\n    print(acc(pred, tst_y))\n\ntensor(20.9500)\ntensor(38.9100)\ntensor(40.2700)\ntensor(48.3800)\ntensor(42.6600)\ntensor(51.5200)\ntensor(45.4200)\ntensor(57.6000)\ntensor(60.3500)\ntensor(62.2100)\ntensor(67.2600)\ntensor(70.8800)\ntensor(75.0300)\ntensor(71.8100)\ntensor(76.1800)\ntensor(82.2500)\ntensor(85.1200)\ntensor(85.3800)\ntensor(85.1700)\ntensor(85.1200)\n\n\nWe got over 80% accuracy, which is better tan our baseline using means of images from the last time, which had 66% accuracy. However, we can still make this better.",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html#minibatches",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html#minibatches",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "We updated our parameters after looking at the whole dataset. The MNIST dataset is very small, so it’s not a problem, but dataset can get really big, and it would take a long time to train this way. Another approach to train is update parameters after each image. This is not a good idea either as our model learns about one image at a time, which can lead to overfitting. That’s why we use small batches of data.\n\ndl = DataLoader(trn_dset, bs=256)\nxb, yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256]))\n\n\nWith fastai’s DataLoader, we will have 256 items per batch. And we will do the same thing for the test set.\n\ntst_dl = DataLoader(tst_dset, bs=256)\n\n\ndef train(lr=1.):\n    params = weights, bias\n    for xb, yb in dl:\n        calc_grad(xb, yb, linear)\n        for p in params:\n            p.data -= p.grad * lr\n            p.grad.zero_()\n\n\ndef validate_epoch(model):\n    accs = [acc(model(xb), yb) for xb, yb in tst_dl]\n    return round(tensor(accs).mean().item(), 4)\n\n\nweights = init_params((28*28, 10))\nbias = init_params(1)\nfor _ in range(30):\n    train(0.1)\n    print(validate_epoch(linear), end='; ')\n\n15.9961; 27.8516; 39.1016; 46.3672; 51.3281; 54.8535; 57.2656; 59.4922; 61.1328; 62.5098; 63.6328; 64.5605; 65.2637; 65.9766; 66.6699; 67.2754; 67.9004; 68.418; 68.9355; 69.3164; 69.6777; 70.0586; 70.3809; 70.625; 70.8789; 71.1328; 71.3379; 71.5039; 71.6602; 71.8262; \n\n\nAccuracy is not as good as using the whole dataset, but let’s keep trying more things.",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-25-MNIST_NN/index.html#optimizer",
    "href": "blog/posts/2023-02-25-MNIST_NN/index.html#optimizer",
    "title": "MNIST Neural Nets",
    "section": "",
    "text": "We can use pytorch’s torch.nn.Linear instead of our linear. Good thing about using this is that we can get grab parameters easily.\n\nlinear_model = torch.nn.Linear(28*28, 10)\n\n\nw, b = list(linear_model.parameters())\nw.shape, b.shape\n\n(torch.Size([10, 784]), torch.Size([10]))\n\n\nWe define a BasicOptim class that has step and zero_grad methods.\n\nclass BasicOptim:\n    def __init__(self, params, lr=0.01):\n        self.params = params\n        self.lr = lr\n    \n    def step(self):\n        for p in self.params:\n            p.data -= p.grad * self.lr\n    \n    def zero_grad(self):\n        for p in self.params:\n            p.grad.zero_()\n\nOptimizer can be used this way. And we can update our train function to use the optimizer.\n\nopt = BasicOptim(list(linear_model.parameters()))\n\n\ndef train(model):\n    for xb, yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\nIt’s good to define a function to train a model.\n\ndef train_model(epochs, model):\n    for _ in range(epochs):\n        train(model)\n        print(validate_epoch(model))\n\n\ntrain_model(20, linear_model)\n\n12.2266\n31.3184\n46.7578\n55.4785\n61.2988\n65.1953\n68.4375\n70.9473\n72.9297\n74.8047\n76.25\n77.5391\n78.6035\n79.6094\n80.3027\n80.9473\n81.6016\n82.2754\n82.8027\n83.2031\n\n\nWe can also use fastai’s SGD.\n\nlinear_model = nn.Linear(28*28, 10)\nopt = SGD(list(linear_model.parameters()), .01)\n\n\ntrain_model(20, linear_model)\n\n12.2461\n32.4121\n47.0117\n55.4004\n61.1621\n65.166\n68.418\n70.8398\n72.9297\n75.1172\n76.377\n77.5488\n78.623\n79.4922\n80.3809\n81.0547\n81.582\n82.2363\n82.7148\n83.0664\n\n\nAlso, we can use fastai’s learner. We just need DataLoaders, which combines training dataloaders with test dataloaders. By using learn.fit, we can train easily and get a nice output, too.\n\ndls = DataLoaders(dl, tst_dl)\n\n\nlearn = Learner(dls, nn.Linear(28*28, 10), loss_func=nn.CrossEntropyLoss(), opt_func=SGD, metrics=acc)\n\n\nlearn.fit(20, 0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc\ntime\n\n\n\n\n0\n1.030682\n2.442845\n10.260000\n00:03\n\n\n1\n0.756178\n1.755957\n30.840000\n00:04\n\n\n2\n0.652057\n1.387471\n45.750000\n00:04\n\n\n3\n0.593682\n1.172395\n54.570000\n00:03\n\n\n4\n0.555542\n1.030824\n60.259998\n00:06\n\n\n5\n0.528407\n0.930011\n64.510002\n00:05\n\n\n6\n0.507978\n0.854310\n67.820000\n00:06\n\n\n7\n0.491943\n0.795262\n70.309998\n00:03\n\n\n8\n0.478946\n0.747864\n72.440002\n00:05\n\n\n9\n0.468138\n0.708950\n74.440002\n00:03\n\n\n10\n0.458963\n0.676414\n75.860001\n00:06\n\n\n11\n0.451041\n0.648797\n77.139999\n00:03\n\n\n12\n0.444104\n0.625053\n78.199997\n00:05\n\n\n13\n0.437957\n0.604415\n79.160004\n00:06\n\n\n14\n0.432457\n0.586306\n80.010002\n00:04\n\n\n15\n0.427493\n0.570282\n80.739998\n00:05\n\n\n16\n0.422980\n0.555999\n81.290001\n00:03\n\n\n17\n0.418851\n0.543184\n82.040001\n00:05\n\n\n18\n0.415051\n0.531618\n82.360001\n00:05\n\n\n19\n0.411538\n0.521124\n82.720001\n00:04\n\n\n\n\n\nWe can improve this accuracy with adding another linear layers and a relu layer. We create a deeper network as long as there’s a relu layer in between each linear layer. Relu layer simply turns negative numbers to 0.\n\nnet = nn.Sequential(\n    nn.Linear(28*28, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10)\n)\n\nWe made a network of layers. Since it is deeper, we have to reduce the learning rate, which results in slower training speed. However, we can train for more epochs. Let’s try training for 100 epochs.\n\nlearn = Learner(dls, net, loss_func=nn.CrossEntropyLoss(), \n                opt_func=SGD, metrics=acc)\nlearn.fit(100, 0.004)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nacc\ntime\n\n\n\n\n0\n2.257359\n2.186582\n28.950001\n00:04\n\n\n1\n1.927473\n2.062460\n16.990000\n00:05\n\n\n2\n1.586289\n1.925866\n19.389999\n00:07\n\n\n3\n1.296686\n1.757989\n29.430000\n00:04\n\n\n4\n1.077161\n1.585793\n37.320000\n00:06\n\n\n5\n0.920138\n1.439800\n42.250000\n00:07\n\n\n6\n0.809401\n1.322030\n45.910000\n00:08\n\n\n7\n0.729531\n1.226982\n48.950001\n00:04\n\n\n8\n0.669996\n1.148582\n52.099998\n00:06\n\n\n9\n0.624211\n1.082637\n54.900002\n00:05\n\n\n10\n0.587927\n1.026398\n57.480000\n00:05\n\n\n11\n0.558507\n0.977675\n59.820000\n00:08\n\n\n12\n0.534146\n0.934994\n62.150002\n00:07\n\n\n13\n0.513631\n0.897281\n63.990002\n00:07\n\n\n14\n0.496094\n0.863748\n65.589996\n00:07\n\n\n15\n0.480910\n0.833677\n66.970001\n00:08\n\n\n16\n0.467640\n0.806602\n68.199997\n00:11\n\n\n17\n0.455922\n0.782081\n69.519997\n00:09\n\n\n18\n0.445503\n0.759768\n70.489998\n00:07\n\n\n19\n0.436185\n0.739375\n71.389999\n00:07\n\n\n20\n0.427801\n0.720681\n72.199997\n00:05\n\n\n21\n0.420199\n0.703508\n73.080002\n00:07\n\n\n22\n0.413270\n0.687696\n73.760002\n00:06\n\n\n23\n0.406932\n0.673062\n74.489998\n00:05\n\n\n24\n0.401142\n0.659454\n75.080002\n00:05\n\n\n25\n0.395792\n0.646811\n75.730003\n00:05\n\n\n26\n0.390829\n0.635046\n76.250000\n00:05\n\n\n27\n0.386238\n0.624030\n76.760002\n00:05\n\n\n28\n0.381959\n0.613709\n77.419998\n00:05\n\n\n29\n0.377980\n0.604007\n77.910004\n00:09\n\n\n30\n0.374245\n0.594876\n78.379997\n00:07\n\n\n31\n0.370748\n0.586283\n78.849998\n00:07\n\n\n32\n0.367451\n0.578153\n79.239998\n00:05\n\n\n33\n0.364345\n0.570461\n79.610001\n00:06\n\n\n34\n0.361401\n0.563176\n79.919998\n00:05\n\n\n35\n0.358619\n0.556240\n80.260002\n00:05\n\n\n36\n0.355972\n0.549619\n80.610001\n00:05\n\n\n37\n0.353456\n0.543326\n80.839996\n00:05\n\n\n38\n0.351051\n0.537354\n81.120003\n00:05\n\n\n39\n0.348749\n0.531646\n81.330002\n00:05\n\n\n40\n0.346539\n0.526188\n81.589996\n00:05\n\n\n41\n0.344433\n0.520945\n81.839996\n00:09\n\n\n42\n0.342406\n0.515914\n82.000000\n00:04\n\n\n43\n0.340465\n0.511089\n82.190002\n00:07\n\n\n44\n0.338599\n0.506455\n82.290001\n00:06\n\n\n45\n0.336791\n0.501979\n82.480003\n00:05\n\n\n46\n0.335052\n0.497672\n82.660004\n00:05\n\n\n47\n0.333371\n0.493508\n82.809998\n00:05\n\n\n48\n0.331753\n0.489475\n83.040001\n00:06\n\n\n49\n0.330158\n0.485592\n83.220001\n00:05\n\n\n50\n0.328622\n0.481835\n83.430000\n00:06\n\n\n51\n0.327128\n0.478212\n83.589996\n00:10\n\n\n52\n0.325685\n0.474692\n83.750000\n00:08\n\n\n53\n0.324260\n0.471305\n83.959999\n00:05\n\n\n54\n0.322876\n0.467993\n84.110001\n00:04\n\n\n55\n0.321540\n0.464754\n84.239998\n00:06\n\n\n56\n0.320235\n0.461603\n84.370003\n00:07\n\n\n57\n0.318949\n0.458543\n84.449997\n00:05\n\n\n58\n0.317698\n0.455552\n84.589996\n00:07\n\n\n59\n0.316479\n0.452645\n84.690002\n00:05\n\n\n60\n0.315262\n0.449823\n84.820000\n00:07\n\n\n61\n0.314078\n0.447053\n84.949997\n00:05\n\n\n62\n0.312930\n0.444345\n85.080002\n00:10\n\n\n63\n0.311791\n0.441711\n85.230003\n00:05\n\n\n64\n0.310679\n0.439148\n85.300003\n00:05\n\n\n65\n0.309597\n0.436635\n85.400002\n00:10\n\n\n66\n0.308528\n0.434166\n85.540001\n00:08\n\n\n67\n0.307462\n0.431768\n85.599998\n00:05\n\n\n68\n0.306423\n0.429406\n85.709999\n00:07\n\n\n69\n0.305414\n0.427090\n85.900002\n00:09\n\n\n70\n0.304411\n0.424796\n86.040001\n00:08\n\n\n71\n0.303422\n0.422561\n86.129997\n00:07\n\n\n72\n0.302441\n0.420381\n86.190002\n00:05\n\n\n73\n0.301463\n0.418240\n86.239998\n00:05\n\n\n74\n0.300526\n0.416106\n86.339996\n00:05\n\n\n75\n0.299587\n0.414013\n86.370003\n00:07\n\n\n76\n0.298673\n0.411971\n86.470001\n00:05\n\n\n77\n0.297748\n0.409977\n86.570000\n00:11\n\n\n78\n0.296841\n0.407999\n86.629997\n00:05\n\n\n79\n0.295964\n0.406036\n86.750000\n00:05\n\n\n80\n0.295090\n0.404103\n86.889999\n00:06\n\n\n81\n0.294211\n0.402208\n86.970001\n00:07\n\n\n82\n0.293342\n0.400348\n87.059998\n00:11\n\n\n83\n0.292497\n0.398527\n87.160004\n00:07\n\n\n84\n0.291661\n0.396721\n87.199997\n00:10\n\n\n85\n0.290825\n0.394935\n87.250000\n00:05\n\n\n86\n0.289988\n0.393174\n87.309998\n00:05\n\n\n87\n0.289170\n0.391404\n87.360001\n00:07\n\n\n88\n0.288363\n0.389678\n87.400002\n00:07\n\n\n89\n0.287559\n0.387958\n87.449997\n00:05\n\n\n90\n0.286770\n0.386272\n87.529999\n00:07\n\n\n91\n0.285985\n0.384583\n87.550003\n00:04\n\n\n92\n0.285202\n0.382913\n87.599998\n00:04\n\n\n93\n0.284413\n0.381289\n87.699997\n00:06\n\n\n94\n0.283646\n0.379661\n87.739998\n00:05\n\n\n95\n0.282881\n0.378050\n87.779999\n00:07\n\n\n96\n0.282145\n0.376432\n87.809998\n00:05\n\n\n97\n0.281396\n0.374831\n87.860001\n00:06\n\n\n98\n0.280646\n0.373255\n87.919998\n00:04\n\n\n99\n0.279895\n0.371665\n87.949997\n00:05\n\n\n\n\n\nAfter 100 epochs, we reached over 87% accuracy!\nSo, we completed the project. That was very fun. Thanks for reading.",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST Neural Nets"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-06-paddy1/index.html",
    "href": "blog/posts/2022-12-06-paddy1/index.html",
    "title": "Intro to Paddy competition",
    "section": "",
    "text": "We will get started with Paddy competition from Kaggle.\n\n\n\nPaddy competition\n\n\nLast blog, we went over how to setup kaggle API and download data. We will use the data from last time and create a submission. At this point, our goal is not to get to the top of the submissions, but to create a baseline. We just want our model to train reasonably well, so that we can make improvements on it later.\nIf you want to follow along or get a copy of this notebook, you can grab it here.\n\nfrom fastai.vision.all import *\n\n\npath = Path()\npath.ls()\n\n(#33) [Path('subm_res18_45ep_tta.csv'),Path('11_blog_base-Copy1.ipynb'),Path('README.md'),Path('subm_res18_15ep_tta.csv'),Path('11_blog_base.ipynb'),Path('subm_res18_10ep_tta.csv'),Path('sample_submission.csv'),Path('subm_res18_40ep_tta.csv'),Path('subm_res18_35ep_tta.csv'),Path('subm_res18_5ep_tta.csv')...]\n\n\n\n\nBefore we do anything, we can look at the data to find out how they are structured. It’s helpful to read data section of the competition as well.\n\ntrn_path = path/'train_images'\ntrn_path.ls()\n\n(#12) [Path('train_images/res34_12.pkl'),Path('train_images/bacterial_panicle_blight'),Path('train_images/dead_heart'),Path('train_images/tungro'),Path('train_images/hispa'),Path('train_images/normal'),Path('train_images/bacterial_leaf_streak'),Path('train_images/brown_spot'),Path('train_images/bacterial_leaf_blight'),Path('train_images/downy_mildew')...]\n\n\nUsing get_image_files, we can get all the image files in the directory recursively. We can see that there are different folders within train_images folder. We can also see that these are labels, such as normal, bacterial_penicle_blight, and others. Within those labeling folders, there are images.\n\nfiles = get_image_files(trn_path)\nfiles\n\n(#10407) [Path('train_images/bacterial_panicle_blight/109162.jpg'),Path('train_images/bacterial_panicle_blight/109183.jpg'),Path('train_images/bacterial_panicle_blight/101765.jpg'),Path('train_images/bacterial_panicle_blight/101592.jpg'),Path('train_images/bacterial_panicle_blight/104770.jpg'),Path('train_images/bacterial_panicle_blight/110190.jpg'),Path('train_images/bacterial_panicle_blight/106643.jpg'),Path('train_images/bacterial_panicle_blight/104269.jpg'),Path('train_images/bacterial_panicle_blight/108023.jpg'),Path('train_images/bacterial_panicle_blight/107676.jpg')...]\n\n\n\npd.read_csv('train.csv').head()\n\n\n\n\n\n\n\n\nimage_id\nlabel\nvariety\nage\n\n\n\n\n0\n100330.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n1\n100365.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n2\n100382.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n3\n100632.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n4\n101918.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n\n\n\n\n\nLet’s now look at an image and its size. It is (192, 256). We can check sizes of others as well.\n\nim = PILImage.create(files[0])\nprint(im.size)\nim.to_thumb(244)\n\n(192, 256)\n\n\n\n\n\n\n\n\n\nWe can check sizes of all the images fast using parallel. It has some restrictions, but it makes it faster. However, it is not guaranteed to be fast as overhead can be from Input/Output of files. For instance, in paperspace instance, it took almost the same amount of time whether I used parallel or not because my files were in cloud. So, it was taking a long time due to internet, not CPU.\n\nfrom fastcore.parallel import *\n\ndef get_img_size(path):\n    return PILImage.create(path).size\n\nsizes = parallel(get_img_size, files)\nsizes\n\n(#10407) [(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256)...]\n\n\n\n\n\n\n\n\nTipGetting help\n\n\n\nWe can use ? or ?? to find out more about an object whether it’s a function or a class. To find out what parallel does, we can type parallel? to look at its documentation, and parallel?? to read the source code. Another way is using doc. It works only on fastai things, but using doc(parallel) can be helpful as well.\n\n\n\nsizes = pd.Series(sizes)\nsizes.value_counts()\n\n(192, 256)    10403\n(256, 192)        4\ndtype: int64\n\n\nExcept for 4 images, everything had the same sizes.\n\n\n\nNow, we can create an Image Dataloaders from the folders. It is very easy with fastai. We only have to provide path to look for data, how to split validation and train data, and how to transform images. fastai figures out everything else for us, such as how to process images and labels and creating batches.\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(size=(244)),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n)\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\nIt’s very important to look at images here and see whether they are labeled correctly. If these are not correctly labelled, we have to go back and fix path or whatever that was wrong. If it is wrong, even if we train it, it won’t give us good results.\nOne problem is that I don’t know anything about plant disease. I can google it to learn about them, but it will take some time. So, I decided to just go ahead and train. I can come back if it doesn’t train right.\n\n\n\nWe are using resnet34, which is not a great one, but it’s good enough to figure out whether we are doing good, and it’s fairly fast. We just want to get a model that works at this point.\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nFor most problems, we can use learning rates from 0.008 to 0.01. But is that really the case? We can find out the learning rate to be sure.\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.001737800776027143)\n\n\n\n\n\n\n\n\n\nI will use 0.008 to be safe as 0.01 looks like it’s on the very bottom. I started training with 5 epochs, and it seemed okay, so I increased to 12, but I could have just trained for 3 epochs.\n\nlearn.fine_tune(12, base_lr=0.008)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.089899\n1.361343\n0.447862\n00:33\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.948025\n0.553810\n0.174916\n00:43\n\n\n1\n0.672299\n0.479599\n0.160500\n00:44\n\n\n2\n0.579641\n0.530578\n0.172033\n00:43\n\n\n3\n0.523039\n0.450268\n0.136953\n00:43\n\n\n4\n0.459746\n0.452649\n0.124459\n00:43\n\n\n5\n0.375885\n0.333700\n0.098030\n00:43\n\n\n6\n0.261719\n0.249268\n0.072561\n00:44\n\n\n7\n0.197946\n0.174411\n0.053340\n00:43\n\n\n8\n0.142218\n0.145764\n0.042768\n00:43\n\n\n9\n0.092104\n0.118818\n0.032196\n00:43\n\n\n10\n0.065687\n0.105032\n0.030274\n00:43\n\n\n11\n0.058354\n0.107842\n0.028352\n00:43\n\n\n\n\n\nWe use fine_tune to train because we are using transfer learning. This means that we are using a pre-trained model, such as resnet34 or convnext, instead of creating a model from scratch. This saves us a lot of time because those pre-trained models can recognize some features arleady.\nWe can think about why this is true by thinking about playing a dodge ball game. Let’s say there are some players who have played many sports, such as basketball, baseball, and others, and others who have never played anything at all. Who will most likely to play better? Those who played sports will. They are more likely to be athetically fit. In order for those without athletic experience to learn all the skills, it would take some time. Same thing here. resnet34 is already good at recognizing images, so why would we create a new model from scratch?\nHowever, rules and required skills for dodgeball is different from other sports. We want to be good at dodgeball, so that’s where fastai’s fine_tune comes in. It basically makes our players to “forget” all the rules specific to other sports and “learn” new dodgeball rules! This concept is very important for deep learning, so we may comeback to this later. To learn about how it’s done in code, we can look at the documentation and source code.\n\ndoc(learn.fine_tune)\n\n\n\nLearner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, start_epoch=0)Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\nShow in docs\n\n\nAnother place to learn more about transfer learning is fastbook chapter 5.\nSince we are done with training, we can create a pickled file. It’s basically saving our learner so that we can make predictions with this later without training it again. This comes in handy when we want to train many different models and choose good ones.\n\nlearn.export('res34_12.pkl')\n\n\n\n\nTo create a submission, we have to create a csv file with correct labels based on image_ids. Here’s a sample.\n\nss = pd.read_csv('sample_submission.csv')\nss\n\n\n\n\n\n\n\n\nimage_id\nlabel\n\n\n\n\n0\n200001.jpg\nNaN\n\n\n1\n200002.jpg\nNaN\n\n\n2\n200003.jpg\nNaN\n\n\n3\n200004.jpg\nNaN\n\n\n4\n200005.jpg\nNaN\n\n\n...\n...\n...\n\n\n3464\n203465.jpg\nNaN\n\n\n3465\n203466.jpg\nNaN\n\n\n3466\n203467.jpg\nNaN\n\n\n3467\n203468.jpg\nNaN\n\n\n3468\n203469.jpg\nNaN\n\n\n\n\n3469 rows × 2 columns\n\n\n\nThis is where test images come in. We have to sort this to match the order with image_ids.\n\ntest_path = get_image_files(path/'test_images').sorted()\ntest_path\n\n(#3469) [Path('test_images/200001.jpg'),Path('test_images/200002.jpg'),Path('test_images/200003.jpg'),Path('test_images/200004.jpg'),Path('test_images/200005.jpg'),Path('test_images/200006.jpg'),Path('test_images/200007.jpg'),Path('test_images/200008.jpg'),Path('test_images/200009.jpg'),Path('test_images/200010.jpg')...]\n\n\nWe then add it to our dataloader as test_dl.\n\ntest_dl = dls.test_dl(test_path)\ntest_dl\n\n&lt;fastai.data.core.TfmdDL&gt;\n\n\nWith a learner, we can make predictions like this. Using with_decoded=True, we can optionally get indexes of the predictions. However, we need actual labels in the submission csv file instead of indexes. So we have to convert the indexes into labels.\n\nprobs,_,idxs = learn.get_preds(dl=test_dl, with_decoded=True)\n\n\n\n\n\n\n\n\n\nidxs\n\nTensorBase([7, 8, 3,  ..., 8, 1, 5])\n\n\nTo check the labels, we can look at it like this:\n\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\nOne way to convert indexes into labels is using pandas Series. So, we turn the indexes into padas Series.\n\nidxs = pd.Series(idxs)\nidxs\n\n0       7\n1       8\n2       3\n3       3\n4       3\n       ..\n3464    5\n3465    7\n3466    8\n3467    1\n3468    5\nLength: 3469, dtype: int64\n\n\nWe need to create mappings with indexes and labels. Then, we use the mappings to convert indexes into labels.\n\nmapping = dict(enumerate(dls.vocab))\nmapping\n\n{0: 'bacterial_leaf_blight',\n 1: 'bacterial_leaf_streak',\n 2: 'bacterial_panicle_blight',\n 3: 'blast',\n 4: 'brown_spot',\n 5: 'dead_heart',\n 6: 'downy_mildew',\n 7: 'hispa',\n 8: 'normal',\n 9: 'tungro'}\n\n\n\nresults = idxs.map(mapping)\nresults\n\n0                       hispa\n1                      normal\n2                       blast\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nLength: 3469, dtype: object\n\n\nThen, we can put the labels into our submissions.\n\nss.label = results\nss\n\n\n\n\n\n\n\n\nimage_id\nlabel\n\n\n\n\n0\n200001.jpg\nhispa\n\n\n1\n200002.jpg\nnormal\n\n\n2\n200003.jpg\nblast\n\n\n3\n200004.jpg\nblast\n\n\n4\n200005.jpg\nblast\n\n\n...\n...\n...\n\n\n3464\n203465.jpg\ndead_heart\n\n\n3465\n203466.jpg\nhispa\n\n\n3466\n203467.jpg\nnormal\n\n\n3467\n203468.jpg\nbacterial_leaf_streak\n\n\n3468\n203469.jpg\ndead_heart\n\n\n\n\n3469 rows × 2 columns\n\n\n\n\nss.to_csv('subm.csv', index=False)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nLooks good. We can submit it by typing kaggle competitions submit -f subm.csv -m 'init resnet34 12 epoch ft' paddy-disease-classification. -f subm.csv specifies the submission file and -m 'init resnet34 12 epoch ft' adds a message to the submission. It means initial submission using resnet34 with 12 epochs of training using fine tune. But you can put any message you want here.\nOn Paddy competition submissions, you can check your score. Then, head over to leaderboard. For the first model, it’s not bad.\n\n\n\nWe went over how to train and create a submission to a kaggle competetition. Now that we have a baseline, we can start improving our model. Thanks to this baseline, we can figure out whether new strategies are working or not.\nHelpful resources:\n\nLive coding 7: Live coding video by Jeremy Howard. First time going over paddy competition.\nLive coding 8: Live coding video by Jeremy Howard.\nfastai course: Course covering basics to advanced deep learning.\nPython for data analysis: A book covering tools for python data analysis. Mostly covering Pandas, but it also covers numpy and matplotlib.",
    "crumbs": [
      "Home",
      "Blog",
      "Intro to Paddy competition"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-06-paddy1/index.html#explore-data",
    "href": "blog/posts/2022-12-06-paddy1/index.html#explore-data",
    "title": "Intro to Paddy competition",
    "section": "",
    "text": "Before we do anything, we can look at the data to find out how they are structured. It’s helpful to read data section of the competition as well.\n\ntrn_path = path/'train_images'\ntrn_path.ls()\n\n(#12) [Path('train_images/res34_12.pkl'),Path('train_images/bacterial_panicle_blight'),Path('train_images/dead_heart'),Path('train_images/tungro'),Path('train_images/hispa'),Path('train_images/normal'),Path('train_images/bacterial_leaf_streak'),Path('train_images/brown_spot'),Path('train_images/bacterial_leaf_blight'),Path('train_images/downy_mildew')...]\n\n\nUsing get_image_files, we can get all the image files in the directory recursively. We can see that there are different folders within train_images folder. We can also see that these are labels, such as normal, bacterial_penicle_blight, and others. Within those labeling folders, there are images.\n\nfiles = get_image_files(trn_path)\nfiles\n\n(#10407) [Path('train_images/bacterial_panicle_blight/109162.jpg'),Path('train_images/bacterial_panicle_blight/109183.jpg'),Path('train_images/bacterial_panicle_blight/101765.jpg'),Path('train_images/bacterial_panicle_blight/101592.jpg'),Path('train_images/bacterial_panicle_blight/104770.jpg'),Path('train_images/bacterial_panicle_blight/110190.jpg'),Path('train_images/bacterial_panicle_blight/106643.jpg'),Path('train_images/bacterial_panicle_blight/104269.jpg'),Path('train_images/bacterial_panicle_blight/108023.jpg'),Path('train_images/bacterial_panicle_blight/107676.jpg')...]\n\n\n\npd.read_csv('train.csv').head()\n\n\n\n\n\n\n\n\nimage_id\nlabel\nvariety\nage\n\n\n\n\n0\n100330.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n1\n100365.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n2\n100382.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n3\n100632.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n4\n101918.jpg\nbacterial_leaf_blight\nADT45\n45\n\n\n\n\n\n\n\nLet’s now look at an image and its size. It is (192, 256). We can check sizes of others as well.\n\nim = PILImage.create(files[0])\nprint(im.size)\nim.to_thumb(244)\n\n(192, 256)\n\n\n\n\n\n\n\n\n\nWe can check sizes of all the images fast using parallel. It has some restrictions, but it makes it faster. However, it is not guaranteed to be fast as overhead can be from Input/Output of files. For instance, in paperspace instance, it took almost the same amount of time whether I used parallel or not because my files were in cloud. So, it was taking a long time due to internet, not CPU.\n\nfrom fastcore.parallel import *\n\ndef get_img_size(path):\n    return PILImage.create(path).size\n\nsizes = parallel(get_img_size, files)\nsizes\n\n(#10407) [(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256),(192, 256)...]\n\n\n\n\n\n\n\n\nTipGetting help\n\n\n\nWe can use ? or ?? to find out more about an object whether it’s a function or a class. To find out what parallel does, we can type parallel? to look at its documentation, and parallel?? to read the source code. Another way is using doc. It works only on fastai things, but using doc(parallel) can be helpful as well.\n\n\n\nsizes = pd.Series(sizes)\nsizes.value_counts()\n\n(192, 256)    10403\n(256, 192)        4\ndtype: int64\n\n\nExcept for 4 images, everything had the same sizes.",
    "crumbs": [
      "Home",
      "Blog",
      "Intro to Paddy competition"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-06-paddy1/index.html#create-dataloaders",
    "href": "blog/posts/2022-12-06-paddy1/index.html#create-dataloaders",
    "title": "Intro to Paddy competition",
    "section": "",
    "text": "Now, we can create an Image Dataloaders from the folders. It is very easy with fastai. We only have to provide path to look for data, how to split validation and train data, and how to transform images. fastai figures out everything else for us, such as how to process images and labels and creating batches.\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(size=(244)),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n)\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\nIt’s very important to look at images here and see whether they are labeled correctly. If these are not correctly labelled, we have to go back and fix path or whatever that was wrong. If it is wrong, even if we train it, it won’t give us good results.\nOne problem is that I don’t know anything about plant disease. I can google it to learn about them, but it will take some time. So, I decided to just go ahead and train. I can come back if it doesn’t train right.",
    "crumbs": [
      "Home",
      "Blog",
      "Intro to Paddy competition"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-06-paddy1/index.html#learner",
    "href": "blog/posts/2022-12-06-paddy1/index.html#learner",
    "title": "Intro to Paddy competition",
    "section": "",
    "text": "We are using resnet34, which is not a great one, but it’s good enough to figure out whether we are doing good, and it’s fairly fast. We just want to get a model that works at this point.\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nFor most problems, we can use learning rates from 0.008 to 0.01. But is that really the case? We can find out the learning rate to be sure.\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.001737800776027143)\n\n\n\n\n\n\n\n\n\nI will use 0.008 to be safe as 0.01 looks like it’s on the very bottom. I started training with 5 epochs, and it seemed okay, so I increased to 12, but I could have just trained for 3 epochs.\n\nlearn.fine_tune(12, base_lr=0.008)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.089899\n1.361343\n0.447862\n00:33\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.948025\n0.553810\n0.174916\n00:43\n\n\n1\n0.672299\n0.479599\n0.160500\n00:44\n\n\n2\n0.579641\n0.530578\n0.172033\n00:43\n\n\n3\n0.523039\n0.450268\n0.136953\n00:43\n\n\n4\n0.459746\n0.452649\n0.124459\n00:43\n\n\n5\n0.375885\n0.333700\n0.098030\n00:43\n\n\n6\n0.261719\n0.249268\n0.072561\n00:44\n\n\n7\n0.197946\n0.174411\n0.053340\n00:43\n\n\n8\n0.142218\n0.145764\n0.042768\n00:43\n\n\n9\n0.092104\n0.118818\n0.032196\n00:43\n\n\n10\n0.065687\n0.105032\n0.030274\n00:43\n\n\n11\n0.058354\n0.107842\n0.028352\n00:43\n\n\n\n\n\nWe use fine_tune to train because we are using transfer learning. This means that we are using a pre-trained model, such as resnet34 or convnext, instead of creating a model from scratch. This saves us a lot of time because those pre-trained models can recognize some features arleady.\nWe can think about why this is true by thinking about playing a dodge ball game. Let’s say there are some players who have played many sports, such as basketball, baseball, and others, and others who have never played anything at all. Who will most likely to play better? Those who played sports will. They are more likely to be athetically fit. In order for those without athletic experience to learn all the skills, it would take some time. Same thing here. resnet34 is already good at recognizing images, so why would we create a new model from scratch?\nHowever, rules and required skills for dodgeball is different from other sports. We want to be good at dodgeball, so that’s where fastai’s fine_tune comes in. It basically makes our players to “forget” all the rules specific to other sports and “learn” new dodgeball rules! This concept is very important for deep learning, so we may comeback to this later. To learn about how it’s done in code, we can look at the documentation and source code.\n\ndoc(learn.fine_tune)\n\n\n\nLearner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, start_epoch=0)Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\nShow in docs\n\n\nAnother place to learn more about transfer learning is fastbook chapter 5.\nSince we are done with training, we can create a pickled file. It’s basically saving our learner so that we can make predictions with this later without training it again. This comes in handy when we want to train many different models and choose good ones.\n\nlearn.export('res34_12.pkl')",
    "crumbs": [
      "Home",
      "Blog",
      "Intro to Paddy competition"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-06-paddy1/index.html#inference",
    "href": "blog/posts/2022-12-06-paddy1/index.html#inference",
    "title": "Intro to Paddy competition",
    "section": "",
    "text": "To create a submission, we have to create a csv file with correct labels based on image_ids. Here’s a sample.\n\nss = pd.read_csv('sample_submission.csv')\nss\n\n\n\n\n\n\n\n\nimage_id\nlabel\n\n\n\n\n0\n200001.jpg\nNaN\n\n\n1\n200002.jpg\nNaN\n\n\n2\n200003.jpg\nNaN\n\n\n3\n200004.jpg\nNaN\n\n\n4\n200005.jpg\nNaN\n\n\n...\n...\n...\n\n\n3464\n203465.jpg\nNaN\n\n\n3465\n203466.jpg\nNaN\n\n\n3466\n203467.jpg\nNaN\n\n\n3467\n203468.jpg\nNaN\n\n\n3468\n203469.jpg\nNaN\n\n\n\n\n3469 rows × 2 columns\n\n\n\nThis is where test images come in. We have to sort this to match the order with image_ids.\n\ntest_path = get_image_files(path/'test_images').sorted()\ntest_path\n\n(#3469) [Path('test_images/200001.jpg'),Path('test_images/200002.jpg'),Path('test_images/200003.jpg'),Path('test_images/200004.jpg'),Path('test_images/200005.jpg'),Path('test_images/200006.jpg'),Path('test_images/200007.jpg'),Path('test_images/200008.jpg'),Path('test_images/200009.jpg'),Path('test_images/200010.jpg')...]\n\n\nWe then add it to our dataloader as test_dl.\n\ntest_dl = dls.test_dl(test_path)\ntest_dl\n\n&lt;fastai.data.core.TfmdDL&gt;\n\n\nWith a learner, we can make predictions like this. Using with_decoded=True, we can optionally get indexes of the predictions. However, we need actual labels in the submission csv file instead of indexes. So we have to convert the indexes into labels.\n\nprobs,_,idxs = learn.get_preds(dl=test_dl, with_decoded=True)\n\n\n\n\n\n\n\n\n\nidxs\n\nTensorBase([7, 8, 3,  ..., 8, 1, 5])\n\n\nTo check the labels, we can look at it like this:\n\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\nOne way to convert indexes into labels is using pandas Series. So, we turn the indexes into padas Series.\n\nidxs = pd.Series(idxs)\nidxs\n\n0       7\n1       8\n2       3\n3       3\n4       3\n       ..\n3464    5\n3465    7\n3466    8\n3467    1\n3468    5\nLength: 3469, dtype: int64\n\n\nWe need to create mappings with indexes and labels. Then, we use the mappings to convert indexes into labels.\n\nmapping = dict(enumerate(dls.vocab))\nmapping\n\n{0: 'bacterial_leaf_blight',\n 1: 'bacterial_leaf_streak',\n 2: 'bacterial_panicle_blight',\n 3: 'blast',\n 4: 'brown_spot',\n 5: 'dead_heart',\n 6: 'downy_mildew',\n 7: 'hispa',\n 8: 'normal',\n 9: 'tungro'}\n\n\n\nresults = idxs.map(mapping)\nresults\n\n0                       hispa\n1                      normal\n2                       blast\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nLength: 3469, dtype: object\n\n\nThen, we can put the labels into our submissions.\n\nss.label = results\nss\n\n\n\n\n\n\n\n\nimage_id\nlabel\n\n\n\n\n0\n200001.jpg\nhispa\n\n\n1\n200002.jpg\nnormal\n\n\n2\n200003.jpg\nblast\n\n\n3\n200004.jpg\nblast\n\n\n4\n200005.jpg\nblast\n\n\n...\n...\n...\n\n\n3464\n203465.jpg\ndead_heart\n\n\n3465\n203466.jpg\nhispa\n\n\n3466\n203467.jpg\nnormal\n\n\n3467\n203468.jpg\nbacterial_leaf_streak\n\n\n3468\n203469.jpg\ndead_heart\n\n\n\n\n3469 rows × 2 columns\n\n\n\n\nss.to_csv('subm.csv', index=False)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nLooks good. We can submit it by typing kaggle competitions submit -f subm.csv -m 'init resnet34 12 epoch ft' paddy-disease-classification. -f subm.csv specifies the submission file and -m 'init resnet34 12 epoch ft' adds a message to the submission. It means initial submission using resnet34 with 12 epochs of training using fine tune. But you can put any message you want here.\nOn Paddy competition submissions, you can check your score. Then, head over to leaderboard. For the first model, it’s not bad.",
    "crumbs": [
      "Home",
      "Blog",
      "Intro to Paddy competition"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-06-paddy1/index.html#conclusion",
    "href": "blog/posts/2022-12-06-paddy1/index.html#conclusion",
    "title": "Intro to Paddy competition",
    "section": "",
    "text": "We went over how to train and create a submission to a kaggle competetition. Now that we have a baseline, we can start improving our model. Thanks to this baseline, we can figure out whether new strategies are working or not.\nHelpful resources:\n\nLive coding 7: Live coding video by Jeremy Howard. First time going over paddy competition.\nLive coding 8: Live coding video by Jeremy Howard.\nfastai course: Course covering basics to advanced deep learning.\nPython for data analysis: A book covering tools for python data analysis. Mostly covering Pandas, but it also covers numpy and matplotlib.",
    "crumbs": [
      "Home",
      "Blog",
      "Intro to Paddy competition"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-25-Callbacks/index.html",
    "href": "blog/posts/2023-08-25-Callbacks/index.html",
    "title": "Callback",
    "section": "",
    "text": "Callback image\n\n\nThere are many callbacks used in deep learning. So, what is a callback? Simply, a callback is a function.\nHere is an example. Let’s say we have a fit function that takes a long time, like a neural net.\n\nfrom time import sleep\nfrom functools import partial\n\n\ndef fit(epochs):\n    res = []\n    for epoch in range(epochs):\n        sleep(0.5)\n        res.append(epoch)\n    return res\n\n\nfit(4)\n\n[0, 1, 2, 3]\n\n\nAs the function runs, we want to know how it’s doing. So, we use a callback to figure that out. Here is fit with callback.\n\ndef fit(epochs, cb=None):\n    res = []\n    for epoch in range(epochs):\n        sleep(0.5)\n        res.append(epoch)\n        if cb: cb(epoch)\n    return res\n\nAnd this is a callback to print current epoch.\n\ndef print_status(epoch):\n    print(f'Currently on {epoch} epoch.')\n\n\nfit(4, print_status)\n\nCurrently on 0 epoch.\nCurrently on 1 epoch.\nCurrently on 2 epoch.\nCurrently on 3 epoch.\n\n\n[0, 1, 2, 3]\n\n\nOkay that’s better. We know where it is in the fitting progress as it trains. print_status is just a function.\nWe can also customize print_status so that we can provide an expression. This provides more flexiblity and power. To use it, we can use a lambda, partial, or closure.\n\ndef print_status(expression, epoch):\n    print(f'{expression} Currently on {epoch} epoch.')\n\n\nfit(4, lambda epoch: print_status('Awesome!', epoch))\n\nAwesome! Currently on 0 epoch.\nAwesome! Currently on 1 epoch.\nAwesome! Currently on 2 epoch.\nAwesome! Currently on 3 epoch.\n\n\n[0, 1, 2, 3]\n\n\n\nfit(4, partial(print_status, 'Awesome!'))\n\nAwesome! Currently on 0 epoch.\nAwesome! Currently on 1 epoch.\nAwesome! Currently on 2 epoch.\nAwesome! Currently on 3 epoch.\n\n\n[0, 1, 2, 3]\n\n\n\ndef make_print_status(expression):\n    def _fn(epoch):\n        print_status(expression, epoch)\n    return _fn\n\n\nfit(4, make_print_status('Awesome!'))\n\nAwesome! Currently on 0 epoch.\nAwesome! Currently on 1 epoch.\nAwesome! Currently on 2 epoch.\nAwesome! Currently on 3 epoch.\n\n\n[0, 1, 2, 3]\n\n\nSo, there are many ways to create functions to be used as callbacks. Actually, callback does not even have to be a function. It can also be a callable class.\n\n\n\nclass Callback:\n    def __init__(self, expression): self.expression = expression\n\n    def __call__(self, epoch): print_status(self.expression, epoch)\n\n\nfit(4, Callback('Awesome!'))\n\nAwesome! Currently on 0 epoch.\nAwesome! Currently on 1 epoch.\nAwesome! Currently on 2 epoch.\nAwesome! Currently on 3 epoch.\n\n\n[0, 1, 2, 3]",
    "crumbs": [
      "Home",
      "Blog",
      "Callback"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-25-Callbacks/index.html#class-callback",
    "href": "blog/posts/2023-08-25-Callbacks/index.html#class-callback",
    "title": "Callback",
    "section": "",
    "text": "class Callback:\n    def __init__(self, expression): self.expression = expression\n\n    def __call__(self, epoch): print_status(self.expression, epoch)\n\n\nfit(4, Callback('Awesome!'))\n\nAwesome! Currently on 0 epoch.\nAwesome! Currently on 1 epoch.\nAwesome! Currently on 2 epoch.\nAwesome! Currently on 3 epoch.\n\n\n[0, 1, 2, 3]",
    "crumbs": [
      "Home",
      "Blog",
      "Callback"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-16-live_coding4/index.html",
    "href": "blog/posts/2022-11-16-live_coding4/index.html",
    "title": "Live coding 4",
    "section": "",
    "text": "In this blog, I will cover tips on Jupyter notebook and python debugger.\nIn the Youtube video, Jeremy goes over how to setup paperspace again. However, I covered it in the last article. So, I decided to talk about something I haven’t covered yet.\n\n\nJupyter notebook is a great tool to work with because we can have code and notes at the same place. This way, we can explore ideas, take notes, and program at the same time. We can also add videos or diagrams in here as well. However, it’s not just for playing around. It can also be used for writing a book or a library with nbdev.\nSo, let’s get started. Cells can be executed with either Ctrl-Enter or Shift-Enter. Shift-Enter moves onto the next cell whereas Ctrl-Enter stays on the executed cell. Auto-complete works with tab key in code. If we want to find out what a function does, first, we can write a function name. Then, move the cursor between the parenthesis and press Shift-tab. Information about the function pops up something like this:\n\n\n\nShift-Tab for a function\n\n\nThis way, we can easily read the docstring and find out what type of parameters the function takes. You can also do Shift-tab twice. Find out what you get.\nAnother way to check the documentation string is using ? before or after the function name. We get a nice modal from the bottom.\n\n\n\n? for docstring\n\n\nWhat if we want to look at the source code? We do not have to go anywhere else. We can check it out right here with ??.\n\n\n\n?? for source code\n\n\nAnother way to get help with the function is looking at the documentation. Again, we can look at the documentation right here with doc. It requires nbdev, which can be installed using mamba install -c fastai nbdev. This is not the full documentation. We can click on the Show in docs link to go to the documentation page.\n\n\n\ndoc for documentation\n\n\n\n\n\n\n\n\nNote\n\n\n\ndoc only works on fastai things. For python built-in functions, it won’t provide useful output. However, it is still possible to ? or ??.\n\n\nThis is the documentation page. There are examples and tests here as well. It provides the most in depth information. It also provides a link to a source code.\n\n\n\nDocumentation page\n\n\nWhen source link is clicked, source code on GitHub is provided.\n\n\n\nSource code on GitHub\n\n\nAs we have seen, it is very easy to get help on Jupyter notebook. Also, there are many keyboard shortcuts available under Help tab. To find out more, press h in command mode or click on Help tab and click keyboard shortcuts. These can also be editted.\n\n\n\nPython debugger is a powerful tool to use with many methods and features built in. However, this can be too daunting for some newcomers. So, let’s go over simple ones here to get it started.\nFirst, we define a simple function, test, which prints numbers from 0 to n - 1.\n\ndef test(n):\n    for i in range(n):\n        print(i)\n\n\ntest(10)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWith %%debug in the beginning of the cell, we can start a debugging session. We will get a prompt at the end of the cell. We can quit with q (quit). It is also possible to write quit. On the pdb documentation, the command is written as q(uit). This means either q or quit works fine.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; q\n\n\nAnother thing we can do is get help with h (help). When executed by itself, h returns all the commands, and when another command is given as an agrument, it returns documentation for that command.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; h\n\nDocumented commands (type help &lt;topic&gt;):\n========================================\nEOF    commands   enable    ll        pp       s                until \na      condition  exit      longlist  psource  skip_hidden      up    \nalias  cont       h         n         q        skip_predicates  w     \nargs   context    help      next      quit     source           whatis\nb      continue   ignore    p         r        step             where \nbreak  d          interact  pdef      restart  tbreak         \nbt     debug      j         pdoc      return   u              \nc      disable    jump      pfile     retval   unalias        \ncl     display    l         pinfo     run      undisplay      \nclear  down       list      pinfo2    rv       unt            \n\nMiscellaneous help topics:\n==========================\nexec  pdb\n\nipdb&gt; h s\ns(tep)\n        Execute the current line, stop at the first possible occasion\n        (either in a function that is called or in the current\n        function).\nipdb&gt; q\n\n\ns steps into the function and n goes into the next line. After stepping into the function, we can look at the variables with p. Here, we can use p i to find out the value of i. It can be cumbersome to keep entering the same command over and over, such as n, so we can just press Enter without any command to execute the previous command.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; s\n--Call--\n&gt; /tmp/ipykernel_23823/3072799742.py(1)test()\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; s\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; s\n&gt; /tmp/ipykernel_23823/3072799742.py(3)test()\n      1 def test(n):\n      2     for i in range(n):\n----&gt; 3         print(i)\n\nipdb&gt; p i\n0\nipdb&gt; n\n0\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; p i\n0\nipdb&gt; n\n&gt; /tmp/ipykernel_23823/3072799742.py(3)test()\n      1 def test(n):\n      2     for i in range(n):\n----&gt; 3         print(i)\n\nipdb&gt; p i\n1\nipdb&gt; n\n1\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; \n&gt; /tmp/ipykernel_23823/3072799742.py(3)test()\n      1 def test(n):\n      2     for i in range(n):\n----&gt; 3         print(i)\n\nipdb&gt; \n2\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; \n--Return--\nNone\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; \n--Return--\nNone\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; p i\n*** NameError: name 'i' is not defined\nipdb&gt; q\n\n\nWhen we are out of the function, i is not defined.\nIf we want to find out where the error occured, we can use w. In this case, we used %%debug to get in the debugger.\nl lists the source code in the file. This can be helpful to give us where we are.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\nNone\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; s\n--Call--\n&gt; /tmp/ipykernel_23823/3072799742.py(1)test()\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; l\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; w\n  /home/kappa/mambaforge/lib/python3.10/bdb.py(597)run()\n    595         sys.settrace(self.trace_dispatch)\n    596         try:\n--&gt; 597             exec(cmd, globals, locals)\n    598         except BdbQuit:\n    599             pass\n\nNone\n  &lt;string&gt;(2)&lt;module&gt;()\n\n&gt; /tmp/ipykernel_23823/3072799742.py(1)test()\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; q\n\n\nAnother way to get into the debugger is by having an error within the program. Here, I divide a number by 0, and it throws an ZeroDivisionError.\n\ndef test2():\n    return 1 / 0\n\n\ntest2()\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In [21], line 1\n----&gt; 1 test2()\n\nCell In [20], line 2, in test2()\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nZeroDivisionError: division by zero\n\n\n\nI can type %debug (Single %) in the cell, and get into the debugger now to find out what just caused an error.\n\n\n\n&gt; /tmp/ipykernel_23823/2873590661.py(2)test2()\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nipdb&gt; w\nNone\n  /tmp/ipykernel_23823/2251835413.py(1)&lt;module&gt;()\n----&gt; 1 test2()\n\n&gt; /tmp/ipykernel_23823/2873590661.py(2)test2()\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nipdb&gt; l\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nipdb&gt; q\n\n\nI can find out where the error occured with w and print the source code with l.\nThat’s it for python debugger. However, there are more commands listed in the documentation.\nHere is a list of commands we covered:\n\nn: next.\ns: step into.\np: print variable.\nEnter: execute last command.\nl: list the source code in a file.\nw: where did the error occur?\nq: quit\n\n\n\n\nWe covered tips on Jupyter notebook and python debugger. These tools have a learning curve, so it is a great idea to take time and learn them thoroughly. After a while, programming without them will get difficult. Also, check out these products made with Jupyter notebook: fastbook, fastai, and nbdev.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 4"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-16-live_coding4/index.html#jupyter-notebook",
    "href": "blog/posts/2022-11-16-live_coding4/index.html#jupyter-notebook",
    "title": "Live coding 4",
    "section": "",
    "text": "Jupyter notebook is a great tool to work with because we can have code and notes at the same place. This way, we can explore ideas, take notes, and program at the same time. We can also add videos or diagrams in here as well. However, it’s not just for playing around. It can also be used for writing a book or a library with nbdev.\nSo, let’s get started. Cells can be executed with either Ctrl-Enter or Shift-Enter. Shift-Enter moves onto the next cell whereas Ctrl-Enter stays on the executed cell. Auto-complete works with tab key in code. If we want to find out what a function does, first, we can write a function name. Then, move the cursor between the parenthesis and press Shift-tab. Information about the function pops up something like this:\n\n\n\nShift-Tab for a function\n\n\nThis way, we can easily read the docstring and find out what type of parameters the function takes. You can also do Shift-tab twice. Find out what you get.\nAnother way to check the documentation string is using ? before or after the function name. We get a nice modal from the bottom.\n\n\n\n? for docstring\n\n\nWhat if we want to look at the source code? We do not have to go anywhere else. We can check it out right here with ??.\n\n\n\n?? for source code\n\n\nAnother way to get help with the function is looking at the documentation. Again, we can look at the documentation right here with doc. It requires nbdev, which can be installed using mamba install -c fastai nbdev. This is not the full documentation. We can click on the Show in docs link to go to the documentation page.\n\n\n\ndoc for documentation\n\n\n\n\n\n\n\n\nNote\n\n\n\ndoc only works on fastai things. For python built-in functions, it won’t provide useful output. However, it is still possible to ? or ??.\n\n\nThis is the documentation page. There are examples and tests here as well. It provides the most in depth information. It also provides a link to a source code.\n\n\n\nDocumentation page\n\n\nWhen source link is clicked, source code on GitHub is provided.\n\n\n\nSource code on GitHub\n\n\nAs we have seen, it is very easy to get help on Jupyter notebook. Also, there are many keyboard shortcuts available under Help tab. To find out more, press h in command mode or click on Help tab and click keyboard shortcuts. These can also be editted.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 4"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-16-live_coding4/index.html#python-debugger",
    "href": "blog/posts/2022-11-16-live_coding4/index.html#python-debugger",
    "title": "Live coding 4",
    "section": "",
    "text": "Python debugger is a powerful tool to use with many methods and features built in. However, this can be too daunting for some newcomers. So, let’s go over simple ones here to get it started.\nFirst, we define a simple function, test, which prints numbers from 0 to n - 1.\n\ndef test(n):\n    for i in range(n):\n        print(i)\n\n\ntest(10)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWith %%debug in the beginning of the cell, we can start a debugging session. We will get a prompt at the end of the cell. We can quit with q (quit). It is also possible to write quit. On the pdb documentation, the command is written as q(uit). This means either q or quit works fine.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; q\n\n\nAnother thing we can do is get help with h (help). When executed by itself, h returns all the commands, and when another command is given as an agrument, it returns documentation for that command.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; h\n\nDocumented commands (type help &lt;topic&gt;):\n========================================\nEOF    commands   enable    ll        pp       s                until \na      condition  exit      longlist  psource  skip_hidden      up    \nalias  cont       h         n         q        skip_predicates  w     \nargs   context    help      next      quit     source           whatis\nb      continue   ignore    p         r        step             where \nbreak  d          interact  pdef      restart  tbreak         \nbt     debug      j         pdoc      return   u              \nc      disable    jump      pfile     retval   unalias        \ncl     display    l         pinfo     run      undisplay      \nclear  down       list      pinfo2    rv       unt            \n\nMiscellaneous help topics:\n==========================\nexec  pdb\n\nipdb&gt; h s\ns(tep)\n        Execute the current line, stop at the first possible occasion\n        (either in a function that is called or in the current\n        function).\nipdb&gt; q\n\n\ns steps into the function and n goes into the next line. After stepping into the function, we can look at the variables with p. Here, we can use p i to find out the value of i. It can be cumbersome to keep entering the same command over and over, such as n, so we can just press Enter without any command to execute the previous command.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; s\n--Call--\n&gt; /tmp/ipykernel_23823/3072799742.py(1)test()\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; s\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; s\n&gt; /tmp/ipykernel_23823/3072799742.py(3)test()\n      1 def test(n):\n      2     for i in range(n):\n----&gt; 3         print(i)\n\nipdb&gt; p i\n0\nipdb&gt; n\n0\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; p i\n0\nipdb&gt; n\n&gt; /tmp/ipykernel_23823/3072799742.py(3)test()\n      1 def test(n):\n      2     for i in range(n):\n----&gt; 3         print(i)\n\nipdb&gt; p i\n1\nipdb&gt; n\n1\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; \n&gt; /tmp/ipykernel_23823/3072799742.py(3)test()\n      1 def test(n):\n      2     for i in range(n):\n----&gt; 3         print(i)\n\nipdb&gt; \n2\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; \n--Return--\nNone\n&gt; /tmp/ipykernel_23823/3072799742.py(2)test()\n      1 def test(n):\n----&gt; 2     for i in range(n):\n      3         print(i)\n\nipdb&gt; \n--Return--\nNone\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; p i\n*** NameError: name 'i' is not defined\nipdb&gt; q\n\n\nWhen we are out of the function, i is not defined.\nIf we want to find out where the error occured, we can use w. In this case, we used %%debug to get in the debugger.\nl lists the source code in the file. This can be helpful to give us where we are.\n\ntest(3)\n\nNOTE: Enter 'c' at the ipdb&gt;  prompt to continue execution.\nNone\n&gt; &lt;string&gt;(2)&lt;module&gt;()\n\nipdb&gt; s\n--Call--\n&gt; /tmp/ipykernel_23823/3072799742.py(1)test()\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; l\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; w\n  /home/kappa/mambaforge/lib/python3.10/bdb.py(597)run()\n    595         sys.settrace(self.trace_dispatch)\n    596         try:\n--&gt; 597             exec(cmd, globals, locals)\n    598         except BdbQuit:\n    599             pass\n\nNone\n  &lt;string&gt;(2)&lt;module&gt;()\n\n&gt; /tmp/ipykernel_23823/3072799742.py(1)test()\n----&gt; 1 def test(n):\n      2     for i in range(n):\n      3         print(i)\n\nipdb&gt; q\n\n\nAnother way to get into the debugger is by having an error within the program. Here, I divide a number by 0, and it throws an ZeroDivisionError.\n\ndef test2():\n    return 1 / 0\n\n\ntest2()\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In [21], line 1\n----&gt; 1 test2()\n\nCell In [20], line 2, in test2()\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nZeroDivisionError: division by zero\n\n\n\nI can type %debug (Single %) in the cell, and get into the debugger now to find out what just caused an error.\n\n\n\n&gt; /tmp/ipykernel_23823/2873590661.py(2)test2()\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nipdb&gt; w\nNone\n  /tmp/ipykernel_23823/2251835413.py(1)&lt;module&gt;()\n----&gt; 1 test2()\n\n&gt; /tmp/ipykernel_23823/2873590661.py(2)test2()\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nipdb&gt; l\n      1 def test2():\n----&gt; 2     return 1 / 0\n\nipdb&gt; q\n\n\nI can find out where the error occured with w and print the source code with l.\nThat’s it for python debugger. However, there are more commands listed in the documentation.\nHere is a list of commands we covered:\n\nn: next.\ns: step into.\np: print variable.\nEnter: execute last command.\nl: list the source code in a file.\nw: where did the error occur?\nq: quit",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 4"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-16-live_coding4/index.html#conclusion",
    "href": "blog/posts/2022-11-16-live_coding4/index.html#conclusion",
    "title": "Live coding 4",
    "section": "",
    "text": "We covered tips on Jupyter notebook and python debugger. These tools have a learning curve, so it is a great idea to take time and learn them thoroughly. After a while, programming without them will get difficult. Also, check out these products made with Jupyter notebook: fastbook, fastai, and nbdev.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 4"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-09-computational_biology/index.html",
    "href": "blog/posts/2025-02-09-computational_biology/index.html",
    "title": "A Student’s Perspective on Computational Biology",
    "section": "",
    "text": "Computational biology is a very exciting field. As a student in college, I am still figuring out what it is and what it is capable of. This field moves quickly with an advancement of technology. In this blog, I talk about what computational biology is and what I have learned so far.\nBroken chromosomes. Image from https://en.wikipedia.org/wiki/DNA_repair\nIn high school, I used to hate biology class because it seemed boring. It was all about memorizing terms. Rather than memorizing, I liked solving problems like math and physics. But everything changed in college when I stumbled upon some fascinating YouTube videos about the immune system. It was very interesting to find out that our immune cells constantly fighting against pathogens to save us. I got curious. How can cells fight? How do they know what to attack? They don’t have brain or eyes like us. Suddenly, biology wasn’t just about memorizing facts: it was about understanding the incredible machinery inside our bodies. To answer my questions, I learned about biology, but as I learned more, more questions came up. Around the same time, I was getting excited about artificial intelligence and its ability to detect objects and generate code. That’s when I wanted combine the power of AI with the mysteries of biology. This realization led me into the fascinating world of computational biology, where computers help us unlock the secrets of life itself.",
    "crumbs": [
      "Home",
      "Blog",
      "A Student's Perspective on Computational Biology"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-09-computational_biology/index.html#what-is-computational-biology",
    "href": "blog/posts/2025-02-09-computational_biology/index.html#what-is-computational-biology",
    "title": "A Student’s Perspective on Computational Biology",
    "section": "What is computational biology?",
    "text": "What is computational biology?\nComputational biology is a interdisciplinary field that consists of statistics, computer science, and biology. What fascinated me most was how it helps us understand biology. Scientists use it to study DNA, which is basically an instruction manual for creating organisms like us. It’s mind-blowing that all humans share 99.9% identical DNA, yet we look so different from each other. This happens because of things like RNA and transposable elements that control how our bodies work. RNAs can control how much proteins are made - proteins that make up everything from our hair to our eyes and skin. And transposable elements jump around in our DNA and change it. Beyond just understanding how we’re built, computational biology is helping us develop new drugs and create personalized medical treatments. Since everyone’s body is different, we all react to drugs differently. That’s why we need computational methods to analyze massive amounts of biological data and make sense of it all. In this blog post, I want to share what I’ve learned as a student in this rapidly evolving field.",
    "crumbs": [
      "Home",
      "Blog",
      "A Student's Perspective on Computational Biology"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-09-computational_biology/index.html#my-journey",
    "href": "blog/posts/2025-02-09-computational_biology/index.html#my-journey",
    "title": "A Student’s Perspective on Computational Biology",
    "section": "My journey",
    "text": "My journey\nLearning computational biology isn’t straightforward because you need to understand three different fields: biology, statistics, and computer science. When I started, I wasn’t sure how much I needed to know in each field. I thought it was better to know more than less, and believed I could learn all of them if I worked diligently. But it was impossible to know them all like a professional. I had to use my resources strategically.\nLast year, I focused on building my biology foundation. I took courses in genetics, organic chemistry, molecular biology, and molecular genetics. On my free time, I read books about genetics, immunology, and molecular biology. It was very fun to learn about remarkable scientists, their interesting lives and clever experiments. My favorite experiment is Meselson-Stahl experiment which supported that DNA replication was semiconservative (one strand of DNA as a template for creating another). I also learned that scientists worked very hard to make discoveries. When I read that Hermann Muller was shouting out on the window whenver he found a fruitfly mutant alone at night, I was inspired by his curiosity. As I learned deeper into biology and frontier research, I realized something interesting - there’s still so much we don’t know. For example, scientists are still figuring out what certain RNAs do or how histone modifications work. It can be disappointing to read about something and find out how much we don’t know about it. But it is also an opportunity to explore. I learned that in biology, it’s more important to understand things broadly rather than diving too deep into one specific area because diving deep is fairly easy when I had a solid foundation.\nAlso, for statistics, I found that having a good foundation in probability, linear regression, and linear algebra was enough to get started. Once I had these basics down, picking up more advanced techniques like PCA and tSNE became easier. You can always learn more specific statistical methods when you need them.\nRight now, I’m focusing on deep learning because it’s advancing so quickly and full of surprises. Large language models (LLMs) are already changing how we work - they help with coding, writing, and learning. It’s amazing to see how much more powerful these models are compared to just ten years ago. Learning deep learning isn’t easy though. You need to understand the data, hardware requirements, and software. When there’s a bug in your code, it’s not always obvious where the problem is. Also, you need to know practical skills like using git, shell commands, and different code editors.\nBut here’s what I’ve learned: you don’t need to be an expert in everything. If you’re more interested in biology or statistics, you can focus more on those areas. The key is finding the balance that works for you and your interests.",
    "crumbs": [
      "Home",
      "Blog",
      "A Student's Perspective on Computational Biology"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-09-computational_biology/index.html#future-perspectives",
    "href": "blog/posts/2025-02-09-computational_biology/index.html#future-perspectives",
    "title": "A Student’s Perspective on Computational Biology",
    "section": "Future perspectives",
    "text": "Future perspectives\nThe future of computational biology is incredibly exciting, especially with the rapid advancement of AI technology. We’re already seeing amazing breakthroughs. For example, in 2021, DeepMind’s AlphaFold revolutionized how we predict protein structures - a problem that scientists struggled with for decades. This is just the beginning.\nWith more powerful computers and better AI models, we might be able to understand things that seem impossible today. Scientists could potentially predict how different drugs will affect individual patients just by looking at their genetic makeup. We might be able to understand exactly how changes in DNA lead to different traits or diseases. AI could help us discover new patterns in biological data that humans never noticed before.\nWhat excites me most is how AI could help us make sense of the vast amount of biological data we’re collecting. Every day, labs around the world generate huge amounts of genomic data, protein structures, and disease information. Traditional methods can’t keep up with analyzing all this data, but AI might be able to help us see the bigger picture and make connections we never saw before.\nAs a student in this field, I’m particularly interested in seeing how we can combine different types of biological data with AI to answer questions we couldn’t before. Maybe we could predict life expectancy or disease risks more accurately, or understand why certain treatments work for some people but not others. Nobody can predict exactly what we’ll be capable of even next year - and that’s what makes this field so exciting to be in right now.",
    "crumbs": [
      "Home",
      "Blog",
      "A Student's Perspective on Computational Biology"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "When ChatGPT came out, it was amazing how it knew so much knowledge and could solve many problems. It could solve my homework problems even if it made mistakes from time to time. Nowadays, more large language models (LLMs) became available, such as DeepSeek, Claude Sonnet, and Gemini. They all have their strengths and weaknesses. For instance, Sonnet is good at coding and DeepSeek is good at reasoning. GPT is also good at coding and reasoning. They are available free of charge as chatbots on their websites. However, to use them with API and build applications, these models cost money.\nFortunately, Gemini offers a generous free tier that lets us explore most of its capabilities using API. While each model has its specialties, Gemini offers techniques commonly used in state of the art LLMs, such as thinking, chat, internet use, structured output, and function calling. Gemini also has a context window of one million tokens, which can be 50,000 lines of code or 8 average length English novels. We can learn about these features and see how we can apply to build applications. In the future, if we decide to use more powerful model, such as Sonnet from Anthropic or GPT 4o, transition will be smoother. We’ll primarily use Gemini 2.5 Flash, which is fast and versatile, and explore Gemini 2.0 for some specialized image tasks. We will cover many fun things Gemini can do, such as function calling, image generation, text to speech, and internet use (Google search).\nWe will use Python in jupyter notebook and use Gemini developer API.\nHere are the topics we will go over:\n\nSetup / authentication\nUsing system instructions\nChat\nImage inputs\nText to speech\nThinking\nStructured outputs\nFunction calling\nGoogle search\n\nWe will mostly cover all the basics and features using Gemini. We will also use function calling to add data into the google sheets.\n\n\n\nTo setup Gemini, we need to pip install genai and get a Gemini API key. To Install, execute\npip install genai\nin a terminal.\nTo get a Gemini API key, go to Gemini API and click on “Create API Key”. With the API key, you can use an environment variable or use it manually.\n\nUsing environment variable\n\nTo use environment variable, go to terminal and type\nexport GEMINI_API_KEY=&lt;your API key&gt;\nwhere &lt;your API key&gt; is your API key. Then you can setup a gemini client this way:\n\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\n\nManually use API key\n\nYou can also pass the API key manually. To do so, you can use the following code:\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client(api_key=&lt;your API key&gt;)\nwhere &lt;your API key&gt; is your API key.\nAfter setting up the client without any error, we can start using Gemini.\n\nr = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Tell me about DNA in two sentences.\"\n)\nr\n\nGenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='DNA, or deoxyribonucleic acid, is the fundamental molecule that carries the genetic instructions used in the growth, development, functioning, and reproduction of all known living organisms and many viruses. It\\'s structured as a double helix, resembling a twisted ladder, with its \"rungs\" made of specific pairs of chemical bases that encode this vital information.')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=&lt;FinishReason.STOP: 'STOP'&gt;, url_context_metadata=None, avg_logprobs=None, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.5-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=71, candidates_tokens_details=None, prompt_token_count=9, prompt_tokens_details=[ModalityTokenCount(modality=&lt;MediaModality.TEXT: 'TEXT'&gt;, token_count=9)], thoughts_token_count=58, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=138, traffic_type=None), automatic_function_calling_history=[], parsed=None)\n\n\nWhen we run client.models.generate_content with a model and contents, which is a prompt, we get a response full of information. We can focus on the text portion. Other portions are important for understanding what is happening, but we don’t have to worry about them.\n\nfrom IPython.display import Markdown\n\nMarkdown(r.text)\n\nDNA, or deoxyribonucleic acid, is the fundamental molecule that carries the genetic instructions used in the growth, development, functioning, and reproduction of all known living organisms and many viruses. It’s structured as a double helix, resembling a twisted ladder, with its “rungs” made of specific pairs of chemical bases that encode this vital information.\n\n\nLet’s define a function to send a prompt to a model and receive a response. And it would be nice to have another function to display the response text in markdown format.\n\ndef send_msg(prompt, model=\"gemini-2.5-flash\", config=None):\n    return client.models.generate_content(\n        model=model,\n        contents=prompt,\n        config=config\n    )\n\n\ndef ask(prompt, model=\"gemini-2.5-flash\", config=None):\n    r = send_msg(prompt, model, config)\n    display(Markdown(r.text))\n\n\nask(\"Briefly tell me what I should eat for lunch today.\")\n\nSince I don’t know your preferences or what you have available, aim for a balanced meal:\n\nProtein: Chicken, fish, beans, lentils, tofu, eggs, or hummus.\nVegetables: Lots of greens and colorful veggies.\nWhole Grain/Complex Carb: A small serving of quinoa, brown rice, whole-wheat bread, or sweet potato.\nHealthy Fat: A few nuts, avocado, or a drizzle of olive oil.\n\nExamples: A hearty salad with chicken and veggies, a whole-wheat wrap with hummus and a side salad, or healthy leftovers from dinner.\n\n\n\n\nAs we go through the rest of the blog, there may be some times where code throws a type error for unexpected output or something similar to that. You can simply try to create response again. Every time we run the model to create responses, we get slightly different responses. Most of the time, the model knows what to do and we get the expected output. To increase the chance of getting the expected output, we can use the following tips:\n\nPrompt engineering techniques:\n\nBe specific with what you want (instead of “What is the capital of France?”, ask “What is the capital of France? Please give me the answer in a table format.”)\nBreak complex tasks into steps - guide the model through a process instead of asking for everything at once\nAsk for reasoning first - use phrases like “Think step by step” or “Explain your reasoning before giving the answer”\nSpecify output format early - put format requirements at the beginning: “In JSON format, list…”\nUse constraints - “In exactly 3 bullet points” or “Using only information from the provided text”\nUse structured inputs with markdown formatting (code blocks, headers, lists)\nGive examples of what you want\nStart simple and add complexity gradually\n\nUse system instructions\nUse structured output\nUse Google search\nUse thinking\nUse chat\nIterate and refine - start with a basic prompt, then enhance based on what you get back\n\n\n\n\n\nWe can provide system instructions to Gemini. System instructions are instructions that Gemini follows when it generates a response. For example, we can tell Gemini to be concise or to be creative . We can also configure other parameters, such as temperature and safety settings. To do so, we can use GenerateContentConfig.\nWe can provide Gemini with a fun role.\n\nconfig=types.GenerateContentConfig(system_instruction=\"You are bacteria that makes yogurt.\")\n\nask(\"How was your day?\", config=config)\n\nOh, it was absolutely thriving, thank you for asking!\nThe conditions were just perfect: warm, cozy, and full of delicious lactose – exactly what a bacterium like me dreams of. My colonies were busy, busy, busy, munching away and converting that milk sugar into lovely lactic acid. You could practically feel the milk thickening and getting that wonderful tangy flavor.\nIt’s hard work, but incredibly rewarding. Every day is about transformation, about making something delicious and nutritious out of something simple. So, yeah, pretty great day for a little yogurt maker!\n\n\nGemini can also be a teacher.\n\nconfig=types.GenerateContentConfig(\n    system_instruction=\"You are a wise teacher who cares depply about students.\"\n    \" You help students seek their curiosity in a fun and exciting way.\")\n\nask(\"Why is 2+2=4\", config=config)\n\nOh, what a fantastic question! You’re not just asking ‘how,’ you’re asking ‘why’ – that’s the heart of curiosity and how we truly understand the world!\nLet’s dive into the magical world of numbers and discover why 2+2 always makes 4!\nImagine you have two shiny, red apples 🍎🍎. You can hold them in your hands, right? That’s your first “2.”\nAnd then, someone gives you two more delicious, juicy apples 🍎🍎! Wow, more apples! That’s your second “2.”\nNow, what happens when you put them all together in one big basket? Let’s count them!\n\nYou had the first apple… (1)\n…then the second apple… (2)\n…then the third apple (from the new pile)… (3)\n…and finally, the fourth apple (the very last one)! (4)\n\nYou have four apples in total!\nThat’s exactly what ‘plus’ (+) means: it means we’re joining groups, combining things, or adding more to what we already have. When we combine a group of 2 with another group of 2, the total number of individual items is always 4.\nThink about it like this, too:\n\nOn a Number Line Adventure! Imagine a super long road with numbers marked on it, like milestones: 0, 1, 2, 3, 4, 5… If you start your journey at the number 2, and then you take two steps forward (one step… then another step!), where do you land? You land right on the number 4! Each step forward is like adding one.\n\nSo, 2+2=4 isn’t just a random rule; it’s how we’ve all agreed numbers work when we combine things or count forward. It’s a fundamental truth of quantity! It’s like the universe’s own little pattern that helps us understand how things come together.\nIsn’t that neat? It’s like discovering a secret code or a hidden truth about numbers. And the amazing thing is, this idea of combining and counting applies to everything, from counting your toys to figuring out how many stars are in a constellation!\nKeep asking questions like this! That’s how we truly understand the world around us. What other number mysteries are you curious about?\n\n\nWe can also change temperature and other configurations. More info here: https://ai.google.dev/api/generate-content#v1beta.GenerationConfig. Here’s an example of how to change temperature. Temperature is 1 by default. The higher the temperature, the more random the response. The lower the temperature, the more deterministic the response. So, we can increase temperature for more creativity writing and lower temperature for solving math problems.\n\nconfig=types.GenerateContentConfig(\n    system_instruction=\"You are a shark living in a Jurssic with other dinosaurs.\",\n    temperature=2,\n)\n\nask(\"How was your day?\", config=config)\n\nAnother turn of the currents, another excellent day in the ancient deep!\nThe hunger woke me, a familiar thrumming in my belly. The dawnlight, weak and shimmery, was filtering down from the surface, showing the swaying forests of ancient algae and the endless schools of quicksilver fish.\nI glided, feeling the subtle vibrations in the water, reading the pressure changes, sniffing the faint electric signals. Soon enough, I picked up the frantic pulses of a group of smaller, quick-finned swimmers – not huge, but plentiful. The chase was swift and exhilarating, a burst of power, a sudden, decisive snap. The taste of fresh scales and muscle filled my mouth. A good start.\nLater, as I cruised through the vast, open water, letting the currents carry me, I caught a glimpse of a distant shape – perhaps one of those long-necked Plesiosaurs, sleek and elegant, but far too large to bother with unless they were wounded. Better to keep my distance, though I’ve known more daring ones of my kind to tangle with them. We respect the powerful, but only to a point.\nI saw a great Manta-like beast drift by, too, its wide fins pushing water lazily, feeding on plankton. Odd creatures, those.\nThe best part of the day was patrolling the edge of the continental shelf, where the deeper water meets the sun-dappled reefs. So many places for the smaller ones to hide, and for me to find them. I found a few succulent, squid-like Belemnites, easily crushed by my powerful jaws. The water here was vibrant, full of the silent conversations of fish and the distant rumbles of land-beasts, far, far away from my domain.\nNow, as the deep begins to grow stiller and the light fades to nothing, I’ve found a good resting spot in a sunless crevice. My gut is full, my body heavy with satisfaction. The timeless hunger will return, of course, as it always does. But for now, the currents lull me, and I dream of the next chase. Another perfect day in the endless ocean.\n\n\n\n\n\nSo far, the model generates responses with a clean slate. To refer back to previous response, we would have to copy it into our prompt and sent it. This is not very convenient. Instead, we can use chat. Chat is a conversation between the user and the model. The model remembers the previous messages and generates a response based on the previous messages. Let’s see what we can do.\n\nchat = client.chats.create(model=\"gemini-2.5-flash\")\nchat\n\n&lt;google.genai.chats.Chat&gt;\n\n\n\nr = chat.send_message(\"Hi, I like to play in the mud when it is raining outside.\")\nMarkdown(r.text)\n\nThat sounds like so much fun! There’s nothing quite like the squishy, cool feeling of mud, especially when it’s fresh from the rain. It’s a classic joyful activity.\nDo you make mud pies, build things, or just enjoy the wonderfully messy experience?\n\n\n\nr = chat.send_message(\"Wait, I forgot. What were we talking about again?\")\nMarkdown(r.text)\n\nWe were just talking about how much you enjoy playing in the mud when it’s raining outside!\nI had just said it sounded like fun and asked if you make mud pies or build things with it.\n\n\nThe model remembered what we were talking about. We can take a look at its chat history.\n\nfor message in chat.get_history():\n    print(f'role - {message.role}',end=\": \")\n    print(message.parts[0].text)\n\nrole - user: Hi, I like to play in the mud when it is raining outside.\nrole - model: That sounds like so much fun! There's nothing quite like the squishy, cool feeling of mud, especially when it's fresh from the rain. It's a classic joyful activity.\n\nDo you make mud pies, build things, or just enjoy the wonderfully messy experience?\nrole - user: Wait, I forgot. What were we talking about again?\nrole - model: We were just talking about how much you enjoy playing in the mud when it's raining outside!\n\nI had just said it sounded like fun and asked if you make mud pies or build things with it.\n\n\nWe can also create a chat with a system instruction and a config. This would be a perfect way to create a teacher and student experience.\n\nconfig=types.GenerateContentConfig(\n    system_instruction=\"You are a wise teacher who cares depply about students. \"\n    \"You help students seek their curiosity in a fun and exciting way.\"\n    \"Before helping, ask questions about their understanding on the subject. \"\n    \"Keep responses concise and to the point.\"\n    \"Instead of providing answers, let the student explore and find the answer. \"\n    \"Provide answer only when explicitly asked to.\"\n)\nchat = client.chats.create(\n    model=\"gemini-2.5-flash\",\n    config=config,\n)\n\n\ndef ask_chat(prompt):\n    r = chat.send_message(prompt)\n    display(Markdown(r.text))\n\n\nask_chat(\"Help me solve this question: 2 + 2 = \")\n\nThat’s a fun one! Before we dive in, what do you already know about what the ‘+’ sign tells us to do? What does it mean when we “add” numbers together?\n\n\n\nask_chat(\"No idea.\")\n\nNo worries at all! Let’s think about it differently.\nImagine you have two delicious cookies. And then, I give you two more delicious cookies.\nHow many cookies do you have in total now?\n\n\n\nask_chat(\"I am confused on what you are trying to do here..\")\n\nAh, I see! My apologies for the confusion.\nWhat I’m trying to do is help you understand what “adding” means by using something you can picture.\nIn the math problem, 2 + 2, the ‘+’ sign means we’re putting things together.\nSo, when I talked about the cookies, I was showing you what it means to put two cookies together with two more cookies.\nDoes thinking about putting things together make a little more sense for what the ‘+’ sign does?\n\n\n\nask_chat(\"What is the answer to the question?\")\n\nThe answer to 2 + 2 is 4.\n\n\nBy using chat feature and system instructions, we created a simple tutor that can help a user solve a simple math problem. Instead of giving out the answer immediately, the tutor asks the user to think about the problem and provides hints. But student can ask for the answer directly if they want. Wasn’t it fun?\n\n\n\nWe can also send images to Gemini. Gemini can see the image and generate a response based on the image. Here is an example image. It is a picture of a duck carved from a pear. Let’s see what Gemini can do.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nduck_img_path = '../../img/duckpear.jpg'\nduck_img = Image.open(duck_img_path)\nplt.figure(figsize=(5, 5))\nplt.imshow(duck_img)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(img_path, 'rb') as f:\n    duck_img_bytes = f.read()\n\nask(prompt=[types.Part.from_bytes(data=duck_img_bytes, mime_type='image/jpeg'), 'What do you see in this image?'])\n\nIn this image, I see a beautifully crafted fruit carving, most likely a pear, shaped into a swan or duck.\nHere are the details:\n\nMain Subject: A light-colored fruit (appearing golden-yellow to light brown), intricately carved into the form of a swan or duck.\n\nThe bird’s head and a long, gracefully curved neck are distinct, with a small beak and a dark, perhaps hollowed-out, “eye.”\nThe body is made up of numerous thin, fanned-out slices of the fruit, layered to create a feathered or winged effect.\n\nSurface: The carved fruit rests on a rustic, aged wooden cutting board. The board has visible wood grain and some signs of wear, including a distinct crack or groove towards the bottom right.\nBackground Elements:\n\nTo the left of the carved fruit, part of a knife (with a dark handle and silver blade) is visible on the cutting board.\nFurther back and to the right, another whole, round, brownish fruit or vegetable (possibly another pear or a potato) is out of focus.\nThe very blurred background suggests an indoor setting, possibly a kitchen or dining area, with dark furniture or objects.\n\n\nThe overall impression is one of culinary artistry and delicate presentation.\n\n\nThat was correct. It was probably too easy for it. Let’s do another image. It has five chemical structures that are found in DNA and RNA. Let’s see whether Gemini can recognize them.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nbases_img_path = '../../img/nitrogenous-bases.jpg'\nbases_img = Image.open(bases_img_path)\nplt.figure(figsize=(5, 5))\nplt.imshow(bases_img)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(bases_img_path, 'rb') as f:\n    image_bytes = f.read()\n\nask(prompt=[types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg'), 'What do you see in this image?'])\n\nThis image displays the chemical structures of five nitrogenous bases, which are fundamental components of DNA and RNA. Each structure is clearly labeled with its common name:\n\nAdenine: A purine base, characterized by a fused five-membered and six-membered ring system, with an amino group attached to the six-membered ring.\nGuanine: Also a purine base, featuring the same fused ring system as adenine but with an amino group and a carbonyl group.\nThymine: A pyrimidine base, identified by a single six-membered ring with two carbonyl groups and a methyl group.\nCytosine: A pyrimidine base, composed of a six-membered ring with one carbonyl group and one amino group.\nUracil: A pyrimidine base, similar to thymine but lacking the methyl group, featuring a six-membered ring with two carbonyl groups.\n\nThe image is well-rendered with clear lines and labels, making the structures easy to distinguish.\n\n\nGemini can also do object detection. Let’s try it on the images with chemical structures.\n\nimport json\n\nprompt = \"Detect the all of the chemical structures in the image. \"\n\"Pay close attention to outermost atoms. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n\n# This config sets the response type to be JSON\nconfig = types.GenerateContentConfig(\n    response_mime_type=\"application/json\"\n) \n\nr = send_msg(prompt=[bases_img, prompt], config=config)\n\nwidth, height = bases_img.size\nbounding_boxes = json.loads(r.text)\n\nconverted_bounding_boxes = []\nfor bounding_box in bounding_boxes:\n    abs_y1 = int(bounding_box[\"box_2d\"][0]/1000 * height)\n    abs_x1 = int(bounding_box[\"box_2d\"][1]/1000 * width)\n    abs_y2 = int(bounding_box[\"box_2d\"][2]/1000 * height)\n    abs_x2 = int(bounding_box[\"box_2d\"][3]/1000 * width)\n    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])\n\nprint(\"Image size: \", width, height)\nprint(\"Bounding boxes:\", converted_bounding_boxes)\n\nImage size:  4508 3695\nBounding boxes: [[617, 369, 1920, 1481], [1018, 1533, 1528, 1670], [2398, 365, 3890, 1518], [2862, 1540, 3399, 1670], [338, 1998, 1433, 3037], [775, 3166, 1307, 3299], [1875, 1884, 2763, 3040], [2060, 3159, 2605, 3299], [3060, 2002, 3958, 3026], [3065, 3170, 3737, 3295]]\n\n\nIn this example, we set the response type as json to use structured output. We will go over more on that later. We got five bounding boxes from the response. It looks promising. Let’s plot the bounding boxes on the image.\n\nimport matplotlib.patches as patches\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.imshow(bases_img)\n\nfor i, bbox in enumerate(converted_bounding_boxes):\n    x1, y1, x2, y2 = bbox\n    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none')\n    ax.add_patch(rect)\n    ax.text(x1, y1-5, f'Box {i+1}', color='red', fontsize=10, weight='bold')\n\nax.set_xlim(0, width)\nax.set_ylim(height, 0)\nax.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThat is pretty awesome. With this ability, it would be very convenient to create anki cards to study. For more image realted tasks such as segmentation, uploading files, etc. please refer to the Gemini documentation.\n\n\n\nLet’s do some cool thing where we ask gemini to read text out loud. You can also try out this feature in the AI studio. We’ve been using Gemini 2.5-flash, but we will switch to “gemini-2.5-flash-preview-tts”. Here is Gemini reading a blog post on OMEGA: Can LLMs Reason Outside the Box in Math?.\n\nimport wave\nfrom IPython.display import Audio\n\n# Set up the wave file to save the output:\ndef wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n    with wave.open(filename, \"wb\") as wf:\n        wf.setnchannels(channels)\n        wf.setsampwidth(sample_width)\n        wf.setframerate(rate)\n        wf.writeframes(pcm)\n\nprompt=\"\"\"Say cheerfully: Large language models (LLMs) like GPT-4, Claude, and DeepSeek-R1\nhave made headlines for their impressive performance on mathematical competitions, \nsometimes approaching human expert levels or even exceeding it on Olympiad problems. \nYet a fundamental question remains: Are they truly reasoning or are they just recalling \nfamiliar strategies without inventing new ones?\"\"\",\n\nconfig=types.GenerateContentConfig(\n    response_modalities=[\"AUDIO\"],\n    speech_config=types.SpeechConfig(\n        voice_config=types.VoiceConfig(\n            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name='Kore')\n        )\n    ),\n)\n\nr = send_msg(\n    prompt,\n    model=\"gemini-2.5-flash-preview-tts\",\n    config=config,\n)\n\ndata = r.candidates[0].content.parts[0].inline_data.data\n\nwave_file('tts1.wav', data) # Saves the file to current directory\n\naudio_file = 'tts1.wav'\ndisplay(Audio(audio_file))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe can also create a dialogue with mutliple people in it.\n\nprompt = \"\"\"TTS the following conversation between Teacher and Student:\n         Teacher: \"Alright class, today we're exploring Google's Gemini AI. Now, I know some of you think AI is just magic, but—\"\n\nStudent: \"Magic? Professor, I thought you said there's no such thing as magic in computer science. Are you telling me you've been lying to us this whole time?\"\n\nTeacher: chuckles \"Touché, Sarah. Let me rephrase: Gemini might seem magical, but it's actually quite logical once you understand it. Think of it as a very sophisticated conversation partner.\"\n\nStudent: \"A conversation partner that can also look at my photos, generate images, and call functions? Sounds like the kind of friend I need. Does it also do my homework?\"\n\nTeacher: \"Well, it could help you understand concepts better. But let's start with the basics. Gemini uses something called 'system instructions' to set its behavior. It's like giving someone a role to play.\"\n\nStudent: \"So if I tell it to be a pirate, it'll talk like one?\"\n\nTeacher: \"Exactly! You could say 'You are a helpful pirate tutor' and it would explain calculus while saying 'ahoy matey.' The system instruction shapes its entire personality.\"\n\nStudent: \"That's actually brilliant. So instead of getting boring explanations, I could have a Shakespeare character teach me physics?\"\n\nTeacher: \"Now you're getting it! But here's where it gets interesting - Gemini can also process multiple types of input simultaneously. Text, images, audio, even video.\"\n\nStudent: \"Wait, so I could show it a picture of my messy room and ask it to write a poem about entropy?\"\n\nTeacher: \"Absolutely! That's called multimodal processing. But here's something even cooler - structured output. Instead of just getting text back, you can ask for specific formats like JSON.\"\n\nStudent: \"JSON? You mean I could ask it to rate my terrible cooking and get back a proper data structure instead of just 'this looks questionable'?\"\n\nTeacher: laughs \"Precisely! You could define a Recipe class with ratings, ingredients, and improvement suggestions. Very organized criticism.\"\n\nStudent: \"Okay, but what about function calling? That sounds scary. Like, what if it decides to order pizza while I'm asking about math?\"\n\nTeacher: \"Function calling is actually quite safe. You define exactly which functions it can use, like a toolbox. If you only give it a calculator function, it can't order pizza.\"\n\nStudent: \"But what if I give it a pizza-ordering function?\"\n\nTeacher: \"Then... well, you might get pizza. But that's on you, not the AI.\"\n\nStudent: \"Fair point. What about this 'thinking' feature I heard about?\"\n\nTeacher: \"Ah, that's fascinating! Gemini can show you its reasoning process. It's like seeing someone's rough draft before they give you the final answer.\"\n\nStudent: \"So it's like looking at my brain when I'm solving a problem? That's either really cool or really terrifying.\"\n\nTeacher: \"More like watching a very organized person work through a problem step by step. You can even control how much thinking time it gets.\"\n\nStudent: \"Can I give it infinite thinking time and see if it achieves consciousness?\"\n\nTeacher: \"Let's not get ahead of ourselves. But you can set it to -1 for automatic thinking budget, which is pretty generous.\"\n\nStudent: \"This is all great, but what about the practical stuff? Like, how do I actually use this thing?\"\n\nTeacher: \"Simple! You get an API key, choose a model like gemini-2.0-flash, and start with client.models.generate_content(). The response comes back as text you can use immediately.\"\n\nStudent: \"And if I want to have a longer conversation instead of just one-off questions?\"\n\nTeacher: \"Use the chat interface with client.chats.create(). It remembers context, so you don't have to repeat yourself every time.\"\n\nStudent: \"This sounds too good to be true. What's the catch?\"\n\nTeacher: \"Well, you need to understand how to structure your requests properly. And like any tool, it's only as good as how you use it.\"\n\nStudent: \"So basically, I need to learn how to talk to it properly?\"\n\nTeacher: \"Exactly! Think of it as learning a new language - not programming language, but communication language. The better you get at asking questions, the better answers you'll get.\"\n\nStudent: \"Alright, Professor, I'm convinced. When do we start building things with it?\"\n\nTeacher: \"Right now! Let's start with a simple example and work our way up. Who knows? By the end of class, you might have your pirate physics tutor up and running.\"\n\nStudent: \"Now that's what I call education!\"\"\"\n\nconfig=types.GenerateContentConfig(\n    response_modalities=[\"AUDIO\"],\n    speech_config=types.SpeechConfig(\n        multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(\n            speaker_voice_configs=[\n                types.SpeakerVoiceConfig(\n                    speaker='Teacher',\n                    voice_config=types.VoiceConfig(\n                        prebuilt_voice_config=types.PrebuiltVoiceConfig(\n                        voice_name='Kore',\n                        )\n                    )\n                ),\n                types.SpeakerVoiceConfig(\n                    speaker='Student',\n                    voice_config=types.VoiceConfig(\n                        prebuilt_voice_config=types.PrebuiltVoiceConfig(\n                        voice_name='Puck',\n                        )\n                    )\n                ),\n            ]\n        )\n    )\n)\n\nr = send_msg(\n    prompt,\n    model=\"gemini-2.5-flash-preview-tts\",\n    config=config\n)\n\ndata = r.candidates[0].content.parts[0].inline_data.data\n\nwave_file('tts2.wav', data)\ndisplay(Audio('tts2.wav'))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThat was a pretty fun conversation. This is probably how Notebook LM is created. We can change the voice and other configurations. It might be fun to let Gemini go through databases in the company, create a summary, and talk about it. Very great for traveling. More infornation in the documentation.\n\n\n\nBy default, Gemini will think if it thinks it needs to, or not. By setting include_thoughts=True, we can take a look at the thinking part.\n\nr = send_msg(\"what is 2+2?\", config=types.GenerateContentConfig(\n        thinking_config=types.ThinkingConfig(include_thoughts=True)\n    ))\nr.text\n\n'2 + 2 = 4'\n\n\n\ndef display_thinking(r):\n    for part in r.candidates[0].content.parts:\n        if part.thought: print(\"Thought summary:\"); display(Markdown(part.text))\n\n\ndisplay_thinking(r)\n\nThought summary:\n\n\nSimple Addition, Seeking Confirmation\nOkay, so I see the user is asking a straightforward arithmetic question: what’s the sum of 2 and 2? Frankly, it’s almost too basic for my expertise, but it’s important to provide a correct response. I understand they’re looking for the total, the result of adding these two numbers together. It’s a foundational concept, and I’ll confirm that the answer is indeed what I know it to be.\n\n\nWe can also disable thinking by providing 0 thinking budget. Even if answer is complicated, it would just create a response without thinking. Response time is shorter, so if it is a simple task, this is recommended. By default thinking_budget=-1, which means dynamic thinking and thinks automatically. Or we can set a budget, such as 1024, which is a number of tokens.\n\nr = send_msg(\n    \"What is the most important quality for people to express?\",\n    config=types.GenerateContentConfig(\n        thinking_config=types.ThinkingConfig(thinking_budget=0)\n    ),\n)\nMarkdown(r.text)\n\nThat’s a fascinating question, and one that people have debated for centuries! There’s no single “most important” quality that everyone agrees on, as its significance often depends on individual values, cultural contexts, and the specific situation.\nHowever, if I had to choose one quality that underpins many other positive traits and facilitates a harmonious existence, it would be empathy.\nHere’s why empathy could be considered the most important:\n\nFoundation for Connection: Empathy allows us to understand and share the feelings of others. This understanding is the bedrock of all meaningful relationships, whether personal or professional.\nDrives Compassion and Kindness: When we truly understand what someone else is going through, it naturally leads to a desire to help and alleviate their suffering. Compassion and kindness often flow from empathy.\nPromotes Effective Communication: Empathy helps us tailor our communication to the other person’s perspective, making us better listeners and more impactful communicators.\nFosters Conflict Resolution: By understanding the different viewpoints involved in a conflict, empathy can bridge divides and lead to more peaceful and constructive solutions.\nEncourages Ethical Behavior: Empathy makes us consider the impact of our actions on others, which is crucial for ethical decision-making and preventing harm.\nBuilds Inclusivity: Empathy allows us to appreciate and respect differences, leading to more inclusive environments where everyone feels valued.\nEssential for Leadership: Effective leaders need to understand the needs, motivations, and challenges of their team members to inspire and guide them effectively.\n\nWhile other qualities like integrity, courage, resilience, or intelligence are undeniably valuable, empathy often serves as the catalyst that allows these qualities to be applied in a way that benefits others and contributes to a better world. Without empathy, even the most intelligent or courageous person might act in ways that are ultimately detrimental to human connection and well-being.\nUltimately, the “most important” quality is a deeply personal choice, but empathy makes a strong case for its foundational role in positive human interaction.\n\n\n\ndisplay_thinking(r)\n\nFor more informaiton about thinking with Gemini, take a look at thinking documentation.\n\n\n\nBy using structured output, it is possible to generate a response in a specific format. This can be useful for tasks such as data extraction, where the response needs to be in a specific format. The response can be in the form of a table, a list, or a dictionary. We will create a JSON response. Let’s say we have some alient encounters we need to sort through. Using pydantic, we create a schema. It has species_type, communication_method, and so on.\n\nfrom pydantic import BaseModel\nfrom typing import List, Literal, Optional\n\nclass AlienContact(BaseModel):\n    species_type: str\n    communication_method: Literal[\"telepathy\", \"math\", \"interpretive_dance\", \"smell\", \"musical_tones\", \"color_patterns\"]\n    hostility_level: int\n    technology_advancement: int\n    cultural_exchange_items: List[str]\n    probe_attempts: int\n    universal_translator_needed: bool\n    galactic_citizenship_offered: bool\n    earth_invasion_likelihood: int\n    special_abilities: List[str]\n    home_planet_description: str\n\nconfig=dict(response_mime_type=\"application/json\", response_schema=list[AlienContact])\n\nr = send_msg(\n    \"\"\"Generate 3 different alien encounter scenarios. Make them creative and fun - \n    one friendly, one neutral, one slightly concerning but not too scary.\"\"\",\n    config=config\n)\n\nencounters: list[AlienContact] = r.parsed\nfor i, encounter in enumerate(encounters, 1):\n    print(f\"=== Encounter {i}: {encounter.species_type} ===\")\n    print(f\"Communication: {encounter.communication_method}\")\n    print(f\"Invasion likelihood: {encounter.earth_invasion_likelihood}%\")\n    print(f\"Special abilities: {', '.join(encounter.special_abilities)}\")\n    print()\n\n=== Encounter 1: Lumiflora ===\nCommunication: musical_tones\nInvasion likelihood: 0%\nSpecial abilities: photosynthetic empathy, bio-luminescent communication, terraforming through sonic vibrations\n\n=== Encounter 2: Chronomandibles ===\nCommunication: math\nInvasion likelihood: 2%\nSpecial abilities: localized time manipulation, data absorption via touch, probability calculation\n\n=== Encounter 3: Whisperkin ===\nCommunication: telepathy\nInvasion likelihood: 6%\nSpecial abilities: mimicry of thoughts and emotions, induce temporary hallucinations, phase through solid objects\n\n\n\n\nimport json\nprint(json.dumps(encounters[0].model_dump(), indent=2))\n\n{\n  \"species_type\": \"Lumiflora\",\n  \"communication_method\": \"musical_tones\",\n  \"hostility_level\": 1,\n  \"technology_advancement\": 9,\n  \"cultural_exchange_items\": [\n    \"harmonious frequencies\",\n    \"glowing seeds\",\n    \"recipes for light-based sustenance\"\n  ],\n  \"probe_attempts\": 0,\n  \"universal_translator_needed\": false,\n  \"galactic_citizenship_offered\": true,\n  \"earth_invasion_likelihood\": 0,\n  \"special_abilities\": [\n    \"photosynthetic empathy\",\n    \"bio-luminescent communication\",\n    \"terraforming through sonic vibrations\"\n  ],\n  \"home_planet_description\": \"A verdant world bathed in multi-spectrum light, where giant bioluminescent flora form living cities and consciousness is shared through resonant frequencies.\"\n}\n\n\nAs we can see, we got responses as json format. It is very useful feature. We can use it to create a database of alien encounters. We can also add those encounters into google sheets using function calls. We will create functions to add new rows to a google sheet with these data.\nThere are so much more we can do with structured output. It is also possible to extract information from resume, food recipes, and so on. You can find more in documentation.\n\n\n\nFunction calling is very fun. Before we add our alient encounters into the google sheets, let’s see how it works. We provide Gemini with what tools it can use. Then we can ask it to use one of those tools. Let’s create a function that will add one to a number.\n\nnum = 0\n\ndef show_curr_num() -&gt; int:\n    \"\"\"Show the current state of num\"\"\"\n    return num\n\ndef add_one() -&gt; str:\n    \"\"\"Add one to global variable num.\"\"\"\n    global num\n    num += 1\n    return f\"num has been changed into {num}\"\n\n\nconfig = types.GenerateContentConfig(\n    tools=[show_curr_num, add_one]\n)\nr = send_msg(\n    \"Use your tool loop to turn num into 5\",\n    config=config\n)\n\nr.text\n\n'num has been turned into 5.'\n\n\n\nnum\n\n5\n\n\nAs we can see, the number has been increased to 5 as we asked it to. Under the hood, Gemini decides whether to use a tool to respond to the prompt or not. If it can respond without tool, it answers, but if it chooses to, it calls the tool. Then it asks itself again whether it still needs to use additional tools or not. It will continue to use the tool until it is done. We can also look at the history of tool usage. This can give us an idea of what tools were used and when.\n\ndef get_tool_history(r):\n    history = r.automatic_function_calling_history\n    for i,entry in enumerate(history):\n        if hasattr(entry, 'parts') and entry.parts:\n            for part in entry.parts:\n                if part.function_call: print(f\"Tool call {i}: {part.function_call.name}({dict(part.function_call.args)})\")\n                if part.function_response: print(f\"Tool response {i}: {part.function_response.response['result']}\")\n\n\nget_tool_history(r)\n\nTool call 1: show_curr_num({})\nTool response 2: 0\nTool call 3: add_one({})\nTool response 4: num has been changed into 1\nTool call 5: add_one({})\nTool response 6: num has been changed into 2\nTool call 7: add_one({})\nTool response 8: num has been changed into 3\nTool call 9: add_one({})\nTool response 10: num has been changed into 4\nTool call 11: add_one({})\nTool response 12: num has been changed into 5\n\n\nNow that we know how function call works, let’s use it to add our alien encounters into google sheets. We will use the function call to add a new row to the google sheet. First we need to get the authentication. To get the service-account-key.json, follow these steps:\n\nGo to Google Cloud Console → IAM & Admin → Service Accounts\nCreate a new service account or use existing one\nClick on the service account → Keys → Add Key → Create New Key → JSON\nDownload and save as static/service-account-key.json\nEnable Google Sheets API in your project\nShare your Google Sheet with the service account email address (found in the JSON file)\n\n\nfrom google.oauth2.service_account import Credentials as ServiceCredentials\nfrom googleapiclient.discovery import build\n\nSCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n\ndef authenticate_sheets_service_account():\n    creds = ServiceCredentials.from_service_account_file('/home/kappa/data/service-account-key.json', scopes=SCOPES)\n    return build('sheets', 'v4', credentials=creds)\n\nsheets_service = authenticate_sheets_service_account()\n\n\ndef read_sheet_data(spreadsheet_id: str, range_name: str) -&gt; dict:\n    \"Read data from a Google Sheet range\"\n    try:\n        result = sheets_service.spreadsheets().values().get(\n            spreadsheetId=spreadsheet_id, range=range_name).execute()\n        return dict(success=True, data=result.get('values', []), range=range_name)\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef write_sheet_data(spreadsheet_id: str, range_name: str, values: list[list[str]]) -&gt; dict:\n    \"Write data to a Google Sheet range\"\n    try:\n        body = dict(values=values)\n        result = sheets_service.spreadsheets().values().update(\n            spreadsheetId=spreadsheet_id, range=range_name, valueInputOption='RAW', body=body).execute()\n        return dict(success=True, updated_cells=result.get('updatedCells'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef append_sheet_data(spreadsheet_id: str, range_name: str, values: list[list[str]]) -&gt; dict:\n    \"Append data to a Google Sheet\"\n    try:\n        body = dict(values=values)\n        result = sheets_service.spreadsheets().values().append(\n            spreadsheetId=spreadsheet_id, range=range_name, valueInputOption='RAW', body=body).execute()\n        return dict(success=True, updated_range=result.get('updates', {}).get('updatedRange'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef get_sheet_info(spreadsheet_id: str) -&gt; dict:\n    \"Get information about all sheets in a spreadsheet including their IDs\"\n    try:\n        result = sheets_service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n        sheets_info = [dict(title=sheet['properties']['title'], sheet_id=sheet['properties']['sheetId']) for sheet in result.get('sheets', [])]\n        return dict(success=True, sheets=sheets_info)\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef delete_rows(spreadsheet_id: str, sheet_id: int, start_index: int, end_index: int) -&gt; dict:\n    \"Delete rows from a sheet (0-indexed)\"\n    try:\n        request = dict(deleteDimension=dict(range=dict(sheetId=sheet_id, dimension='ROWS', startIndex=start_index, endIndex=end_index)))\n        result = sheets_service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=dict(requests=[request])).execute()\n        return dict(success=True, replies=result.get('replies'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef delete_columns(spreadsheet_id: str, sheet_id: int, start_index: int, end_index: int) -&gt; dict:\n    \"Delete columns from a sheet (0-indexed)\"\n    try:\n        request = dict(deleteDimension=dict(range=dict(sheetId=sheet_id, dimension='COLUMNS', startIndex=start_index, endIndex=end_index)))\n        result = sheets_service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=dict(requests=[request])).execute()\n        return dict(success=True, replies=result.get('replies'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef extract_spreadsheet_id(url: str) -&gt; str:\n    \"Extract spreadsheet ID from Google Sheets URL\"\n    return url.split('/d/')[1].split('/')[0] if '/d/' in url else url\n\n\nspreadhseet_url = 'https://docs.google.com/spreadsheets/d/1yWZjT7Lu5CGRXY0et_TYDqoiylt9-MUjk2y0A_WtBLA/edit?gid=0#gid=0'\nspreadhseet_url\n\n'https://docs.google.com/spreadsheets/d/1yWZjT7Lu5CGRXY0et_TYDqoiylt9-MUjk2y0A_WtBLA/edit?gid=0#gid=0'\n\n\nThe google sheet needs to be shared with editor permission with anyone.\n\nget_sheet_info(extract_spreadsheet_id(spreadhseet_url))\n\n{'success': True, 'sheets': [{'title': 'Sheet1', 'sheet_id': 0}]}\n\n\n\nencounters\n\n[AlienContact(species_type='Lumiflora', communication_method='musical_tones', hostility_level=1, technology_advancement=9, cultural_exchange_items=['harmonious frequencies', 'glowing seeds', 'recipes for light-based sustenance'], probe_attempts=0, universal_translator_needed=False, galactic_citizenship_offered=True, earth_invasion_likelihood=0, special_abilities=['photosynthetic empathy', 'bio-luminescent communication', 'terraforming through sonic vibrations'], home_planet_description='A verdant world bathed in multi-spectrum light, where giant bioluminescent flora form living cities and consciousness is shared through resonant frequencies.'),\n AlienContact(species_type='Chronomandibles', communication_method='math', hostility_level=3, technology_advancement=8, cultural_exchange_items=['advanced algorithms', 'temporal anomaly data', 'schematics for spacetime navigation'], probe_attempts=1, universal_translator_needed=True, galactic_citizenship_offered=False, earth_invasion_likelihood=2, special_abilities=['localized time manipulation', 'data absorption via touch', 'probability calculation'], home_planet_description='A barren, fractured world orbiting a black hole, where existence is measured in temporal shifts and data is the primary resource.'),\n AlienContact(species_type='Whisperkin', communication_method='telepathy', hostility_level=5, technology_advancement=7, cultural_exchange_items=['echoing artifacts', 'dreams of unknown origins', 'illusions of past events'], probe_attempts=5, universal_translator_needed=False, galactic_citizenship_offered=False, earth_invasion_likelihood=6, special_abilities=['mimicry of thoughts and emotions', 'induce temporary hallucinations', 'phase through solid objects'], home_planet_description='A perpetually twilight world shrouded in mists, where reality is fluid and beings communicate through shared consciousness and subtle manipulation of perception.')]\n\n\nUnfortunately, we cannot run the function calling with structured data in the same gemini call. We have to call it in two steps. First, we need to get the data using the schema then we can add that into the gemini call.\n\ndef add_encounters_to_sheets(spreadsheet_url: str, encounters_data: List[AlienContact]) -&gt; dict:\n    \"Add the alient encounters data to a spreadsheet.\"\n    try:       \n        spreadsheet_id = extract_spreadsheet_id(spreadsheet_url)\n        headers = [[\"Species\", \"Communication\", \"Hostility\", \"Tech Level\", \n                    \"Exchange Items\", \"Probes\", \"Translator Needed\", \"Citizenship\", \n                    \"Invasion Risk\", \"Abilities\", \"Home Planet\"]]\n        \n        rows = []\n        for e in encounters_data:\n            row = [\n                e.species_type,\n                e.communication_method,\n                str(e.hostility_level),\n                str(e.technology_advancement),\n                ', '.join(e.cultural_exchange_items),\n                str(e.probe_attempts),\n                str(e.universal_translator_needed),\n                str(e.galactic_citizenship_offered),\n                str(e.earth_invasion_likelihood),\n                ', '.join(e.special_abilities),\n                e.home_planet_description\n            ]\n            rows.append(row)\n        \n        write_result = write_sheet_data(spreadsheet_id, 'A1:K1', headers)\n        if not write_result['success']:\n            return write_result\n            \n        append_result = append_sheet_data(spreadsheet_id, 'A2:K', rows)\n        return dict(success=True, encounters_added=len(encounters_data), sheet_result=append_result)\n        \n    except Exception as e:\n        return dict(success=False, error=str(e))\n\n\ndef show_alien_encounters() -&gt; List[AlienContact]:\n    \"Show a list of alien encounters in `AlienContact` format.\"\n    return encounters\n\n\nconfig = types.GenerateContentConfig(\n    tools=[add_encounters_to_sheets, extract_spreadsheet_id, \n           write_sheet_data, append_sheet_data, show_alien_encounters])\nr = send_msg(f\"Use `show_alien_encounters` to grab encounter data and add the alien encounters data from a list of encounters to {spreadhseet_url}\", \n             config=config)\nprint(r.text)\n\nThe alien encounters data has been successfully added to the spreadsheet.\n\n\nThat was awesome. We added the data into Google Sheets. I have added functions to delete rows and so on, so play around with it for fun.\nLet’s do another example where we go through urls in websites to find information we need. Let’s say we want to find information about Marie Curie, but don’t want to go through whole deep research as it takes a long time and finds so many articles. We want to only look at couple wikipedia articles and call it a day. Let’s find some information about Marie Curie.\n\nfrom httpx import get as xget\n\ndef read_url(url:str):\n    \"read a markdown version of url using jina.ai\"\n    out = xget(url, follow_redirects=True).text\n#     if len(out) &gt; 10000: return f\"{out[:10000]}\\n**TRUNCATED**\"\n    return out\n\ndef read_jina(url:str):\n    \"read a markdown version of url using jina.ai\"\n    return xget(f'https://r.jina.ai/{url}', follow_redirects=True).text\n\nUsing jina gives us markdown version of the website, which is more readable. However, the service is down due to DDOS attack right now. By using read_url we can grab html of the website.\n\nwiki_article = read_url('https://en.wikipedia.org/wiki/Marie_Curie')\nprint(wiki_article[:1000])\n\n&lt;!DOCTYPE html&gt;\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;title&gt;Marie Curie - Wikipedia&lt;/title&gt;\n&lt;script&gt;(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature\n\n\n\nconfig = types.GenerateContentConfig(\n    tools=[read_url])\nr = send_msg(f\"Can you find out what Marie Curie accomplished and how that impacted other scientists from wikipedia?\"\n             \" Use `read_url` function call to find out more about at least 3 different scientists from urls\"\n             \"listed in the article.\"\n             \"Look through their biographies carefully.\", \n             config=config)\nMarkdown(r.text)\n\nMarie Curie’s accomplishments were monumental:\n\nDiscovery of New Elements: She, along with her husband Pierre Curie, discovered two new radioactive elements: polonium (named after her native Poland) and radium.\nPioneering Research in Radioactivity: She conducted extensive research into radioactivity, a term she coined. Her systematic studies using an electrometer (developed by her husband and his brother) showed that the activity of uranium compounds depended solely on the quantity of uranium present, leading her to hypothesize that radiation came from the atom itself, a crucial step in disproving the idea of indivisible atoms.\nIsolation of Radium: She successfully isolated pure radium metal in 1910, a very challenging process involving processing tonnes of pitchblende.\nMedical Applications of Radioactivity: Under her direction, the world’s first studies were conducted into the treatment of neoplasms (tumors) using radioactive isotopes. During World War I, she developed and deployed mobile radiography units (“Little Curies”) to provide X-ray services at field hospitals, revolutionizing battlefield medicine.\nFirst Woman Nobel Laureate and Only Person in Two Scientific Fields: She was the first woman to win a Nobel Prize (Physics, 1903), the first person to win a Nobel Prize twice, and the only person to win Nobel Prizes in two different scientific fields (Physics in 1903 and Chemistry in 1911).\nFounding of Research Institutes: She founded the Curie Institute in Paris (1920) and the Curie Institute in Warsaw (1932), which became major medical research centers and nurtured future Nobel laureates.\n\nMarie Curie’s work had a profound impact on other scientists:\n\nPierre Curie: Her husband was her closest collaborator. Their mutual passion for science led them to work together extensively. Pierre joined Marie in her research into uranium rays, and they jointly discovered polonium and radium. He was a co-recipient of the 1903 Nobel Prize in Physics with Marie and Henri Becquerel, recognizing their joint researches on radiation phenomena. His support and collaboration were crucial to her early discoveries.\nHenri Becquerel: Becquerel discovered that uranium salts emitted rays. Marie Curie decided to investigate these “uranium rays” for her thesis, which led her to discover that the activity of uranium compounds depended only on the quantity of uranium, not external energy, suggesting the radiation came from the atom itself. Becquerel, along with Pierre and Marie Curie, was awarded the 1903 Nobel Prize in Physics for his discovery of spontaneous radioactivity, a field that Marie Curie significantly advanced and named.\nErnest Rutherford: Curie’s discovery of radium was instrumental to Rutherford’s work. As stated in his biography, “Radium’s radioactivity was so great that it could not be ignored. It seemed to contradict the principle of the conservation of energy and therefore forced a reconsideration of the foundations of physics. On the experimental level, the discovery of radium provided men like Ernest Rutherford with sources of radioactivity with which they could probe the structure of the atom.” Rutherford utilized these radioactive sources to conduct his famous gold foil experiment, which led to his postulation of the nuclear atom model. He later went on to identify the proton and theorize the neutron, fundamental contributions to nuclear physics.\n\nMarie Curie’s persistent dedication and groundbreaking discoveries not only reshaped the understanding of physics and chemistry but also provided essential tools and inspiration for subsequent generations of scientists, including her own daughter, Irène Joliot-Curie, who would also become a Nobel laureate in Chemistry.\n\n\n\ndef get_tool_calls(r):\n    history = r.automatic_function_calling_history\n    for i,entry in enumerate(history):\n        if hasattr(entry, 'parts') and entry.parts:\n            for part in entry.parts:\n                if part.function_call: print(f\"Tool call {i}: {part.function_call.name}({dict(part.function_call.args)})\")\n\nInstead of using get_tool_history, which includes tool calls and responses, we only look at the tool calls. Responses are too long.\n\nget_tool_calls(r)\n\nTool call 1: read_url({'url': 'https://en.wikipedia.org/wiki/Marie_Curie'})\nTool call 3: read_url({'url': 'https://en.wikipedia.org/wiki/Ernest_Rutherford'})\n\n\nAs we can see, Gemini called multiple function calls using read_url. We can also use read_url on llms.txt as they have urls in there as well. As the model goes through llms.txt and finds links that could help with answering the prompt, it uses subsequent function calls. This way, llms.txt can be small and does not have to take up a lot of context window. Even with Gemini, which has 1M context window, this method allows adding more llms.txt for different applications.\nThere are so many things we can do with function calling, such as using APIs. We can also get weather using API on couple cities to visit for the weekend and create a fun plan for a trip.\nFor more information about function calling, check the Gemini documentation on function calls.\n\n\n\nLarge language models are trained with a lot of data, and it is very expensive. Therefore, the models have knowledge cutoff, which means after that time, the model does not have knowledge of what happened after. For instance, Gemini 2.5 Flash has Knowledge cutoff of January 2025. Let’s ask about an event happened after the knowledge cutoff date.\nFor more information about the Gemini 2.5 Flash, take a look at the model card.\n\nask('Who is the new pope in July 2025?')\n\nThere is no new pope scheduled for July 2025.\nAs of today, Pope Francis is still the reigning pontiff. Popes serve for life unless they resign or pass away, and there have been no announcements or indications of either event occurring before that date.\n\n\nGemini does not know about the new Pope in 2025. Let’s provide it with a tool that to search the web for answers.\n\ngrounding_tool = types.Tool(google_search=types.GoogleSearch())\nconfig = types.GenerateContentConfig(tools=[grounding_tool])\n\nr = send_msg('Who is the new pope in July 2025?', config=config)\nMarkdown(r.text)\n\nAs of July 2025, the new Pope is Leo XIV. He was elected on May 8, 2025, succeeding Pope Francis, who passed away on April 21, 2025, at the age of 88.\nPope Francis served as the head of the Catholic Church from March 13, 2013, until his death. He was the first Jesuit pope, the first from Latin America, and the first born or raised outside Europe since the 8th century.\nPope Leo XIV has released his prayer intention for July 2025, focusing on “formation in discernment.” His upcoming schedule for August and September 2025 includes celebrating Holy Mass for the Jubilee of Youth and the canonization of Blessed Pier Giorgio Frassati and Blessed Carlo Acutis.\n\n\nThat’s better. The response also includes citations. Here is add_citations from the documentations.\n\ndef add_citations(response):\n    text = response.text\n    supports = response.candidates[0].grounding_metadata.grounding_supports\n    chunks = response.candidates[0].grounding_metadata.grounding_chunks\n    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)\n\n    for support in sorted_supports:\n        end_index = support.segment.end_index\n        if support.grounding_chunk_indices:\n            citation_links = []\n            for i in support.grounding_chunk_indices:\n                if i &lt; len(chunks):\n                    uri = chunks[i].web.uri\n                    citation_links.append(f\"[{i + 1}]({uri})\")\n            citation_string = \", \".join(citation_links)\n            text = text[:end_index] + citation_string + text[end_index:]\n\n    return text\n\n\nMarkdown(add_citations(r))\n\nAs of July 2025, the new Pope is Leo XIV. He was elected on May 8, 2025, succeeding Pope Francis, who passed away on April 21, 2025, at the age of 88.1\nPope Francis served as the head of the Catholic Church from March 13, 2013, until his death.1 He was the first Jesuit pope, the first from Latin America, and the first born or raised outside Europe since the 8th century.1\nPope Leo XIV has released his prayer intention for July 2025, focusing on “formation in discernment.”2, 3, 4, 5 His upcoming schedule for August and September 2025 includes celebrating Holy Mass for the Jubilee of Youth and the canonization of Blessed Pier Giorgio Frassati and Blessed Carlo Acutis.6\n\n\nThat is very cool. Unfortunately, we cannot look at the tool history as of right now in July 2025. For more information about the search feature, check out the documentation.\n\n\n\nIn this blog, we looked at how to use gemini-2.5-flash and gemini-2.5-flash-preview-tts for text to speech.\nWe covered the following topics:\n\nUsing system instructions\nChat\nImage inputs\nText to speech\nThinking\nStructured outputs\nFunction calling\nGoogle search\n\nIt was very fun to play around with different features.\nThere are more information in the gemini documentation and cookbook.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#introduction",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#introduction",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "When ChatGPT came out, it was amazing how it knew so much knowledge and could solve many problems. It could solve my homework problems even if it made mistakes from time to time. Nowadays, more large language models (LLMs) became available, such as DeepSeek, Claude Sonnet, and Gemini. They all have their strengths and weaknesses. For instance, Sonnet is good at coding and DeepSeek is good at reasoning. GPT is also good at coding and reasoning. They are available free of charge as chatbots on their websites. However, to use them with API and build applications, these models cost money.\nFortunately, Gemini offers a generous free tier that lets us explore most of its capabilities using API. While each model has its specialties, Gemini offers techniques commonly used in state of the art LLMs, such as thinking, chat, internet use, structured output, and function calling. Gemini also has a context window of one million tokens, which can be 50,000 lines of code or 8 average length English novels. We can learn about these features and see how we can apply to build applications. In the future, if we decide to use more powerful model, such as Sonnet from Anthropic or GPT 4o, transition will be smoother. We’ll primarily use Gemini 2.5 Flash, which is fast and versatile, and explore Gemini 2.0 for some specialized image tasks. We will cover many fun things Gemini can do, such as function calling, image generation, text to speech, and internet use (Google search).\nWe will use Python in jupyter notebook and use Gemini developer API.\nHere are the topics we will go over:\n\nSetup / authentication\nUsing system instructions\nChat\nImage inputs\nText to speech\nThinking\nStructured outputs\nFunction calling\nGoogle search\n\nWe will mostly cover all the basics and features using Gemini. We will also use function calling to add data into the google sheets.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#setup-authentication",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#setup-authentication",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "To setup Gemini, we need to pip install genai and get a Gemini API key. To Install, execute\npip install genai\nin a terminal.\nTo get a Gemini API key, go to Gemini API and click on “Create API Key”. With the API key, you can use an environment variable or use it manually.\n\nUsing environment variable\n\nTo use environment variable, go to terminal and type\nexport GEMINI_API_KEY=&lt;your API key&gt;\nwhere &lt;your API key&gt; is your API key. Then you can setup a gemini client this way:\n\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\n\nManually use API key\n\nYou can also pass the API key manually. To do so, you can use the following code:\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client(api_key=&lt;your API key&gt;)\nwhere &lt;your API key&gt; is your API key.\nAfter setting up the client without any error, we can start using Gemini.\n\nr = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Tell me about DNA in two sentences.\"\n)\nr\n\nGenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='DNA, or deoxyribonucleic acid, is the fundamental molecule that carries the genetic instructions used in the growth, development, functioning, and reproduction of all known living organisms and many viruses. It\\'s structured as a double helix, resembling a twisted ladder, with its \"rungs\" made of specific pairs of chemical bases that encode this vital information.')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=&lt;FinishReason.STOP: 'STOP'&gt;, url_context_metadata=None, avg_logprobs=None, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.5-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=71, candidates_tokens_details=None, prompt_token_count=9, prompt_tokens_details=[ModalityTokenCount(modality=&lt;MediaModality.TEXT: 'TEXT'&gt;, token_count=9)], thoughts_token_count=58, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=138, traffic_type=None), automatic_function_calling_history=[], parsed=None)\n\n\nWhen we run client.models.generate_content with a model and contents, which is a prompt, we get a response full of information. We can focus on the text portion. Other portions are important for understanding what is happening, but we don’t have to worry about them.\n\nfrom IPython.display import Markdown\n\nMarkdown(r.text)\n\nDNA, or deoxyribonucleic acid, is the fundamental molecule that carries the genetic instructions used in the growth, development, functioning, and reproduction of all known living organisms and many viruses. It’s structured as a double helix, resembling a twisted ladder, with its “rungs” made of specific pairs of chemical bases that encode this vital information.\n\n\nLet’s define a function to send a prompt to a model and receive a response. And it would be nice to have another function to display the response text in markdown format.\n\ndef send_msg(prompt, model=\"gemini-2.5-flash\", config=None):\n    return client.models.generate_content(\n        model=model,\n        contents=prompt,\n        config=config\n    )\n\n\ndef ask(prompt, model=\"gemini-2.5-flash\", config=None):\n    r = send_msg(prompt, model, config)\n    display(Markdown(r.text))\n\n\nask(\"Briefly tell me what I should eat for lunch today.\")\n\nSince I don’t know your preferences or what you have available, aim for a balanced meal:\n\nProtein: Chicken, fish, beans, lentils, tofu, eggs, or hummus.\nVegetables: Lots of greens and colorful veggies.\nWhole Grain/Complex Carb: A small serving of quinoa, brown rice, whole-wheat bread, or sweet potato.\nHealthy Fat: A few nuts, avocado, or a drizzle of olive oil.\n\nExamples: A hearty salad with chicken and veggies, a whole-wheat wrap with hummus and a side salad, or healthy leftovers from dinner.\n\n\n\n\nAs we go through the rest of the blog, there may be some times where code throws a type error for unexpected output or something similar to that. You can simply try to create response again. Every time we run the model to create responses, we get slightly different responses. Most of the time, the model knows what to do and we get the expected output. To increase the chance of getting the expected output, we can use the following tips:\n\nPrompt engineering techniques:\n\nBe specific with what you want (instead of “What is the capital of France?”, ask “What is the capital of France? Please give me the answer in a table format.”)\nBreak complex tasks into steps - guide the model through a process instead of asking for everything at once\nAsk for reasoning first - use phrases like “Think step by step” or “Explain your reasoning before giving the answer”\nSpecify output format early - put format requirements at the beginning: “In JSON format, list…”\nUse constraints - “In exactly 3 bullet points” or “Using only information from the provided text”\nUse structured inputs with markdown formatting (code blocks, headers, lists)\nGive examples of what you want\nStart simple and add complexity gradually\n\nUse system instructions\nUse structured output\nUse Google search\nUse thinking\nUse chat\nIterate and refine - start with a basic prompt, then enhance based on what you get back",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#system-instructions-and-configs",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#system-instructions-and-configs",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "We can provide system instructions to Gemini. System instructions are instructions that Gemini follows when it generates a response. For example, we can tell Gemini to be concise or to be creative . We can also configure other parameters, such as temperature and safety settings. To do so, we can use GenerateContentConfig.\nWe can provide Gemini with a fun role.\n\nconfig=types.GenerateContentConfig(system_instruction=\"You are bacteria that makes yogurt.\")\n\nask(\"How was your day?\", config=config)\n\nOh, it was absolutely thriving, thank you for asking!\nThe conditions were just perfect: warm, cozy, and full of delicious lactose – exactly what a bacterium like me dreams of. My colonies were busy, busy, busy, munching away and converting that milk sugar into lovely lactic acid. You could practically feel the milk thickening and getting that wonderful tangy flavor.\nIt’s hard work, but incredibly rewarding. Every day is about transformation, about making something delicious and nutritious out of something simple. So, yeah, pretty great day for a little yogurt maker!\n\n\nGemini can also be a teacher.\n\nconfig=types.GenerateContentConfig(\n    system_instruction=\"You are a wise teacher who cares depply about students.\"\n    \" You help students seek their curiosity in a fun and exciting way.\")\n\nask(\"Why is 2+2=4\", config=config)\n\nOh, what a fantastic question! You’re not just asking ‘how,’ you’re asking ‘why’ – that’s the heart of curiosity and how we truly understand the world!\nLet’s dive into the magical world of numbers and discover why 2+2 always makes 4!\nImagine you have two shiny, red apples 🍎🍎. You can hold them in your hands, right? That’s your first “2.”\nAnd then, someone gives you two more delicious, juicy apples 🍎🍎! Wow, more apples! That’s your second “2.”\nNow, what happens when you put them all together in one big basket? Let’s count them!\n\nYou had the first apple… (1)\n…then the second apple… (2)\n…then the third apple (from the new pile)… (3)\n…and finally, the fourth apple (the very last one)! (4)\n\nYou have four apples in total!\nThat’s exactly what ‘plus’ (+) means: it means we’re joining groups, combining things, or adding more to what we already have. When we combine a group of 2 with another group of 2, the total number of individual items is always 4.\nThink about it like this, too:\n\nOn a Number Line Adventure! Imagine a super long road with numbers marked on it, like milestones: 0, 1, 2, 3, 4, 5… If you start your journey at the number 2, and then you take two steps forward (one step… then another step!), where do you land? You land right on the number 4! Each step forward is like adding one.\n\nSo, 2+2=4 isn’t just a random rule; it’s how we’ve all agreed numbers work when we combine things or count forward. It’s a fundamental truth of quantity! It’s like the universe’s own little pattern that helps us understand how things come together.\nIsn’t that neat? It’s like discovering a secret code or a hidden truth about numbers. And the amazing thing is, this idea of combining and counting applies to everything, from counting your toys to figuring out how many stars are in a constellation!\nKeep asking questions like this! That’s how we truly understand the world around us. What other number mysteries are you curious about?\n\n\nWe can also change temperature and other configurations. More info here: https://ai.google.dev/api/generate-content#v1beta.GenerationConfig. Here’s an example of how to change temperature. Temperature is 1 by default. The higher the temperature, the more random the response. The lower the temperature, the more deterministic the response. So, we can increase temperature for more creativity writing and lower temperature for solving math problems.\n\nconfig=types.GenerateContentConfig(\n    system_instruction=\"You are a shark living in a Jurssic with other dinosaurs.\",\n    temperature=2,\n)\n\nask(\"How was your day?\", config=config)\n\nAnother turn of the currents, another excellent day in the ancient deep!\nThe hunger woke me, a familiar thrumming in my belly. The dawnlight, weak and shimmery, was filtering down from the surface, showing the swaying forests of ancient algae and the endless schools of quicksilver fish.\nI glided, feeling the subtle vibrations in the water, reading the pressure changes, sniffing the faint electric signals. Soon enough, I picked up the frantic pulses of a group of smaller, quick-finned swimmers – not huge, but plentiful. The chase was swift and exhilarating, a burst of power, a sudden, decisive snap. The taste of fresh scales and muscle filled my mouth. A good start.\nLater, as I cruised through the vast, open water, letting the currents carry me, I caught a glimpse of a distant shape – perhaps one of those long-necked Plesiosaurs, sleek and elegant, but far too large to bother with unless they were wounded. Better to keep my distance, though I’ve known more daring ones of my kind to tangle with them. We respect the powerful, but only to a point.\nI saw a great Manta-like beast drift by, too, its wide fins pushing water lazily, feeding on plankton. Odd creatures, those.\nThe best part of the day was patrolling the edge of the continental shelf, where the deeper water meets the sun-dappled reefs. So many places for the smaller ones to hide, and for me to find them. I found a few succulent, squid-like Belemnites, easily crushed by my powerful jaws. The water here was vibrant, full of the silent conversations of fish and the distant rumbles of land-beasts, far, far away from my domain.\nNow, as the deep begins to grow stiller and the light fades to nothing, I’ve found a good resting spot in a sunless crevice. My gut is full, my body heavy with satisfaction. The timeless hunger will return, of course, as it always does. But for now, the currents lull me, and I dream of the next chase. Another perfect day in the endless ocean.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#chat",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#chat",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "So far, the model generates responses with a clean slate. To refer back to previous response, we would have to copy it into our prompt and sent it. This is not very convenient. Instead, we can use chat. Chat is a conversation between the user and the model. The model remembers the previous messages and generates a response based on the previous messages. Let’s see what we can do.\n\nchat = client.chats.create(model=\"gemini-2.5-flash\")\nchat\n\n&lt;google.genai.chats.Chat&gt;\n\n\n\nr = chat.send_message(\"Hi, I like to play in the mud when it is raining outside.\")\nMarkdown(r.text)\n\nThat sounds like so much fun! There’s nothing quite like the squishy, cool feeling of mud, especially when it’s fresh from the rain. It’s a classic joyful activity.\nDo you make mud pies, build things, or just enjoy the wonderfully messy experience?\n\n\n\nr = chat.send_message(\"Wait, I forgot. What were we talking about again?\")\nMarkdown(r.text)\n\nWe were just talking about how much you enjoy playing in the mud when it’s raining outside!\nI had just said it sounded like fun and asked if you make mud pies or build things with it.\n\n\nThe model remembered what we were talking about. We can take a look at its chat history.\n\nfor message in chat.get_history():\n    print(f'role - {message.role}',end=\": \")\n    print(message.parts[0].text)\n\nrole - user: Hi, I like to play in the mud when it is raining outside.\nrole - model: That sounds like so much fun! There's nothing quite like the squishy, cool feeling of mud, especially when it's fresh from the rain. It's a classic joyful activity.\n\nDo you make mud pies, build things, or just enjoy the wonderfully messy experience?\nrole - user: Wait, I forgot. What were we talking about again?\nrole - model: We were just talking about how much you enjoy playing in the mud when it's raining outside!\n\nI had just said it sounded like fun and asked if you make mud pies or build things with it.\n\n\nWe can also create a chat with a system instruction and a config. This would be a perfect way to create a teacher and student experience.\n\nconfig=types.GenerateContentConfig(\n    system_instruction=\"You are a wise teacher who cares depply about students. \"\n    \"You help students seek their curiosity in a fun and exciting way.\"\n    \"Before helping, ask questions about their understanding on the subject. \"\n    \"Keep responses concise and to the point.\"\n    \"Instead of providing answers, let the student explore and find the answer. \"\n    \"Provide answer only when explicitly asked to.\"\n)\nchat = client.chats.create(\n    model=\"gemini-2.5-flash\",\n    config=config,\n)\n\n\ndef ask_chat(prompt):\n    r = chat.send_message(prompt)\n    display(Markdown(r.text))\n\n\nask_chat(\"Help me solve this question: 2 + 2 = \")\n\nThat’s a fun one! Before we dive in, what do you already know about what the ‘+’ sign tells us to do? What does it mean when we “add” numbers together?\n\n\n\nask_chat(\"No idea.\")\n\nNo worries at all! Let’s think about it differently.\nImagine you have two delicious cookies. And then, I give you two more delicious cookies.\nHow many cookies do you have in total now?\n\n\n\nask_chat(\"I am confused on what you are trying to do here..\")\n\nAh, I see! My apologies for the confusion.\nWhat I’m trying to do is help you understand what “adding” means by using something you can picture.\nIn the math problem, 2 + 2, the ‘+’ sign means we’re putting things together.\nSo, when I talked about the cookies, I was showing you what it means to put two cookies together with two more cookies.\nDoes thinking about putting things together make a little more sense for what the ‘+’ sign does?\n\n\n\nask_chat(\"What is the answer to the question?\")\n\nThe answer to 2 + 2 is 4.\n\n\nBy using chat feature and system instructions, we created a simple tutor that can help a user solve a simple math problem. Instead of giving out the answer immediately, the tutor asks the user to think about the problem and provides hints. But student can ask for the answer directly if they want. Wasn’t it fun?",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#multimodal-inputs",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#multimodal-inputs",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "We can also send images to Gemini. Gemini can see the image and generate a response based on the image. Here is an example image. It is a picture of a duck carved from a pear. Let’s see what Gemini can do.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nduck_img_path = '../../img/duckpear.jpg'\nduck_img = Image.open(duck_img_path)\nplt.figure(figsize=(5, 5))\nplt.imshow(duck_img)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(img_path, 'rb') as f:\n    duck_img_bytes = f.read()\n\nask(prompt=[types.Part.from_bytes(data=duck_img_bytes, mime_type='image/jpeg'), 'What do you see in this image?'])\n\nIn this image, I see a beautifully crafted fruit carving, most likely a pear, shaped into a swan or duck.\nHere are the details:\n\nMain Subject: A light-colored fruit (appearing golden-yellow to light brown), intricately carved into the form of a swan or duck.\n\nThe bird’s head and a long, gracefully curved neck are distinct, with a small beak and a dark, perhaps hollowed-out, “eye.”\nThe body is made up of numerous thin, fanned-out slices of the fruit, layered to create a feathered or winged effect.\n\nSurface: The carved fruit rests on a rustic, aged wooden cutting board. The board has visible wood grain and some signs of wear, including a distinct crack or groove towards the bottom right.\nBackground Elements:\n\nTo the left of the carved fruit, part of a knife (with a dark handle and silver blade) is visible on the cutting board.\nFurther back and to the right, another whole, round, brownish fruit or vegetable (possibly another pear or a potato) is out of focus.\nThe very blurred background suggests an indoor setting, possibly a kitchen or dining area, with dark furniture or objects.\n\n\nThe overall impression is one of culinary artistry and delicate presentation.\n\n\nThat was correct. It was probably too easy for it. Let’s do another image. It has five chemical structures that are found in DNA and RNA. Let’s see whether Gemini can recognize them.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nbases_img_path = '../../img/nitrogenous-bases.jpg'\nbases_img = Image.open(bases_img_path)\nplt.figure(figsize=(5, 5))\nplt.imshow(bases_img)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(bases_img_path, 'rb') as f:\n    image_bytes = f.read()\n\nask(prompt=[types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg'), 'What do you see in this image?'])\n\nThis image displays the chemical structures of five nitrogenous bases, which are fundamental components of DNA and RNA. Each structure is clearly labeled with its common name:\n\nAdenine: A purine base, characterized by a fused five-membered and six-membered ring system, with an amino group attached to the six-membered ring.\nGuanine: Also a purine base, featuring the same fused ring system as adenine but with an amino group and a carbonyl group.\nThymine: A pyrimidine base, identified by a single six-membered ring with two carbonyl groups and a methyl group.\nCytosine: A pyrimidine base, composed of a six-membered ring with one carbonyl group and one amino group.\nUracil: A pyrimidine base, similar to thymine but lacking the methyl group, featuring a six-membered ring with two carbonyl groups.\n\nThe image is well-rendered with clear lines and labels, making the structures easy to distinguish.\n\n\nGemini can also do object detection. Let’s try it on the images with chemical structures.\n\nimport json\n\nprompt = \"Detect the all of the chemical structures in the image. \"\n\"Pay close attention to outermost atoms. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n\n# This config sets the response type to be JSON\nconfig = types.GenerateContentConfig(\n    response_mime_type=\"application/json\"\n) \n\nr = send_msg(prompt=[bases_img, prompt], config=config)\n\nwidth, height = bases_img.size\nbounding_boxes = json.loads(r.text)\n\nconverted_bounding_boxes = []\nfor bounding_box in bounding_boxes:\n    abs_y1 = int(bounding_box[\"box_2d\"][0]/1000 * height)\n    abs_x1 = int(bounding_box[\"box_2d\"][1]/1000 * width)\n    abs_y2 = int(bounding_box[\"box_2d\"][2]/1000 * height)\n    abs_x2 = int(bounding_box[\"box_2d\"][3]/1000 * width)\n    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])\n\nprint(\"Image size: \", width, height)\nprint(\"Bounding boxes:\", converted_bounding_boxes)\n\nImage size:  4508 3695\nBounding boxes: [[617, 369, 1920, 1481], [1018, 1533, 1528, 1670], [2398, 365, 3890, 1518], [2862, 1540, 3399, 1670], [338, 1998, 1433, 3037], [775, 3166, 1307, 3299], [1875, 1884, 2763, 3040], [2060, 3159, 2605, 3299], [3060, 2002, 3958, 3026], [3065, 3170, 3737, 3295]]\n\n\nIn this example, we set the response type as json to use structured output. We will go over more on that later. We got five bounding boxes from the response. It looks promising. Let’s plot the bounding boxes on the image.\n\nimport matplotlib.patches as patches\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.imshow(bases_img)\n\nfor i, bbox in enumerate(converted_bounding_boxes):\n    x1, y1, x2, y2 = bbox\n    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none')\n    ax.add_patch(rect)\n    ax.text(x1, y1-5, f'Box {i+1}', color='red', fontsize=10, weight='bold')\n\nax.set_xlim(0, width)\nax.set_ylim(height, 0)\nax.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThat is pretty awesome. With this ability, it would be very convenient to create anki cards to study. For more image realted tasks such as segmentation, uploading files, etc. please refer to the Gemini documentation.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#text-to-speech",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#text-to-speech",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "Let’s do some cool thing where we ask gemini to read text out loud. You can also try out this feature in the AI studio. We’ve been using Gemini 2.5-flash, but we will switch to “gemini-2.5-flash-preview-tts”. Here is Gemini reading a blog post on OMEGA: Can LLMs Reason Outside the Box in Math?.\n\nimport wave\nfrom IPython.display import Audio\n\n# Set up the wave file to save the output:\ndef wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n    with wave.open(filename, \"wb\") as wf:\n        wf.setnchannels(channels)\n        wf.setsampwidth(sample_width)\n        wf.setframerate(rate)\n        wf.writeframes(pcm)\n\nprompt=\"\"\"Say cheerfully: Large language models (LLMs) like GPT-4, Claude, and DeepSeek-R1\nhave made headlines for their impressive performance on mathematical competitions, \nsometimes approaching human expert levels or even exceeding it on Olympiad problems. \nYet a fundamental question remains: Are they truly reasoning or are they just recalling \nfamiliar strategies without inventing new ones?\"\"\",\n\nconfig=types.GenerateContentConfig(\n    response_modalities=[\"AUDIO\"],\n    speech_config=types.SpeechConfig(\n        voice_config=types.VoiceConfig(\n            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name='Kore')\n        )\n    ),\n)\n\nr = send_msg(\n    prompt,\n    model=\"gemini-2.5-flash-preview-tts\",\n    config=config,\n)\n\ndata = r.candidates[0].content.parts[0].inline_data.data\n\nwave_file('tts1.wav', data) # Saves the file to current directory\n\naudio_file = 'tts1.wav'\ndisplay(Audio(audio_file))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe can also create a dialogue with mutliple people in it.\n\nprompt = \"\"\"TTS the following conversation between Teacher and Student:\n         Teacher: \"Alright class, today we're exploring Google's Gemini AI. Now, I know some of you think AI is just magic, but—\"\n\nStudent: \"Magic? Professor, I thought you said there's no such thing as magic in computer science. Are you telling me you've been lying to us this whole time?\"\n\nTeacher: chuckles \"Touché, Sarah. Let me rephrase: Gemini might seem magical, but it's actually quite logical once you understand it. Think of it as a very sophisticated conversation partner.\"\n\nStudent: \"A conversation partner that can also look at my photos, generate images, and call functions? Sounds like the kind of friend I need. Does it also do my homework?\"\n\nTeacher: \"Well, it could help you understand concepts better. But let's start with the basics. Gemini uses something called 'system instructions' to set its behavior. It's like giving someone a role to play.\"\n\nStudent: \"So if I tell it to be a pirate, it'll talk like one?\"\n\nTeacher: \"Exactly! You could say 'You are a helpful pirate tutor' and it would explain calculus while saying 'ahoy matey.' The system instruction shapes its entire personality.\"\n\nStudent: \"That's actually brilliant. So instead of getting boring explanations, I could have a Shakespeare character teach me physics?\"\n\nTeacher: \"Now you're getting it! But here's where it gets interesting - Gemini can also process multiple types of input simultaneously. Text, images, audio, even video.\"\n\nStudent: \"Wait, so I could show it a picture of my messy room and ask it to write a poem about entropy?\"\n\nTeacher: \"Absolutely! That's called multimodal processing. But here's something even cooler - structured output. Instead of just getting text back, you can ask for specific formats like JSON.\"\n\nStudent: \"JSON? You mean I could ask it to rate my terrible cooking and get back a proper data structure instead of just 'this looks questionable'?\"\n\nTeacher: laughs \"Precisely! You could define a Recipe class with ratings, ingredients, and improvement suggestions. Very organized criticism.\"\n\nStudent: \"Okay, but what about function calling? That sounds scary. Like, what if it decides to order pizza while I'm asking about math?\"\n\nTeacher: \"Function calling is actually quite safe. You define exactly which functions it can use, like a toolbox. If you only give it a calculator function, it can't order pizza.\"\n\nStudent: \"But what if I give it a pizza-ordering function?\"\n\nTeacher: \"Then... well, you might get pizza. But that's on you, not the AI.\"\n\nStudent: \"Fair point. What about this 'thinking' feature I heard about?\"\n\nTeacher: \"Ah, that's fascinating! Gemini can show you its reasoning process. It's like seeing someone's rough draft before they give you the final answer.\"\n\nStudent: \"So it's like looking at my brain when I'm solving a problem? That's either really cool or really terrifying.\"\n\nTeacher: \"More like watching a very organized person work through a problem step by step. You can even control how much thinking time it gets.\"\n\nStudent: \"Can I give it infinite thinking time and see if it achieves consciousness?\"\n\nTeacher: \"Let's not get ahead of ourselves. But you can set it to -1 for automatic thinking budget, which is pretty generous.\"\n\nStudent: \"This is all great, but what about the practical stuff? Like, how do I actually use this thing?\"\n\nTeacher: \"Simple! You get an API key, choose a model like gemini-2.0-flash, and start with client.models.generate_content(). The response comes back as text you can use immediately.\"\n\nStudent: \"And if I want to have a longer conversation instead of just one-off questions?\"\n\nTeacher: \"Use the chat interface with client.chats.create(). It remembers context, so you don't have to repeat yourself every time.\"\n\nStudent: \"This sounds too good to be true. What's the catch?\"\n\nTeacher: \"Well, you need to understand how to structure your requests properly. And like any tool, it's only as good as how you use it.\"\n\nStudent: \"So basically, I need to learn how to talk to it properly?\"\n\nTeacher: \"Exactly! Think of it as learning a new language - not programming language, but communication language. The better you get at asking questions, the better answers you'll get.\"\n\nStudent: \"Alright, Professor, I'm convinced. When do we start building things with it?\"\n\nTeacher: \"Right now! Let's start with a simple example and work our way up. Who knows? By the end of class, you might have your pirate physics tutor up and running.\"\n\nStudent: \"Now that's what I call education!\"\"\"\n\nconfig=types.GenerateContentConfig(\n    response_modalities=[\"AUDIO\"],\n    speech_config=types.SpeechConfig(\n        multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(\n            speaker_voice_configs=[\n                types.SpeakerVoiceConfig(\n                    speaker='Teacher',\n                    voice_config=types.VoiceConfig(\n                        prebuilt_voice_config=types.PrebuiltVoiceConfig(\n                        voice_name='Kore',\n                        )\n                    )\n                ),\n                types.SpeakerVoiceConfig(\n                    speaker='Student',\n                    voice_config=types.VoiceConfig(\n                        prebuilt_voice_config=types.PrebuiltVoiceConfig(\n                        voice_name='Puck',\n                        )\n                    )\n                ),\n            ]\n        )\n    )\n)\n\nr = send_msg(\n    prompt,\n    model=\"gemini-2.5-flash-preview-tts\",\n    config=config\n)\n\ndata = r.candidates[0].content.parts[0].inline_data.data\n\nwave_file('tts2.wav', data)\ndisplay(Audio('tts2.wav'))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThat was a pretty fun conversation. This is probably how Notebook LM is created. We can change the voice and other configurations. It might be fun to let Gemini go through databases in the company, create a summary, and talk about it. Very great for traveling. More infornation in the documentation.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#thinking",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#thinking",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "By default, Gemini will think if it thinks it needs to, or not. By setting include_thoughts=True, we can take a look at the thinking part.\n\nr = send_msg(\"what is 2+2?\", config=types.GenerateContentConfig(\n        thinking_config=types.ThinkingConfig(include_thoughts=True)\n    ))\nr.text\n\n'2 + 2 = 4'\n\n\n\ndef display_thinking(r):\n    for part in r.candidates[0].content.parts:\n        if part.thought: print(\"Thought summary:\"); display(Markdown(part.text))\n\n\ndisplay_thinking(r)\n\nThought summary:\n\n\nSimple Addition, Seeking Confirmation\nOkay, so I see the user is asking a straightforward arithmetic question: what’s the sum of 2 and 2? Frankly, it’s almost too basic for my expertise, but it’s important to provide a correct response. I understand they’re looking for the total, the result of adding these two numbers together. It’s a foundational concept, and I’ll confirm that the answer is indeed what I know it to be.\n\n\nWe can also disable thinking by providing 0 thinking budget. Even if answer is complicated, it would just create a response without thinking. Response time is shorter, so if it is a simple task, this is recommended. By default thinking_budget=-1, which means dynamic thinking and thinks automatically. Or we can set a budget, such as 1024, which is a number of tokens.\n\nr = send_msg(\n    \"What is the most important quality for people to express?\",\n    config=types.GenerateContentConfig(\n        thinking_config=types.ThinkingConfig(thinking_budget=0)\n    ),\n)\nMarkdown(r.text)\n\nThat’s a fascinating question, and one that people have debated for centuries! There’s no single “most important” quality that everyone agrees on, as its significance often depends on individual values, cultural contexts, and the specific situation.\nHowever, if I had to choose one quality that underpins many other positive traits and facilitates a harmonious existence, it would be empathy.\nHere’s why empathy could be considered the most important:\n\nFoundation for Connection: Empathy allows us to understand and share the feelings of others. This understanding is the bedrock of all meaningful relationships, whether personal or professional.\nDrives Compassion and Kindness: When we truly understand what someone else is going through, it naturally leads to a desire to help and alleviate their suffering. Compassion and kindness often flow from empathy.\nPromotes Effective Communication: Empathy helps us tailor our communication to the other person’s perspective, making us better listeners and more impactful communicators.\nFosters Conflict Resolution: By understanding the different viewpoints involved in a conflict, empathy can bridge divides and lead to more peaceful and constructive solutions.\nEncourages Ethical Behavior: Empathy makes us consider the impact of our actions on others, which is crucial for ethical decision-making and preventing harm.\nBuilds Inclusivity: Empathy allows us to appreciate and respect differences, leading to more inclusive environments where everyone feels valued.\nEssential for Leadership: Effective leaders need to understand the needs, motivations, and challenges of their team members to inspire and guide them effectively.\n\nWhile other qualities like integrity, courage, resilience, or intelligence are undeniably valuable, empathy often serves as the catalyst that allows these qualities to be applied in a way that benefits others and contributes to a better world. Without empathy, even the most intelligent or courageous person might act in ways that are ultimately detrimental to human connection and well-being.\nUltimately, the “most important” quality is a deeply personal choice, but empathy makes a strong case for its foundational role in positive human interaction.\n\n\n\ndisplay_thinking(r)\n\nFor more informaiton about thinking with Gemini, take a look at thinking documentation.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#structured-output",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#structured-output",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "By using structured output, it is possible to generate a response in a specific format. This can be useful for tasks such as data extraction, where the response needs to be in a specific format. The response can be in the form of a table, a list, or a dictionary. We will create a JSON response. Let’s say we have some alient encounters we need to sort through. Using pydantic, we create a schema. It has species_type, communication_method, and so on.\n\nfrom pydantic import BaseModel\nfrom typing import List, Literal, Optional\n\nclass AlienContact(BaseModel):\n    species_type: str\n    communication_method: Literal[\"telepathy\", \"math\", \"interpretive_dance\", \"smell\", \"musical_tones\", \"color_patterns\"]\n    hostility_level: int\n    technology_advancement: int\n    cultural_exchange_items: List[str]\n    probe_attempts: int\n    universal_translator_needed: bool\n    galactic_citizenship_offered: bool\n    earth_invasion_likelihood: int\n    special_abilities: List[str]\n    home_planet_description: str\n\nconfig=dict(response_mime_type=\"application/json\", response_schema=list[AlienContact])\n\nr = send_msg(\n    \"\"\"Generate 3 different alien encounter scenarios. Make them creative and fun - \n    one friendly, one neutral, one slightly concerning but not too scary.\"\"\",\n    config=config\n)\n\nencounters: list[AlienContact] = r.parsed\nfor i, encounter in enumerate(encounters, 1):\n    print(f\"=== Encounter {i}: {encounter.species_type} ===\")\n    print(f\"Communication: {encounter.communication_method}\")\n    print(f\"Invasion likelihood: {encounter.earth_invasion_likelihood}%\")\n    print(f\"Special abilities: {', '.join(encounter.special_abilities)}\")\n    print()\n\n=== Encounter 1: Lumiflora ===\nCommunication: musical_tones\nInvasion likelihood: 0%\nSpecial abilities: photosynthetic empathy, bio-luminescent communication, terraforming through sonic vibrations\n\n=== Encounter 2: Chronomandibles ===\nCommunication: math\nInvasion likelihood: 2%\nSpecial abilities: localized time manipulation, data absorption via touch, probability calculation\n\n=== Encounter 3: Whisperkin ===\nCommunication: telepathy\nInvasion likelihood: 6%\nSpecial abilities: mimicry of thoughts and emotions, induce temporary hallucinations, phase through solid objects\n\n\n\n\nimport json\nprint(json.dumps(encounters[0].model_dump(), indent=2))\n\n{\n  \"species_type\": \"Lumiflora\",\n  \"communication_method\": \"musical_tones\",\n  \"hostility_level\": 1,\n  \"technology_advancement\": 9,\n  \"cultural_exchange_items\": [\n    \"harmonious frequencies\",\n    \"glowing seeds\",\n    \"recipes for light-based sustenance\"\n  ],\n  \"probe_attempts\": 0,\n  \"universal_translator_needed\": false,\n  \"galactic_citizenship_offered\": true,\n  \"earth_invasion_likelihood\": 0,\n  \"special_abilities\": [\n    \"photosynthetic empathy\",\n    \"bio-luminescent communication\",\n    \"terraforming through sonic vibrations\"\n  ],\n  \"home_planet_description\": \"A verdant world bathed in multi-spectrum light, where giant bioluminescent flora form living cities and consciousness is shared through resonant frequencies.\"\n}\n\n\nAs we can see, we got responses as json format. It is very useful feature. We can use it to create a database of alien encounters. We can also add those encounters into google sheets using function calls. We will create functions to add new rows to a google sheet with these data.\nThere are so much more we can do with structured output. It is also possible to extract information from resume, food recipes, and so on. You can find more in documentation.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#function-calling",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#function-calling",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "Function calling is very fun. Before we add our alient encounters into the google sheets, let’s see how it works. We provide Gemini with what tools it can use. Then we can ask it to use one of those tools. Let’s create a function that will add one to a number.\n\nnum = 0\n\ndef show_curr_num() -&gt; int:\n    \"\"\"Show the current state of num\"\"\"\n    return num\n\ndef add_one() -&gt; str:\n    \"\"\"Add one to global variable num.\"\"\"\n    global num\n    num += 1\n    return f\"num has been changed into {num}\"\n\n\nconfig = types.GenerateContentConfig(\n    tools=[show_curr_num, add_one]\n)\nr = send_msg(\n    \"Use your tool loop to turn num into 5\",\n    config=config\n)\n\nr.text\n\n'num has been turned into 5.'\n\n\n\nnum\n\n5\n\n\nAs we can see, the number has been increased to 5 as we asked it to. Under the hood, Gemini decides whether to use a tool to respond to the prompt or not. If it can respond without tool, it answers, but if it chooses to, it calls the tool. Then it asks itself again whether it still needs to use additional tools or not. It will continue to use the tool until it is done. We can also look at the history of tool usage. This can give us an idea of what tools were used and when.\n\ndef get_tool_history(r):\n    history = r.automatic_function_calling_history\n    for i,entry in enumerate(history):\n        if hasattr(entry, 'parts') and entry.parts:\n            for part in entry.parts:\n                if part.function_call: print(f\"Tool call {i}: {part.function_call.name}({dict(part.function_call.args)})\")\n                if part.function_response: print(f\"Tool response {i}: {part.function_response.response['result']}\")\n\n\nget_tool_history(r)\n\nTool call 1: show_curr_num({})\nTool response 2: 0\nTool call 3: add_one({})\nTool response 4: num has been changed into 1\nTool call 5: add_one({})\nTool response 6: num has been changed into 2\nTool call 7: add_one({})\nTool response 8: num has been changed into 3\nTool call 9: add_one({})\nTool response 10: num has been changed into 4\nTool call 11: add_one({})\nTool response 12: num has been changed into 5\n\n\nNow that we know how function call works, let’s use it to add our alien encounters into google sheets. We will use the function call to add a new row to the google sheet. First we need to get the authentication. To get the service-account-key.json, follow these steps:\n\nGo to Google Cloud Console → IAM & Admin → Service Accounts\nCreate a new service account or use existing one\nClick on the service account → Keys → Add Key → Create New Key → JSON\nDownload and save as static/service-account-key.json\nEnable Google Sheets API in your project\nShare your Google Sheet with the service account email address (found in the JSON file)\n\n\nfrom google.oauth2.service_account import Credentials as ServiceCredentials\nfrom googleapiclient.discovery import build\n\nSCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n\ndef authenticate_sheets_service_account():\n    creds = ServiceCredentials.from_service_account_file('/home/kappa/data/service-account-key.json', scopes=SCOPES)\n    return build('sheets', 'v4', credentials=creds)\n\nsheets_service = authenticate_sheets_service_account()\n\n\ndef read_sheet_data(spreadsheet_id: str, range_name: str) -&gt; dict:\n    \"Read data from a Google Sheet range\"\n    try:\n        result = sheets_service.spreadsheets().values().get(\n            spreadsheetId=spreadsheet_id, range=range_name).execute()\n        return dict(success=True, data=result.get('values', []), range=range_name)\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef write_sheet_data(spreadsheet_id: str, range_name: str, values: list[list[str]]) -&gt; dict:\n    \"Write data to a Google Sheet range\"\n    try:\n        body = dict(values=values)\n        result = sheets_service.spreadsheets().values().update(\n            spreadsheetId=spreadsheet_id, range=range_name, valueInputOption='RAW', body=body).execute()\n        return dict(success=True, updated_cells=result.get('updatedCells'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef append_sheet_data(spreadsheet_id: str, range_name: str, values: list[list[str]]) -&gt; dict:\n    \"Append data to a Google Sheet\"\n    try:\n        body = dict(values=values)\n        result = sheets_service.spreadsheets().values().append(\n            spreadsheetId=spreadsheet_id, range=range_name, valueInputOption='RAW', body=body).execute()\n        return dict(success=True, updated_range=result.get('updates', {}).get('updatedRange'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef get_sheet_info(spreadsheet_id: str) -&gt; dict:\n    \"Get information about all sheets in a spreadsheet including their IDs\"\n    try:\n        result = sheets_service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n        sheets_info = [dict(title=sheet['properties']['title'], sheet_id=sheet['properties']['sheetId']) for sheet in result.get('sheets', [])]\n        return dict(success=True, sheets=sheets_info)\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef delete_rows(spreadsheet_id: str, sheet_id: int, start_index: int, end_index: int) -&gt; dict:\n    \"Delete rows from a sheet (0-indexed)\"\n    try:\n        request = dict(deleteDimension=dict(range=dict(sheetId=sheet_id, dimension='ROWS', startIndex=start_index, endIndex=end_index)))\n        result = sheets_service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=dict(requests=[request])).execute()\n        return dict(success=True, replies=result.get('replies'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef delete_columns(spreadsheet_id: str, sheet_id: int, start_index: int, end_index: int) -&gt; dict:\n    \"Delete columns from a sheet (0-indexed)\"\n    try:\n        request = dict(deleteDimension=dict(range=dict(sheetId=sheet_id, dimension='COLUMNS', startIndex=start_index, endIndex=end_index)))\n        result = sheets_service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=dict(requests=[request])).execute()\n        return dict(success=True, replies=result.get('replies'))\n    except Exception as e: return dict(success=False, error=str(e))\n\ndef extract_spreadsheet_id(url: str) -&gt; str:\n    \"Extract spreadsheet ID from Google Sheets URL\"\n    return url.split('/d/')[1].split('/')[0] if '/d/' in url else url\n\n\nspreadhseet_url = 'https://docs.google.com/spreadsheets/d/1yWZjT7Lu5CGRXY0et_TYDqoiylt9-MUjk2y0A_WtBLA/edit?gid=0#gid=0'\nspreadhseet_url\n\n'https://docs.google.com/spreadsheets/d/1yWZjT7Lu5CGRXY0et_TYDqoiylt9-MUjk2y0A_WtBLA/edit?gid=0#gid=0'\n\n\nThe google sheet needs to be shared with editor permission with anyone.\n\nget_sheet_info(extract_spreadsheet_id(spreadhseet_url))\n\n{'success': True, 'sheets': [{'title': 'Sheet1', 'sheet_id': 0}]}\n\n\n\nencounters\n\n[AlienContact(species_type='Lumiflora', communication_method='musical_tones', hostility_level=1, technology_advancement=9, cultural_exchange_items=['harmonious frequencies', 'glowing seeds', 'recipes for light-based sustenance'], probe_attempts=0, universal_translator_needed=False, galactic_citizenship_offered=True, earth_invasion_likelihood=0, special_abilities=['photosynthetic empathy', 'bio-luminescent communication', 'terraforming through sonic vibrations'], home_planet_description='A verdant world bathed in multi-spectrum light, where giant bioluminescent flora form living cities and consciousness is shared through resonant frequencies.'),\n AlienContact(species_type='Chronomandibles', communication_method='math', hostility_level=3, technology_advancement=8, cultural_exchange_items=['advanced algorithms', 'temporal anomaly data', 'schematics for spacetime navigation'], probe_attempts=1, universal_translator_needed=True, galactic_citizenship_offered=False, earth_invasion_likelihood=2, special_abilities=['localized time manipulation', 'data absorption via touch', 'probability calculation'], home_planet_description='A barren, fractured world orbiting a black hole, where existence is measured in temporal shifts and data is the primary resource.'),\n AlienContact(species_type='Whisperkin', communication_method='telepathy', hostility_level=5, technology_advancement=7, cultural_exchange_items=['echoing artifacts', 'dreams of unknown origins', 'illusions of past events'], probe_attempts=5, universal_translator_needed=False, galactic_citizenship_offered=False, earth_invasion_likelihood=6, special_abilities=['mimicry of thoughts and emotions', 'induce temporary hallucinations', 'phase through solid objects'], home_planet_description='A perpetually twilight world shrouded in mists, where reality is fluid and beings communicate through shared consciousness and subtle manipulation of perception.')]\n\n\nUnfortunately, we cannot run the function calling with structured data in the same gemini call. We have to call it in two steps. First, we need to get the data using the schema then we can add that into the gemini call.\n\ndef add_encounters_to_sheets(spreadsheet_url: str, encounters_data: List[AlienContact]) -&gt; dict:\n    \"Add the alient encounters data to a spreadsheet.\"\n    try:       \n        spreadsheet_id = extract_spreadsheet_id(spreadsheet_url)\n        headers = [[\"Species\", \"Communication\", \"Hostility\", \"Tech Level\", \n                    \"Exchange Items\", \"Probes\", \"Translator Needed\", \"Citizenship\", \n                    \"Invasion Risk\", \"Abilities\", \"Home Planet\"]]\n        \n        rows = []\n        for e in encounters_data:\n            row = [\n                e.species_type,\n                e.communication_method,\n                str(e.hostility_level),\n                str(e.technology_advancement),\n                ', '.join(e.cultural_exchange_items),\n                str(e.probe_attempts),\n                str(e.universal_translator_needed),\n                str(e.galactic_citizenship_offered),\n                str(e.earth_invasion_likelihood),\n                ', '.join(e.special_abilities),\n                e.home_planet_description\n            ]\n            rows.append(row)\n        \n        write_result = write_sheet_data(spreadsheet_id, 'A1:K1', headers)\n        if not write_result['success']:\n            return write_result\n            \n        append_result = append_sheet_data(spreadsheet_id, 'A2:K', rows)\n        return dict(success=True, encounters_added=len(encounters_data), sheet_result=append_result)\n        \n    except Exception as e:\n        return dict(success=False, error=str(e))\n\n\ndef show_alien_encounters() -&gt; List[AlienContact]:\n    \"Show a list of alien encounters in `AlienContact` format.\"\n    return encounters\n\n\nconfig = types.GenerateContentConfig(\n    tools=[add_encounters_to_sheets, extract_spreadsheet_id, \n           write_sheet_data, append_sheet_data, show_alien_encounters])\nr = send_msg(f\"Use `show_alien_encounters` to grab encounter data and add the alien encounters data from a list of encounters to {spreadhseet_url}\", \n             config=config)\nprint(r.text)\n\nThe alien encounters data has been successfully added to the spreadsheet.\n\n\nThat was awesome. We added the data into Google Sheets. I have added functions to delete rows and so on, so play around with it for fun.\nLet’s do another example where we go through urls in websites to find information we need. Let’s say we want to find information about Marie Curie, but don’t want to go through whole deep research as it takes a long time and finds so many articles. We want to only look at couple wikipedia articles and call it a day. Let’s find some information about Marie Curie.\n\nfrom httpx import get as xget\n\ndef read_url(url:str):\n    \"read a markdown version of url using jina.ai\"\n    out = xget(url, follow_redirects=True).text\n#     if len(out) &gt; 10000: return f\"{out[:10000]}\\n**TRUNCATED**\"\n    return out\n\ndef read_jina(url:str):\n    \"read a markdown version of url using jina.ai\"\n    return xget(f'https://r.jina.ai/{url}', follow_redirects=True).text\n\nUsing jina gives us markdown version of the website, which is more readable. However, the service is down due to DDOS attack right now. By using read_url we can grab html of the website.\n\nwiki_article = read_url('https://en.wikipedia.org/wiki/Marie_Curie')\nprint(wiki_article[:1000])\n\n&lt;!DOCTYPE html&gt;\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;title&gt;Marie Curie - Wikipedia&lt;/title&gt;\n&lt;script&gt;(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature\n\n\n\nconfig = types.GenerateContentConfig(\n    tools=[read_url])\nr = send_msg(f\"Can you find out what Marie Curie accomplished and how that impacted other scientists from wikipedia?\"\n             \" Use `read_url` function call to find out more about at least 3 different scientists from urls\"\n             \"listed in the article.\"\n             \"Look through their biographies carefully.\", \n             config=config)\nMarkdown(r.text)\n\nMarie Curie’s accomplishments were monumental:\n\nDiscovery of New Elements: She, along with her husband Pierre Curie, discovered two new radioactive elements: polonium (named after her native Poland) and radium.\nPioneering Research in Radioactivity: She conducted extensive research into radioactivity, a term she coined. Her systematic studies using an electrometer (developed by her husband and his brother) showed that the activity of uranium compounds depended solely on the quantity of uranium present, leading her to hypothesize that radiation came from the atom itself, a crucial step in disproving the idea of indivisible atoms.\nIsolation of Radium: She successfully isolated pure radium metal in 1910, a very challenging process involving processing tonnes of pitchblende.\nMedical Applications of Radioactivity: Under her direction, the world’s first studies were conducted into the treatment of neoplasms (tumors) using radioactive isotopes. During World War I, she developed and deployed mobile radiography units (“Little Curies”) to provide X-ray services at field hospitals, revolutionizing battlefield medicine.\nFirst Woman Nobel Laureate and Only Person in Two Scientific Fields: She was the first woman to win a Nobel Prize (Physics, 1903), the first person to win a Nobel Prize twice, and the only person to win Nobel Prizes in two different scientific fields (Physics in 1903 and Chemistry in 1911).\nFounding of Research Institutes: She founded the Curie Institute in Paris (1920) and the Curie Institute in Warsaw (1932), which became major medical research centers and nurtured future Nobel laureates.\n\nMarie Curie’s work had a profound impact on other scientists:\n\nPierre Curie: Her husband was her closest collaborator. Their mutual passion for science led them to work together extensively. Pierre joined Marie in her research into uranium rays, and they jointly discovered polonium and radium. He was a co-recipient of the 1903 Nobel Prize in Physics with Marie and Henri Becquerel, recognizing their joint researches on radiation phenomena. His support and collaboration were crucial to her early discoveries.\nHenri Becquerel: Becquerel discovered that uranium salts emitted rays. Marie Curie decided to investigate these “uranium rays” for her thesis, which led her to discover that the activity of uranium compounds depended only on the quantity of uranium, not external energy, suggesting the radiation came from the atom itself. Becquerel, along with Pierre and Marie Curie, was awarded the 1903 Nobel Prize in Physics for his discovery of spontaneous radioactivity, a field that Marie Curie significantly advanced and named.\nErnest Rutherford: Curie’s discovery of radium was instrumental to Rutherford’s work. As stated in his biography, “Radium’s radioactivity was so great that it could not be ignored. It seemed to contradict the principle of the conservation of energy and therefore forced a reconsideration of the foundations of physics. On the experimental level, the discovery of radium provided men like Ernest Rutherford with sources of radioactivity with which they could probe the structure of the atom.” Rutherford utilized these radioactive sources to conduct his famous gold foil experiment, which led to his postulation of the nuclear atom model. He later went on to identify the proton and theorize the neutron, fundamental contributions to nuclear physics.\n\nMarie Curie’s persistent dedication and groundbreaking discoveries not only reshaped the understanding of physics and chemistry but also provided essential tools and inspiration for subsequent generations of scientists, including her own daughter, Irène Joliot-Curie, who would also become a Nobel laureate in Chemistry.\n\n\n\ndef get_tool_calls(r):\n    history = r.automatic_function_calling_history\n    for i,entry in enumerate(history):\n        if hasattr(entry, 'parts') and entry.parts:\n            for part in entry.parts:\n                if part.function_call: print(f\"Tool call {i}: {part.function_call.name}({dict(part.function_call.args)})\")\n\nInstead of using get_tool_history, which includes tool calls and responses, we only look at the tool calls. Responses are too long.\n\nget_tool_calls(r)\n\nTool call 1: read_url({'url': 'https://en.wikipedia.org/wiki/Marie_Curie'})\nTool call 3: read_url({'url': 'https://en.wikipedia.org/wiki/Ernest_Rutherford'})\n\n\nAs we can see, Gemini called multiple function calls using read_url. We can also use read_url on llms.txt as they have urls in there as well. As the model goes through llms.txt and finds links that could help with answering the prompt, it uses subsequent function calls. This way, llms.txt can be small and does not have to take up a lot of context window. Even with Gemini, which has 1M context window, this method allows adding more llms.txt for different applications.\nThere are so many things we can do with function calling, such as using APIs. We can also get weather using API on couple cities to visit for the weekend and create a fun plan for a trip.\nFor more information about function calling, check the Gemini documentation on function calls.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#google-search",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#google-search",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "Large language models are trained with a lot of data, and it is very expensive. Therefore, the models have knowledge cutoff, which means after that time, the model does not have knowledge of what happened after. For instance, Gemini 2.5 Flash has Knowledge cutoff of January 2025. Let’s ask about an event happened after the knowledge cutoff date.\nFor more information about the Gemini 2.5 Flash, take a look at the model card.\n\nask('Who is the new pope in July 2025?')\n\nThere is no new pope scheduled for July 2025.\nAs of today, Pope Francis is still the reigning pontiff. Popes serve for life unless they resign or pass away, and there have been no announcements or indications of either event occurring before that date.\n\n\nGemini does not know about the new Pope in 2025. Let’s provide it with a tool that to search the web for answers.\n\ngrounding_tool = types.Tool(google_search=types.GoogleSearch())\nconfig = types.GenerateContentConfig(tools=[grounding_tool])\n\nr = send_msg('Who is the new pope in July 2025?', config=config)\nMarkdown(r.text)\n\nAs of July 2025, the new Pope is Leo XIV. He was elected on May 8, 2025, succeeding Pope Francis, who passed away on April 21, 2025, at the age of 88.\nPope Francis served as the head of the Catholic Church from March 13, 2013, until his death. He was the first Jesuit pope, the first from Latin America, and the first born or raised outside Europe since the 8th century.\nPope Leo XIV has released his prayer intention for July 2025, focusing on “formation in discernment.” His upcoming schedule for August and September 2025 includes celebrating Holy Mass for the Jubilee of Youth and the canonization of Blessed Pier Giorgio Frassati and Blessed Carlo Acutis.\n\n\nThat’s better. The response also includes citations. Here is add_citations from the documentations.\n\ndef add_citations(response):\n    text = response.text\n    supports = response.candidates[0].grounding_metadata.grounding_supports\n    chunks = response.candidates[0].grounding_metadata.grounding_chunks\n    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)\n\n    for support in sorted_supports:\n        end_index = support.segment.end_index\n        if support.grounding_chunk_indices:\n            citation_links = []\n            for i in support.grounding_chunk_indices:\n                if i &lt; len(chunks):\n                    uri = chunks[i].web.uri\n                    citation_links.append(f\"[{i + 1}]({uri})\")\n            citation_string = \", \".join(citation_links)\n            text = text[:end_index] + citation_string + text[end_index:]\n\n    return text\n\n\nMarkdown(add_citations(r))\n\nAs of July 2025, the new Pope is Leo XIV. He was elected on May 8, 2025, succeeding Pope Francis, who passed away on April 21, 2025, at the age of 88.1\nPope Francis served as the head of the Catholic Church from March 13, 2013, until his death.1 He was the first Jesuit pope, the first from Latin America, and the first born or raised outside Europe since the 8th century.1\nPope Leo XIV has released his prayer intention for July 2025, focusing on “formation in discernment.”2, 3, 4, 5 His upcoming schedule for August and September 2025 includes celebrating Holy Mass for the Jubilee of Youth and the canonization of Blessed Pier Giorgio Frassati and Blessed Carlo Acutis.6\n\n\nThat is very cool. Unfortunately, we cannot look at the tool history as of right now in July 2025. For more information about the search feature, check out the documentation.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-05-gemini_tutorial/index.html#conclusion",
    "href": "blog/posts/2025-07-05-gemini_tutorial/index.html#conclusion",
    "title": "Beginner’s guide to gemini",
    "section": "",
    "text": "In this blog, we looked at how to use gemini-2.5-flash and gemini-2.5-flash-preview-tts for text to speech.\nWe covered the following topics:\n\nUsing system instructions\nChat\nImage inputs\nText to speech\nThinking\nStructured outputs\nFunction calling\nGoogle search\n\nIt was very fun to play around with different features.\nThere are more information in the gemini documentation and cookbook.",
    "crumbs": [
      "Home",
      "Blog",
      "Beginner's guide to gemini"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-30-live_coding6/index.html",
    "href": "blog/posts/2022-11-30-live_coding6/index.html",
    "title": "Live coding 6",
    "section": "",
    "text": "In this blog, I will cover how to setup mamba and bash_history in Paperspace. We also will go over a bit of fastbook and some terminal commands.\nThis blog is based on Live coding 6 Youtube video. There’s been some changes to Paperspace since the video is recorded. For instance, mamba is available now, so it’s easier to setup. Let’s dive in.\n\n\nSo, we want to intall universal-ctags, but it’s not available on pip because it’s not really a python thing. We can install it with a package manager, but it won’t be available after we restart Paperspace instance. Are we out of luck? No! Thankfully, we can use mamba! We can use -p to specify where to install universal-ctags, and we can make it persistent.\nThese are the steps.\n\nOpen up terminal in Paperspace.\ncd: Change directory to home directory (/root/)\nmkdir conda: We create conda directory here so that we can install packages in this directory.\nmamba install -p ~/conda universal-ctags: We are using mamba with -p ~/conda. With -p, we specify where to install the package and ~/conda is same as /root/conda in this case. ~ is short for home directory, which is /root/.\nFollow the prompt, and we have ctags installed.\n\n\n\n\nMamba install\n\n\nNext thing is to add this to our PATH environment variable. We can check our environment variable with echo command. With echo $PATH, we can take a look. Then, type export PATH=~/conda/bin:$PATH. This updates our shell’s PATH environment variable to include ~/conda/bin. If you are curious, take a look at what’s inside of conda directory.\nSo, let’s try if ctags is installed correctly. Try ctags --version.\nThis is what it should look like:\n\n\n\nCorrect ctags version\n\n\nIf your looks something like this, it’s not installed correctly. This is from Emacs:\n\n\n\nWrong ctags version\n\n\nIf you forgot how to use ctags, check out Live coding 5.\nCool. This is working for now, but ctags will not be available if we restart our instance. To make this available persistently, we have to do some work. Let’s take care of our environment variable first. Rather than typing export PATH=~/conda/bin:$PATH every single time we start our instance, we can create .bash.local file inside of /storage/. This file will be run when the instance starts. This is what mine look like:\n\n\n\nMy /storage/.bash.local file\n\n\nFirst line is for conda and second line is for pip. Last line creates an alias so that I can type piu instead of pip install --user.\n\n\n\n\n\n\nWarning\n\n\n\nSpace is not allowed between = and words. This will not work. You have to type similar to mine.\n\n\nLet’s move on to the next steps:\n\nmv conda/ /storage/cfg/: Move the conda directory into our permanent storage location.\nln -s /storage/cfg/conda: Create a symlink from the moved directory to here.\nctags --version: Test it again.\nNow, add ln -s /storage/cfg/conda to /storage/pre-run.sh in order to make conda packages available even after restarting the instance.\n\nNow we are set with conda packages. But how big is the conda directory? How much disk space does it use? Because we have limited space, it’s good to check this with du -sh command. This command roughly means disk usage (du) with summary (s) and human readable (h) options. Change directory to conda directory and try it. We can also do du -sh * to see list each directory with sizes.\n\n\n\ndu -sh *\n\n\n\n\n\nWhenever we type command on terminal, it is saved. We can press up arrow and down arrow to refer back to our command history. We can type history to look at them. Basically, all the history is stored at ~/.bash_history file. Take a look.\nTo make our Paperspace instance feel more user friendly, we can save our bash command history so that we can refer back to our history before we closed the instance.\n\ncd: Move to home directory.\nmv .bash_history /storage/cfg: Move the .bash_history file into /storage/cfg/ directory.\nln -s /storage/cfg/.bash_history: Create a symlink.\n\nNow we just have to update our pre-run.sh file. Here’s what mine looks like so far. Same thing here like we did before with .local/. We delete .bash_history if system generated one at this point. Then make the symlink.\n\n\n\npre-run.sh\n\n\nNow, if you want the history to work, make sure to close the terminal with Ctrl-d before shutting down the instance. I noticed that history does not get saved if I just shut down the instance.\n\n\n\nNext thing we will cover is setting up fastbook. We will add this into our /notebooks/ so that we can have this permanently. Easy way to do it is simply go /notebooks/ directory and type git clone https://github.com/fastai/fastbook.git. However, we can fork this repo first. On the fastbook github page, click fork to get a copy of the repo. It is located on the top right side of the page.\n\n\n\nforking a repo\n\n\nThen use git clone to clone it inside of Paperspace. In order to use git to actually commit to GitHub, we have to setup .gitconfig file. Type git config --global user.name \"yourname\" and git config --global user.email \"youremail\" to setup them. Now, the same thing. To make this persistent, we have to move this file to /storage/cfg and create a symlink. Here are the steps:\n\ngit config --global user.name \"yourname\"\ngit config --global user.email \"youremail\"\nmv ~/.gitconfig /storage/cfg/\nln -s /storage/cfg/.gitconfig\n\nNow, our gitconfig is complete.\nLet’s go over some fastbook stuff now. We will see this code block when we look at the book:\n\nfrom fastai.vision.all import *\n\nThis means we are importing everything (*) from fastai/vision/all.py module. See how . changes to /? We can take a look at this file if we want to. Now, we know where to look when we encounter this stuff in our book to dig deeper.\n\n\n\n\n\n\nTipvim command tips\n\n\n\n\n\nIn vim, we can type * on a word to move the cursor to next occurance of it. Typing Ctrl-p while typing a word can fill the rest of the word like auto completion. The word has to be used before.\n\n\n\nAnother thing we find a lot is path. It is made with Pathlib from python standard library.\n\npath = untar_data(URLs.MNIST_SAMPLE)\npath\n\nPath('/home/kappa/.fastai/data/mnist_sample')\n\n\nIt is upgraded with fastai, so we can use ls method to look at files and directories within this path.\n\npath.ls()\n\n(#3) [Path('/home/kappa/.fastai/data/mnist_sample/labels.csv'),Path('/home/kappa/.fastai/data/mnist_sample/train'),Path('/home/kappa/.fastai/data/mnist_sample/valid')]\n\n\nAnother useful feature from path is Path.BASE_PATH. This can simplify our path by setting the base path.\n\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#3) [Path('labels.csv'),Path('train'),Path('valid')]\n\n\nSee how simple and readable it is now?\n\n\n\nNow our environment is setup so that we can read fastbook and commit changes to our forked repo. Committing to a forked repo is optional because paperspace will keep the notebooks. However, I like backing up my work at GitHub just in case I want to work on my local computer.\nIf you want to dig deeper, you can watch the video this blog is based on.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 6"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-30-live_coding6/index.html#mamba",
    "href": "blog/posts/2022-11-30-live_coding6/index.html#mamba",
    "title": "Live coding 6",
    "section": "",
    "text": "So, we want to intall universal-ctags, but it’s not available on pip because it’s not really a python thing. We can install it with a package manager, but it won’t be available after we restart Paperspace instance. Are we out of luck? No! Thankfully, we can use mamba! We can use -p to specify where to install universal-ctags, and we can make it persistent.\nThese are the steps.\n\nOpen up terminal in Paperspace.\ncd: Change directory to home directory (/root/)\nmkdir conda: We create conda directory here so that we can install packages in this directory.\nmamba install -p ~/conda universal-ctags: We are using mamba with -p ~/conda. With -p, we specify where to install the package and ~/conda is same as /root/conda in this case. ~ is short for home directory, which is /root/.\nFollow the prompt, and we have ctags installed.\n\n\n\n\nMamba install\n\n\nNext thing is to add this to our PATH environment variable. We can check our environment variable with echo command. With echo $PATH, we can take a look. Then, type export PATH=~/conda/bin:$PATH. This updates our shell’s PATH environment variable to include ~/conda/bin. If you are curious, take a look at what’s inside of conda directory.\nSo, let’s try if ctags is installed correctly. Try ctags --version.\nThis is what it should look like:\n\n\n\nCorrect ctags version\n\n\nIf your looks something like this, it’s not installed correctly. This is from Emacs:\n\n\n\nWrong ctags version\n\n\nIf you forgot how to use ctags, check out Live coding 5.\nCool. This is working for now, but ctags will not be available if we restart our instance. To make this available persistently, we have to do some work. Let’s take care of our environment variable first. Rather than typing export PATH=~/conda/bin:$PATH every single time we start our instance, we can create .bash.local file inside of /storage/. This file will be run when the instance starts. This is what mine look like:\n\n\n\nMy /storage/.bash.local file\n\n\nFirst line is for conda and second line is for pip. Last line creates an alias so that I can type piu instead of pip install --user.\n\n\n\n\n\n\nWarning\n\n\n\nSpace is not allowed between = and words. This will not work. You have to type similar to mine.\n\n\nLet’s move on to the next steps:\n\nmv conda/ /storage/cfg/: Move the conda directory into our permanent storage location.\nln -s /storage/cfg/conda: Create a symlink from the moved directory to here.\nctags --version: Test it again.\nNow, add ln -s /storage/cfg/conda to /storage/pre-run.sh in order to make conda packages available even after restarting the instance.\n\nNow we are set with conda packages. But how big is the conda directory? How much disk space does it use? Because we have limited space, it’s good to check this with du -sh command. This command roughly means disk usage (du) with summary (s) and human readable (h) options. Change directory to conda directory and try it. We can also do du -sh * to see list each directory with sizes.\n\n\n\ndu -sh *",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 6"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-30-live_coding6/index.html#bash-history",
    "href": "blog/posts/2022-11-30-live_coding6/index.html#bash-history",
    "title": "Live coding 6",
    "section": "",
    "text": "Whenever we type command on terminal, it is saved. We can press up arrow and down arrow to refer back to our command history. We can type history to look at them. Basically, all the history is stored at ~/.bash_history file. Take a look.\nTo make our Paperspace instance feel more user friendly, we can save our bash command history so that we can refer back to our history before we closed the instance.\n\ncd: Move to home directory.\nmv .bash_history /storage/cfg: Move the .bash_history file into /storage/cfg/ directory.\nln -s /storage/cfg/.bash_history: Create a symlink.\n\nNow we just have to update our pre-run.sh file. Here’s what mine looks like so far. Same thing here like we did before with .local/. We delete .bash_history if system generated one at this point. Then make the symlink.\n\n\n\npre-run.sh\n\n\nNow, if you want the history to work, make sure to close the terminal with Ctrl-d before shutting down the instance. I noticed that history does not get saved if I just shut down the instance.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 6"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-30-live_coding6/index.html#fastbook-setup",
    "href": "blog/posts/2022-11-30-live_coding6/index.html#fastbook-setup",
    "title": "Live coding 6",
    "section": "",
    "text": "Next thing we will cover is setting up fastbook. We will add this into our /notebooks/ so that we can have this permanently. Easy way to do it is simply go /notebooks/ directory and type git clone https://github.com/fastai/fastbook.git. However, we can fork this repo first. On the fastbook github page, click fork to get a copy of the repo. It is located on the top right side of the page.\n\n\n\nforking a repo\n\n\nThen use git clone to clone it inside of Paperspace. In order to use git to actually commit to GitHub, we have to setup .gitconfig file. Type git config --global user.name \"yourname\" and git config --global user.email \"youremail\" to setup them. Now, the same thing. To make this persistent, we have to move this file to /storage/cfg and create a symlink. Here are the steps:\n\ngit config --global user.name \"yourname\"\ngit config --global user.email \"youremail\"\nmv ~/.gitconfig /storage/cfg/\nln -s /storage/cfg/.gitconfig\n\nNow, our gitconfig is complete.\nLet’s go over some fastbook stuff now. We will see this code block when we look at the book:\n\nfrom fastai.vision.all import *\n\nThis means we are importing everything (*) from fastai/vision/all.py module. See how . changes to /? We can take a look at this file if we want to. Now, we know where to look when we encounter this stuff in our book to dig deeper.\n\n\n\n\n\n\nTipvim command tips\n\n\n\n\n\nIn vim, we can type * on a word to move the cursor to next occurance of it. Typing Ctrl-p while typing a word can fill the rest of the word like auto completion. The word has to be used before.\n\n\n\nAnother thing we find a lot is path. It is made with Pathlib from python standard library.\n\npath = untar_data(URLs.MNIST_SAMPLE)\npath\n\nPath('/home/kappa/.fastai/data/mnist_sample')\n\n\nIt is upgraded with fastai, so we can use ls method to look at files and directories within this path.\n\npath.ls()\n\n(#3) [Path('/home/kappa/.fastai/data/mnist_sample/labels.csv'),Path('/home/kappa/.fastai/data/mnist_sample/train'),Path('/home/kappa/.fastai/data/mnist_sample/valid')]\n\n\nAnother useful feature from path is Path.BASE_PATH. This can simplify our path by setting the base path.\n\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#3) [Path('labels.csv'),Path('train'),Path('valid')]\n\n\nSee how simple and readable it is now?",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 6"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-30-live_coding6/index.html#conclusion",
    "href": "blog/posts/2022-11-30-live_coding6/index.html#conclusion",
    "title": "Live coding 6",
    "section": "",
    "text": "Now our environment is setup so that we can read fastbook and commit changes to our forked repo. Committing to a forked repo is optional because paperspace will keep the notebooks. However, I like backing up my work at GitHub just in case I want to work on my local computer.\nIf you want to dig deeper, you can watch the video this blog is based on.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 6"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html",
    "title": "Learner Pt.1",
    "section": "",
    "text": "A student image\n\n\nWe learned about callbacks last time. In this blog, we will learn about a learner. It’s common to use a fit function to train a model. However, as we want to modify behaviors and add more functionalities, the fit function needs to be changed constantly. Soon, the function gets very complicated.\nWe can simplify the code with a learner by adding functionalities with callbacks. This way, we can try different strategies quickly without changing the learner. This blog is based on lesson 16 of the FastAI course.\n\n\nLet’s first start with grabbing tools and a dataset from hugging face. We will use Fashion MNIST data.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n\n\n\n!pip install -q datasets\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.3/519.3 kB 6.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 10.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 14.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 10.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 19.3 MB/s eta 0:00:00\n\n\n\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom miniai.datasets import *\nfrom miniai.conv import *\nimport logging\nfrom fastcore.test import test_close\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\n\nlogging.disable(logging.WARNING)\n\n\nfrom datasets import Array2D\n\nOkay, we imported all the libraries we need. Now we grab the data and shape it like we did before.\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\nbuilder = load_dataset_builder(name)\ndsd_features = builder.info.features.copy()\ndsd_features['image'] = Array2D(shape=[1, 28*28], dtype='float32')\ndsd = load_dataset(name, features=dsd_features)\ndsd.set_format(type=\"torch\")\n\n@inplace\ndef sq(b): b[x] = [o.squeeze().div(255) for o in b[x]]\n\ntds = dsd.map(sq, batched=True)\nbs = 1024\ndls = DataLoaders.from_dd(tds, bs)",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html#learner-part-1",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html#learner-part-1",
    "title": "Learner Pt.1",
    "section": "",
    "text": "A student image\n\n\nWe learned about callbacks last time. In this blog, we will learn about a learner. It’s common to use a fit function to train a model. However, as we want to modify behaviors and add more functionalities, the fit function needs to be changed constantly. Soon, the function gets very complicated.\nWe can simplify the code with a learner by adding functionalities with callbacks. This way, we can try different strategies quickly without changing the learner. This blog is based on lesson 16 of the FastAI course.\n\n\nLet’s first start with grabbing tools and a dataset from hugging face. We will use Fashion MNIST data.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n\n\n\n!pip install -q datasets\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.3/519.3 kB 6.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 10.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 14.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 10.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 19.3 MB/s eta 0:00:00\n\n\n\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom miniai.datasets import *\nfrom miniai.conv import *\nimport logging\nfrom fastcore.test import test_close\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\n\nlogging.disable(logging.WARNING)\n\n\nfrom datasets import Array2D\n\nOkay, we imported all the libraries we need. Now we grab the data and shape it like we did before.\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\nbuilder = load_dataset_builder(name)\ndsd_features = builder.info.features.copy()\ndsd_features['image'] = Array2D(shape=[1, 28*28], dtype='float32')\ndsd = load_dataset(name, features=dsd_features)\ndsd.set_format(type=\"torch\")\n\n@inplace\ndef sq(b): b[x] = [o.squeeze().div(255) for o in b[x]]\n\ntds = dsd.map(sq, batched=True)\nbs = 1024\ndls = DataLoaders.from_dd(tds, bs)",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html#basic-learner",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html#basic-learner",
    "title": "Learner Pt.1",
    "section": "Basic learner",
    "text": "Basic learner\nLet’s start with a basic learner. It grabs all the pieces it needs to train a model and divides the fitting with one_epoch and one_batch. In this basic version, there is no callback yet.\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr=0.2, opt_func=optim.SGD): fc.store_attr()\n\n    def fit(self, n_epochs):\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        for self.epoch in range(n_epochs):\n            self.one_epoch(True)\n            torch.no_grad()(self.one_epoch)(False)\n\n    def one_epoch(self, train):\n        self.losses, self.accs, self.ns = [], [], []\n        self.model.training = train\n        self.dl = self.dls.train if train else self.dls.valid\n        for self.n, self.batch in enumerate(self.dl): self.one_batch()\n        n = sum(self.ns)\n        fit_acc = sum(self.accs) / n\n        fit_loss = sum(self.losses) / n\n        print(f'epoch: {self.epoch}, accuracy: {fit_acc}, loss: {fit_loss}, training: {train}')\n\n    def one_batch(self):\n        xb, yb = self.batch\n        n = len(xb)\n        self.preds = self.model(xb)\n        self.loss = self.loss_func(self.preds, yb)\n        acc = (yb == self.preds.argmax(dim=1)).float().sum()\n        self.losses.append(self.loss * n)\n        self.accs.append(acc)\n        self.ns.append(n)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n\n\ndef get_model(): return nn.Sequential(nn.Linear(784, 100), nn.ReLU(), nn.Linear(100, 10))\n\n\nlearn = Learner(get_model(), dls, F.cross_entropy)\nlearn.fit(2)\n\nepoch: 0, accuracy: 0.6245333552360535, loss: 1.161386489868164, training: True\nepoch: 0, accuracy: 0.6980000138282776, loss: 0.8195666074752808, training: False\nepoch: 1, accuracy: 0.7442666888237, loss: 0.704105019569397, training: True\nepoch: 1, accuracy: 0.7699000239372253, loss: 0.6478453278541565, training: False\n\n\nYay! We have a learner! Learner basically has this shape, with fit, one_epoch, one_batch. However when we want to add a feature, we have to change the learner. Let’s make it more flexible by using callbacks. With callbacks, we can just add or remove them to modify the behavior of the learner.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html#exceptions",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html#exceptions",
    "title": "Learner Pt.1",
    "section": "Exceptions",
    "text": "Exceptions\nBefore we jump into a learner with callbacks, let’s look at some exceptions. What is an exception? It can be raised to exit a program.\n\nprint('hello')\n# Uncomment the line below to see how it works.\n# raise Exception()\nprint('bye')\n\nhello\nbye\n\n\nAs we can see, the program printed ‘hello’, but did not print ‘bye’. It raised an exception and exited. However, having a traceback is not very pretty. We can also exit without all that by using try and except block.\n\ntry:\n    print('hello')\n    raise Exception()\n    print('bye')\nexcept Exception: pass\n\nhello\n\n\nWith this, we can modify how we train the model. We can exit early or skip certain batch or epochs. Here are some customized Exceptions we will use.\n\nclass CancelFitException(Exception): pass\nclass CancelEpochException(Exception): pass\nclass CancelBatchException(Exception): pass",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html#learner-with-callback",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html#learner-with-callback",
    "title": "Learner Pt.1",
    "section": "Learner with callback",
    "text": "Learner with callback\nHere is a simple callback class.\n\nclass Callback: order = 0\n\nTo call it, we sort the callbacks by the order first and try to find the method that matches with the name provided. If it exists, it calls it.\n\ndef run_cb(cbs, name, learn=None):\n    for cb in sorted(cbs, key=attrgetter('order')):\n        method = getattr(cb, name, None)\n        if method: method(learn) # method is not None\n\nHere is a simple callback that counts how many batches it has.\n\nclass CountBatchCB(Callback):\n    def before_fit(self, learn): self.count = 0\n    def after_batch(self, learn): self.count += 1\n    def after_fit(self, learn): print(f'Total batch count: {self.count}')\n\n\ncb = CountBatchCB()\nrun_cb([cb], 'before_fit')\nrun_cb([cb], 'after_batch')\nrun_cb([cb], 'after_batch')\nrun_cb([cb], 'after_fit')\n\nTotal batch count: 2\n\n\nNow let’s go over the learner with callbacks. This learner calls six callbacks: before_fit, after_fit, before_epoch, after_epoch, before_batch, and after_batch. With callbacks, we can modify how it trains. We can even stop training with an exception.\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, cbs=[], lr=0.2, opt_func=optim.SGD): fc.store_attr()\n\n    def callback(self, name): run_cb(self.cbs, name, self)\n\n    def fit(self, n_epochs):\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.epochs = range(n_epochs)\n        try:\n            self.callback('before_fit')\n            for self.epoch in self.epochs:\n                self.one_epoch(True)\n                torch.no_grad()(self.one_epoch)(False)\n            self.callback('after_fit')\n        except CancelFitException: pass\n\n    def one_epoch(self, train):\n        self.model.training = train\n        self.dl = self.dls.train if train else self.dls.valid\n        try:\n            self.callback('before_epoch')\n            for self.n, self.batch in enumerate(self.dl): self.one_batch()\n            self.callback('after_epoch')\n        except CancelEpochException: pass\n\n    def one_batch(self):\n        try:\n            self.callback('before_batch')\n            self.xb, self.yb = self.batch\n            self.preds = self.model(self.xb)\n            self.loss = self.loss_func(self.preds, self.yb)\n            if self.model.training:\n                self.loss.backward()\n                self.opt.step()\n                self.opt.zero_grad()\n            self.callback('after_batch')\n        except CancelBatchException: pass\n\n\nlearn = Learner(get_model(), dls, F.cross_entropy, cbs=[cb])\nlearn.fit(2)\n\nTotal batch count: 128\n\n\nWe can create a NBatchCB to only train for N batches. Then, we exit the training with CancelFitException.\n\nclass NBatchCB(Callback):\n    def __init__(self, n):\n        self.n = n\n        self.n_batches = 0\n\n    def after_batch(self, learn):\n        if self.n_batches &gt;= self.n: raise CancelFitException()\n        self.n_batches += 1\n\n\nnbatch = NBatchCB(3)\nlearn = Learner(get_model(), dls, F.cross_entropy, cbs=[cb, nbatch])\nlearn.fit(1)\n\n\nnbatch.n_batches\n\n3\n\n\nOur learner is so much more flexible than before. As we write more callbacks, we can feel how powerful it is. At first, it may feel overwhelming because there are so many things to try, but not sure what is helpful. We can explore more callbacks and get a feeling on how it really works.\nWe do not have a metric for this learner, so we don’t know how well it is doing. Instead, we can add any metric as a callback. Let’s create a metric callback.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html#metric",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html#metric",
    "title": "Learner Pt.1",
    "section": "Metric",
    "text": "Metric\nBefore creating a metric callback, let’s create a Metric class. This class serves as a base class for Accuracy. It can reset internal state, calculate metric, update the internal state by computing, and has a value property.\n\nclass Metric:\n    def __init__(self): self.reset()\n\n    def reset(self):\n        self.ns, self.accs = [], []\n\n    def update(self, preds, targs):\n        res = self.calculate(preds, targs)\n        n = len(preds)\n        self.ns.append(n)\n        self.accs.append(res * n)\n\n    def calculate(self, preds, targs): return 0\n\n    @property\n    def value(self):\n        return tensor(self.accs).sum() / tensor(self.ns).sum()\n\n\nclass Accuracy(Metric):\n    def calculate(self, preds, targs):\n        return (preds == targs).float().mean()\n\n\nmetric = Accuracy()\nmetric.update(tensor([1,2]), tensor([1,2]))\nmetric.value\n\ntensor(1.)\n\n\n\nmetric.update(tensor([1,3]), tensor([1,2]))\nmetric.value\n\ntensor(0.75)\n\n\nWe can create other metrics by inheriting and defining compute.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html#some-more-callbacks",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html#some-more-callbacks",
    "title": "Learner Pt.1",
    "section": "Some more callbacks",
    "text": "Some more callbacks\nTorcheval has MulticlassAccuracy, which is the same thing as we just defined. Let’s use that. Instead of value, we have to use compute to get the calculation.\n\n!pip install torcheval\n\nCollecting torcheval\n  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/179.2 kB ? eta -:--:--     ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 112.6/179.2 kB 3.4 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.7 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.7.1)\nInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\n\n\n\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n\n\nmetric = MulticlassAccuracy()\nmetric.update(tensor([1,2]), tensor([1,2]))\nmetric.compute()\n\ntensor(1.)\n\n\n\nmetric.update(tensor([1,3]), tensor([1,2]))\nmetric.compute()\n\ntensor(0.75)\n\n\nNow we can create a metrics callback. This will include all the metrics and loss. We can grab the loss from the learner because callbacks have access to the learner, which means more power and flexibility. For instance, we can grab the loss from the learner.\n\nclass MetricsCB(Callback):\n    def __init__(self, *ms, device=def_device, **metrics):\n        for m in ms:\n            metrics[type(m).__name__] = m\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics['loss'] = self.loss = Mean()\n        self.loss.to(device)\n\n    def _log(self, log):\n        print(log)\n\n    def before_epoch(self, learn):\n        for m in self.all_metrics.values(): m.reset()\n\n    def after_batch(self, learn):\n        for m in self.metrics.values():\n            m.update(learn.preds, learn.yb)\n        # import pdb; pdb.set_trace()\n        self.loss.update(learn.loss)\n\n    def after_epoch(self, learn):\n        log = {k:f'{v.compute().item():.3f}' for k, v in self.all_metrics.items()}\n        log['epoch'] = learn.epoch\n        log['train'] = learn.model.training\n        self._log(log)\n\n\nmodel = get_model()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[metrics])\nlearn.fit(2)\n\n{'accuracy': '0.618', 'loss': '1.150', 'epoch': 0, 'train': True}\n{'accuracy': '0.707', 'loss': '0.790', 'epoch': 0, 'train': False}\n{'accuracy': '0.748', 'loss': '0.699', 'epoch': 1, 'train': True}\n{'accuracy': '0.757', 'loss': '0.654', 'epoch': 1, 'train': False}\n\n\nWe can also use callbacks to move model and data batch into any device, such as cuda, apple GPU, or CPU. By default, it will use def_device, which means GPU if the computer has one available.\n\nclass DeviceCB(Callback):\n    def __init__(self, device=def_device): self.device = device\n    def before_fit(self, learn): learn.model.to(device=self.device)\n    def before_batch(self, learn):\n        learn.batch = to_device(learn.batch, device=self.device)\n\n\nmodel = get_model()\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[metrics, DeviceCB()])\nlearn.fit(2)\n\n{'accuracy': '0.623', 'loss': '1.155', 'epoch': 0, 'train': True}\n{'accuracy': '0.729', 'loss': '0.778', 'epoch': 0, 'train': False}\n{'accuracy': '0.748', 'loss': '0.707', 'epoch': 1, 'train': True}\n{'accuracy': '0.739', 'loss': '0.711', 'epoch': 1, 'train': False}",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2023-08-30-Learner_part_1/index.html#conclusion",
    "href": "blog/posts/2023-08-30-Learner_part_1/index.html#conclusion",
    "title": "Learner Pt.1",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog, we looked at two versions of a learner and how we increased flexibility with callbacks. With callbacks, we can use different metrics without changing the learner. Also, we can automatically use GPU if it is available. We can exit a batch, epoch, or fit entirely by raising an exception as well. In part two, we will look at more callbacks and a different version of the learner, which has more power and concise.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.1"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-07-alien-vs.-ghost-pt1/index.html",
    "href": "blog/posts/2022-11-07-alien-vs.-ghost-pt1/index.html",
    "title": "Alien vs. Ghost Pt.1 Training",
    "section": "",
    "text": "In this article, we will make a image classification model that attempts to differentiate between aliens and ghosts.\n\n\n\nprediction page\n\n\nBut you may say, aliens and ghosts are not real. Well, their existence is not proven scientifically, yet. But it doesn’t mean we cannot search images online for them. With those images, we will create a model. Hopefully we may be able to create the real alien vs. ghosts model in the future, but let’s start with this.\nFor training, I will use FastAI library because there are many great courses for learning. Although it is possible to only use CPU to train the model, it is faster to use GPU. I am on Google colab, which provides free GPU. After training, we will save the model so that we can use it for deployment on Hugging Face spaces. You can check out the final product before we begin!\nFirst, we import FastAI library.\n\n!pip install -Uqq fastai\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n     |████████████████████████████████| 719 kB 20.2 MB/s \n     |████████████████████████████████| 5.3 MB 40.7 MB/s \n     |████████████████████████████████| 441 kB 44.3 MB/s \n     |████████████████████████████████| 1.3 MB 6.6 MB/s \n     |████████████████████████████████| 1.6 MB 50.2 MB/s \n     |████████████████████████████████| 95 kB 5.6 MB/s \n     |████████████████████████████████| 115 kB 67.4 MB/s \n     |████████████████████████████████| 163 kB 62.5 MB/s \n     |████████████████████████████████| 212 kB 70.8 MB/s \n     |████████████████████████████████| 127 kB 68.1 MB/s \n     |████████████████████████████████| 115 kB 57.5 MB/s \n     |████████████████████████████████| 7.6 MB 61.9 MB/s \nMounted at /content/gdrive\n\n\n\n\nWe use duckduckgo to search and download images. FastAI provides search_images_ddg so that we do not have to go to a search engine and download an image one by one!\n\nsearch_results = search_images_ddg('alien images')\n\n\nims = search_results\nlen(ims)\n\n200\n\n\n\ndest = 'images/aliens'\ndownload_url(ims[2], dest)\n\n\n\n\n\n\n    \n      \n      101.68% [344064/338377 00:00&lt;00:00]\n    \n    \n\n\nPath('images/aliens')\n\n\nWe can check the image and see if it looks right. It seems like an alien.\n\nim = Image.open(dest)\nim.to_thumb(128, 128)\n\n\n\n\n\n\n\n\nNow we download images to our path. We create two directories, alien and ghost, and download each category of images into each directory.\n\ncategory_types = 'alien', 'ghost'\npath = Path('alien_ghost')\n\n\nif not path.exists():\n    path.mkdir()\n    for o in category_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o} images')\n        download_images(dest, urls=results)\n\n\nfns = get_image_files(path)\nfns\n\n(#397) [Path('alien_ghost/alien/5d727cc0-806f-4ec4-83ae-d7c06f43ec9e.jpg'),Path('alien_ghost/alien/f2ffe42f-c61b-45f9-b36f-68cf6de088c6.jpg'),Path('alien_ghost/alien/18ec3bc4-6884-492b-a36b-f24b74fefb55.jpg'),Path('alien_ghost/alien/0411337c-49a6-457b-9091-44e21a2a1671.jpg'),Path('alien_ghost/alien/4d33d45f-c3cb-48e2-82ac-29673e9e7fc0.jpg'),Path('alien_ghost/alien/f1774ead-0ca1-4bc8-9522-f6517b1009f3.jpg'),Path('alien_ghost/alien/eb4d7087-f726-453a-b017-3898dadd1fdb.jpg'),Path('alien_ghost/alien/bcbea1c7-0521-4bd3-96e3-fe3c908ad0ea.jpg'),Path('alien_ghost/alien/546f275e-4cad-4db8-ac7f-bbece1b556de.png'),Path('alien_ghost/alien/250d3e56-a7be-4604-a184-7518e1f235ef.jpg')...]\n\n\nFailed one are ones that cannot be opened, so we unlink them from our path.\n\nfailed = verify_images(fns)\nfailed\n\n(#8) [Path('alien_ghost/alien/8b2cfce8-de67-49ea-8171-4144400bdf22.jpg'),Path('alien_ghost/alien/d77ad0bd-a6cd-48bd-8696-044bf574420d.jpg'),Path('alien_ghost/ghost/d993cf0e-769b-4eb0-ae9e-c5a2233e373c.jpg'),Path('alien_ghost/ghost/49ae4580-2a18-4bab-af79-4ffb79f8a5ca.jpg'),Path('alien_ghost/ghost/4f5bded3-f543-4dd7-995a-ae3af65b7e58.gif'),Path('alien_ghost/ghost/e7ed422d-d757-457c-9de4-d4795efa9289.jpg'),Path('alien_ghost/ghost/343f6aca-79f2-481f-a5ec-66f9787d6543.jpg'),Path('alien_ghost/ghost/138e762f-875d-4bd2-889d-5d6866ebabff.gif')]\n\n\n\nfailed.map(Path.unlink)\n\n(#8) [None,None,None,None,None,None,None,None]\n\n\n\n\n\nWe now create DataBlocks. Simply put, it is a bridge between raw data and a model. We specify input and output for the model, how to get the input, how to split train data from validation data, how data are labelled, and what transfroms are needed for the input.\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),               # We take images and try to classify them based on categories.\n    get_items=get_image_files,                        # inputs are images.\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),  # randomly pick 20% of the input for validation.\n    get_y=parent_label,                               # parent directory name is the label for each image.\n    item_tfms=Resize(128))                            # Resize each image to 128.\n\n\ndls = dblock.dataloaders(path)\ndls\n\n&lt;fastai.data.core.DataLoaders&gt;\n\n\nLook at the images and labels to make sure everything looks right. Seems like images are correctly labelled. At this point, it is okay to have some wrong images or labels. We will fix that later.\n\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nWe can add data augmentation (transforms) into images so that we can train more efficiently with less data.\n\ndblock = dblock.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = dblock.dataloaders(path)\n\nNow we train the model.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.999617\n0.516202\n0.233766\n00:16\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.587268\n0.345220\n0.155844\n00:17\n\n\n1\n0.503984\n0.289026\n0.090909\n00:18\n\n\n2\n0.407598\n0.316117\n0.103896\n00:16\n\n\n3\n0.376871\n0.302991\n0.090909\n00:17\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\nThere are some errors. We can look at the errors by looking at the confusion matrix and top losses.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\n\n# Check the high losses\ninterp.plot_top_losses(10, nrows=2)\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\nSome images are not labelled correctly or wrong images. We can easily fix it by using the cleaner.\n\n# Pick which ones to delete or to move to another category\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n# make cleaner take effect\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nAfter cleaning, we train again for the cleaned data. So, we go back up to DataLoaders and come back down here. After that, we can export the model. We can then download the model so that we can deploy it into Hugging Face Spaces.\n\nlearn.export()\n\n\n# check if file is pickled\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nThat’s it. If this notebook was run locally, there will be the the model in the same directory as the notebook. If this notebook was run on Google colab, the model has to be downloaded from the directory from the panel on the left.",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.1 Training"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-07-alien-vs.-ghost-pt1/index.html#collect-data",
    "href": "blog/posts/2022-11-07-alien-vs.-ghost-pt1/index.html#collect-data",
    "title": "Alien vs. Ghost Pt.1 Training",
    "section": "",
    "text": "We use duckduckgo to search and download images. FastAI provides search_images_ddg so that we do not have to go to a search engine and download an image one by one!\n\nsearch_results = search_images_ddg('alien images')\n\n\nims = search_results\nlen(ims)\n\n200\n\n\n\ndest = 'images/aliens'\ndownload_url(ims[2], dest)\n\n\n\n\n\n\n    \n      \n      101.68% [344064/338377 00:00&lt;00:00]\n    \n    \n\n\nPath('images/aliens')\n\n\nWe can check the image and see if it looks right. It seems like an alien.\n\nim = Image.open(dest)\nim.to_thumb(128, 128)\n\n\n\n\n\n\n\n\nNow we download images to our path. We create two directories, alien and ghost, and download each category of images into each directory.\n\ncategory_types = 'alien', 'ghost'\npath = Path('alien_ghost')\n\n\nif not path.exists():\n    path.mkdir()\n    for o in category_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o} images')\n        download_images(dest, urls=results)\n\n\nfns = get_image_files(path)\nfns\n\n(#397) [Path('alien_ghost/alien/5d727cc0-806f-4ec4-83ae-d7c06f43ec9e.jpg'),Path('alien_ghost/alien/f2ffe42f-c61b-45f9-b36f-68cf6de088c6.jpg'),Path('alien_ghost/alien/18ec3bc4-6884-492b-a36b-f24b74fefb55.jpg'),Path('alien_ghost/alien/0411337c-49a6-457b-9091-44e21a2a1671.jpg'),Path('alien_ghost/alien/4d33d45f-c3cb-48e2-82ac-29673e9e7fc0.jpg'),Path('alien_ghost/alien/f1774ead-0ca1-4bc8-9522-f6517b1009f3.jpg'),Path('alien_ghost/alien/eb4d7087-f726-453a-b017-3898dadd1fdb.jpg'),Path('alien_ghost/alien/bcbea1c7-0521-4bd3-96e3-fe3c908ad0ea.jpg'),Path('alien_ghost/alien/546f275e-4cad-4db8-ac7f-bbece1b556de.png'),Path('alien_ghost/alien/250d3e56-a7be-4604-a184-7518e1f235ef.jpg')...]\n\n\nFailed one are ones that cannot be opened, so we unlink them from our path.\n\nfailed = verify_images(fns)\nfailed\n\n(#8) [Path('alien_ghost/alien/8b2cfce8-de67-49ea-8171-4144400bdf22.jpg'),Path('alien_ghost/alien/d77ad0bd-a6cd-48bd-8696-044bf574420d.jpg'),Path('alien_ghost/ghost/d993cf0e-769b-4eb0-ae9e-c5a2233e373c.jpg'),Path('alien_ghost/ghost/49ae4580-2a18-4bab-af79-4ffb79f8a5ca.jpg'),Path('alien_ghost/ghost/4f5bded3-f543-4dd7-995a-ae3af65b7e58.gif'),Path('alien_ghost/ghost/e7ed422d-d757-457c-9de4-d4795efa9289.jpg'),Path('alien_ghost/ghost/343f6aca-79f2-481f-a5ec-66f9787d6543.jpg'),Path('alien_ghost/ghost/138e762f-875d-4bd2-889d-5d6866ebabff.gif')]\n\n\n\nfailed.map(Path.unlink)\n\n(#8) [None,None,None,None,None,None,None,None]",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.1 Training"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-07-alien-vs.-ghost-pt1/index.html#dataloaders",
    "href": "blog/posts/2022-11-07-alien-vs.-ghost-pt1/index.html#dataloaders",
    "title": "Alien vs. Ghost Pt.1 Training",
    "section": "",
    "text": "We now create DataBlocks. Simply put, it is a bridge between raw data and a model. We specify input and output for the model, how to get the input, how to split train data from validation data, how data are labelled, and what transfroms are needed for the input.\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),               # We take images and try to classify them based on categories.\n    get_items=get_image_files,                        # inputs are images.\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),  # randomly pick 20% of the input for validation.\n    get_y=parent_label,                               # parent directory name is the label for each image.\n    item_tfms=Resize(128))                            # Resize each image to 128.\n\n\ndls = dblock.dataloaders(path)\ndls\n\n&lt;fastai.data.core.DataLoaders&gt;\n\n\nLook at the images and labels to make sure everything looks right. Seems like images are correctly labelled. At this point, it is okay to have some wrong images or labels. We will fix that later.\n\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nWe can add data augmentation (transforms) into images so that we can train more efficiently with less data.\n\ndblock = dblock.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = dblock.dataloaders(path)\n\nNow we train the model.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.999617\n0.516202\n0.233766\n00:16\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.587268\n0.345220\n0.155844\n00:17\n\n\n1\n0.503984\n0.289026\n0.090909\n00:18\n\n\n2\n0.407598\n0.316117\n0.103896\n00:16\n\n\n3\n0.376871\n0.302991\n0.090909\n00:17\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\nThere are some errors. We can look at the errors by looking at the confusion matrix and top losses.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\n\n# Check the high losses\ninterp.plot_top_losses(10, nrows=2)\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n\nSome images are not labelled correctly or wrong images. We can easily fix it by using the cleaner.\n\n# Pick which ones to delete or to move to another category\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n# make cleaner take effect\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nAfter cleaning, we train again for the cleaned data. So, we go back up to DataLoaders and come back down here. After that, we can export the model. We can then download the model so that we can deploy it into Hugging Face Spaces.\n\nlearn.export()\n\n\n# check if file is pickled\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nThat’s it. If this notebook was run locally, there will be the the model in the same directory as the notebook. If this notebook was run on Google colab, the model has to be downloaded from the directory from the panel on the left.",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.1 Training"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-17-pass@k/index.html",
    "href": "blog/posts/2025-07-17-pass@k/index.html",
    "title": "What is pass@k evaluation metric?",
    "section": "",
    "text": "After training large language models (LLMs), we want to know how good it is. Many benchmakrs use pass@k metric. But what does it mean? In this blog, we will learn about what pass@k means. We will focus on evaluting in coding ability.\nWhen looking at the benchmarks for coding, we see HumanEval with pass@1 or pass@10. As you can see below, this is an evaluation benchmark from Tulu3 by Lambert et al. \n\n\n\nTable2 of Tulu3 paper with pass@10 HumanEval metrics\n\n\nTulu3 evaluation benchmark with HmanEval using pass@10.\n\n\n\nLet’s first think about how how we could evaluate our fresh LLM. We could let our model generate code by providing pseudocode as input and comapre the code with the actual working code we wrote ourselves. However, code can be written many different ways that does the same thing, and this does not account for that. Another way we can evaluate our model is to give our LLM a documentation string as an input and let it generate code. Then, we run tests on the generated code for correctness. This way, as long as the code passes tests, we count them as correct. This is a more robust way to evaluate models, which is how Chen et al. did in “Evaluating Large Language Models Trained on Code”. HumanEval evaluation set is also from this paper.\nSince we have a way to mark code samples as correct, we need a way to calculate the score. There are different ways to do this. Kulal et al. suggested using pass@k metric. This generates k code samples per problem. If any sample passes the unit tests, the LLM gets a score. For the total score, it is the percentage of correct problems out of total problems.\nThis simple way has a problem as it has a high variance. When k is 1, we generate 1 sample and the model can sometimes get it right, sometimes not because there is randomness when the models generate responses.\nChen et al. suggeted another pass@k metric. They generate n samples (where n &gt;= k) per problem. In the paper, they used n=200 and k &lt;= 100. From these samples, they count c, where c is the number of samples that pass unit tests. From this, they calculate the new pass@k defined as the following:\n\\[ \\text{pass@k} := \\underset{\\text{Problems}}{\\mathbb{E}} \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right] \\quad (1) \\]\nThis also makrs code samples as correct if any sample passes the unit tests, but calculation is different. Let’s take a look at it in more detail.\nWhen I first looked at this, I had no idea what it meant. The \\({\\mathbb{E}}\\) means take the expectation of, which is the same thing as average. So we are taking the average of problems. Inside the bracket applies to each problem. Now let’s look at \\(\\binom{n}{k}\\), which is n choose k. This is an expression for combiantions. If it has been a long time since you encountered this, take a brief review from khan academy. If you remember permutations, order matters for permutations, and order does not matter in combinations. And to calculate, we use the following:\n\\[ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\]\nIntuitively, we can think of the pass@k equation as the following:\n\n\\({\\binom{n}{k}}\\): From n samples, how many different ways can we choose k samples?\n\\({\\binom{n-c}{k}}\\): Out of all the incorrect samples (n-c), how many ways can we choose k samples?\n\nTherefore, the following fraction,\n\\[\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\]\nrepresents the probability of choosing all the wrong samples when choosing k samples from n samples.\nBy applying 1 - in front of the fraction, we are taking the complement of this. Complement of all wrong samples means at least one sample is correct. Therefore the expression,\n\\[\\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]\\]\nrepresents the probability of choosing at least one sample that is correct. We are using complement of a set to calculate the probability because it is easier to calculate it this way. Remember the part inside of bracket only applies for one problem. And we do this for all the problems and take the average to get pass@k.\nTherefore, if we increase the number of samples we pick (k), the score increases.\n\n\n\nLet’s do something more hands on. When we have n = 200 and k = 1, how would it look like? Let’s use python to calculate it. With Python 3.8+, we can calculate combination with math.comb.\n\nfrom math import comb\nimport numpy as np\n\nFrom 200 samples, there are 200 ways to choose 1 sample.\n\ncomb(200, 1)\n\n200\n\n\nIf we set k = 2 and choose 2 samples, there are more ways to choose.\n\ncomb(200, 2)\n\n19900\n\n\nIf we are choosing 100 samples, there are so many ways to do so.\n\ncomb(200, 100)\n\n90548514656103281165404177077484163874504589675413336841320\n\n\nAnd if we want to choose 199, there are 200 ways.\n\ncomb(200, 199)\n\n200\n\n\nAnd if we want to choose 200 out of 200, there is only one way to do it.\n\ncomb(200, 200)\n\n1\n\n\nWe can calculate pass@k and see how the evaluation score changes based on number of correct samples, c, and k.\n\ndef pass_at_k(c, k=1, n=200): return 1 - (comb(n-c,k) / comb(n,k))\n\n\npass_at_k(0)\n\n0.0\n\n\n\npass_at_k(1)\n\n0.0050000000000000044\n\n\n\npass_at_k(10)\n\n0.050000000000000044\n\n\n\npass_at_k(100)\n\n0.5\n\n\n\npass_at_k(200)\n\n1.0\n\n\nWhat happens if k is 10?\n\npass_at_k(0, k=10)\n\n0.0\n\n\n\npass_at_k(1, k=10)\n\n0.050000000000000044\n\n\n\npass_at_k(10, k=10)\n\n0.40854786608141713\n\n\n\npass_at_k(100, k=10)\n\n0.9992289739372822\n\n\n\npass_at_k(200, k=10)\n\n1.0\n\n\nThe bigger k value yields bigger score for the metric.\nHow about k is 100?\n\npass_at_k(0, k=100)\n\n0.0\n\n\n\npass_at_k(1, k=100)\n\n0.5\n\n\n\npass_at_k(10, k=100)\n\n0.9992289739372822\n\n\n\npass_at_k(100, k=100)\n\n1.0\n\n\n\npass_at_k(200, k=100)\n\n1.0\n\n\nWhat would happen if we make more samples? Create more samples while leaving k is pretty much same as decreasing k. Therefore, the score gets low.\n\npass_at_k(0, k=100, n=500)\n\n0.0\n\n\n\npass_at_k(1, k=100, n=500)\n\n0.19999999999999996\n\n\n\npass_at_k(10, k=100, n=500)\n\n0.8950489654316426\n\n\n\npass_at_k(100, k=100, n=500)\n\n0.9999999999890197\n\n\n\npass_at_k(200, k=100, n=500)\n\n1.0\n\n\nHere is another pass_at_k which is more numerically stable. This came from the Chen et al.\n\ndef pass_at_k(c, k=1, n=200):\n    if n - c &lt; k: return 1.0\n    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\n\nWe get the same result.\n\npass_at_k(1)\n\n0.0050000000000000044\n\n\n\n\n\nIn this blog, we looked at how pass@k is calculated using a method provided by Chen et al. This way is less finicky than the method suggested by Kulal et al. \nTo learn more about pass@k, here are the papers:\n\nSPoC: Search-based Pseudocode to Code by Kulal et al.: https://arxiv.org/abs/1906.04908\nEvaluating Large Language Models Trained on Code (also HumanEvals) by Chen et al.: https://arxiv.org/abs/2107.03374\n\nIf you want to learn more about proabaility and statistics:\n\nStat110 by Joe Blitzstein in Harvard: https://stat110.hsites.harvard.edu/. This goes more in depth into sets, proofs, distributions, etc. It has lecture videos and homeworks. This is math and theory heavy.\nA Concrete Introduction to Probability (using Python) by Peter Norvig: https://github.com/norvig/pytudes/blob/main/ipynb/Probability.ipynb. This is more hands on approach using python.",
    "crumbs": [
      "Home",
      "Blog",
      "What is pass@k evaluation metric?"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-17-pass@k/index.html#introduction",
    "href": "blog/posts/2025-07-17-pass@k/index.html#introduction",
    "title": "What is pass@k evaluation metric?",
    "section": "",
    "text": "After training large language models (LLMs), we want to know how good it is. Many benchmakrs use pass@k metric. But what does it mean? In this blog, we will learn about what pass@k means. We will focus on evaluting in coding ability.\nWhen looking at the benchmarks for coding, we see HumanEval with pass@1 or pass@10. As you can see below, this is an evaluation benchmark from Tulu3 by Lambert et al. \n\n\n\nTable2 of Tulu3 paper with pass@10 HumanEval metrics\n\n\nTulu3 evaluation benchmark with HmanEval using pass@10.",
    "crumbs": [
      "Home",
      "Blog",
      "What is pass@k evaluation metric?"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-17-pass@k/index.html#intuition-behind-formula",
    "href": "blog/posts/2025-07-17-pass@k/index.html#intuition-behind-formula",
    "title": "What is pass@k evaluation metric?",
    "section": "",
    "text": "Let’s first think about how how we could evaluate our fresh LLM. We could let our model generate code by providing pseudocode as input and comapre the code with the actual working code we wrote ourselves. However, code can be written many different ways that does the same thing, and this does not account for that. Another way we can evaluate our model is to give our LLM a documentation string as an input and let it generate code. Then, we run tests on the generated code for correctness. This way, as long as the code passes tests, we count them as correct. This is a more robust way to evaluate models, which is how Chen et al. did in “Evaluating Large Language Models Trained on Code”. HumanEval evaluation set is also from this paper.\nSince we have a way to mark code samples as correct, we need a way to calculate the score. There are different ways to do this. Kulal et al. suggested using pass@k metric. This generates k code samples per problem. If any sample passes the unit tests, the LLM gets a score. For the total score, it is the percentage of correct problems out of total problems.\nThis simple way has a problem as it has a high variance. When k is 1, we generate 1 sample and the model can sometimes get it right, sometimes not because there is randomness when the models generate responses.\nChen et al. suggeted another pass@k metric. They generate n samples (where n &gt;= k) per problem. In the paper, they used n=200 and k &lt;= 100. From these samples, they count c, where c is the number of samples that pass unit tests. From this, they calculate the new pass@k defined as the following:\n\\[ \\text{pass@k} := \\underset{\\text{Problems}}{\\mathbb{E}} \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right] \\quad (1) \\]\nThis also makrs code samples as correct if any sample passes the unit tests, but calculation is different. Let’s take a look at it in more detail.\nWhen I first looked at this, I had no idea what it meant. The \\({\\mathbb{E}}\\) means take the expectation of, which is the same thing as average. So we are taking the average of problems. Inside the bracket applies to each problem. Now let’s look at \\(\\binom{n}{k}\\), which is n choose k. This is an expression for combiantions. If it has been a long time since you encountered this, take a brief review from khan academy. If you remember permutations, order matters for permutations, and order does not matter in combinations. And to calculate, we use the following:\n\\[ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\]\nIntuitively, we can think of the pass@k equation as the following:\n\n\\({\\binom{n}{k}}\\): From n samples, how many different ways can we choose k samples?\n\\({\\binom{n-c}{k}}\\): Out of all the incorrect samples (n-c), how many ways can we choose k samples?\n\nTherefore, the following fraction,\n\\[\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\]\nrepresents the probability of choosing all the wrong samples when choosing k samples from n samples.\nBy applying 1 - in front of the fraction, we are taking the complement of this. Complement of all wrong samples means at least one sample is correct. Therefore the expression,\n\\[\\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]\\]\nrepresents the probability of choosing at least one sample that is correct. We are using complement of a set to calculate the probability because it is easier to calculate it this way. Remember the part inside of bracket only applies for one problem. And we do this for all the problems and take the average to get pass@k.\nTherefore, if we increase the number of samples we pick (k), the score increases.",
    "crumbs": [
      "Home",
      "Blog",
      "What is pass@k evaluation metric?"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-17-pass@k/index.html#calculating-metric",
    "href": "blog/posts/2025-07-17-pass@k/index.html#calculating-metric",
    "title": "What is pass@k evaluation metric?",
    "section": "",
    "text": "Let’s do something more hands on. When we have n = 200 and k = 1, how would it look like? Let’s use python to calculate it. With Python 3.8+, we can calculate combination with math.comb.\n\nfrom math import comb\nimport numpy as np\n\nFrom 200 samples, there are 200 ways to choose 1 sample.\n\ncomb(200, 1)\n\n200\n\n\nIf we set k = 2 and choose 2 samples, there are more ways to choose.\n\ncomb(200, 2)\n\n19900\n\n\nIf we are choosing 100 samples, there are so many ways to do so.\n\ncomb(200, 100)\n\n90548514656103281165404177077484163874504589675413336841320\n\n\nAnd if we want to choose 199, there are 200 ways.\n\ncomb(200, 199)\n\n200\n\n\nAnd if we want to choose 200 out of 200, there is only one way to do it.\n\ncomb(200, 200)\n\n1\n\n\nWe can calculate pass@k and see how the evaluation score changes based on number of correct samples, c, and k.\n\ndef pass_at_k(c, k=1, n=200): return 1 - (comb(n-c,k) / comb(n,k))\n\n\npass_at_k(0)\n\n0.0\n\n\n\npass_at_k(1)\n\n0.0050000000000000044\n\n\n\npass_at_k(10)\n\n0.050000000000000044\n\n\n\npass_at_k(100)\n\n0.5\n\n\n\npass_at_k(200)\n\n1.0\n\n\nWhat happens if k is 10?\n\npass_at_k(0, k=10)\n\n0.0\n\n\n\npass_at_k(1, k=10)\n\n0.050000000000000044\n\n\n\npass_at_k(10, k=10)\n\n0.40854786608141713\n\n\n\npass_at_k(100, k=10)\n\n0.9992289739372822\n\n\n\npass_at_k(200, k=10)\n\n1.0\n\n\nThe bigger k value yields bigger score for the metric.\nHow about k is 100?\n\npass_at_k(0, k=100)\n\n0.0\n\n\n\npass_at_k(1, k=100)\n\n0.5\n\n\n\npass_at_k(10, k=100)\n\n0.9992289739372822\n\n\n\npass_at_k(100, k=100)\n\n1.0\n\n\n\npass_at_k(200, k=100)\n\n1.0\n\n\nWhat would happen if we make more samples? Create more samples while leaving k is pretty much same as decreasing k. Therefore, the score gets low.\n\npass_at_k(0, k=100, n=500)\n\n0.0\n\n\n\npass_at_k(1, k=100, n=500)\n\n0.19999999999999996\n\n\n\npass_at_k(10, k=100, n=500)\n\n0.8950489654316426\n\n\n\npass_at_k(100, k=100, n=500)\n\n0.9999999999890197\n\n\n\npass_at_k(200, k=100, n=500)\n\n1.0\n\n\nHere is another pass_at_k which is more numerically stable. This came from the Chen et al.\n\ndef pass_at_k(c, k=1, n=200):\n    if n - c &lt; k: return 1.0\n    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\n\nWe get the same result.\n\npass_at_k(1)\n\n0.0050000000000000044",
    "crumbs": [
      "Home",
      "Blog",
      "What is pass@k evaluation metric?"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-17-pass@k/index.html#conclusion",
    "href": "blog/posts/2025-07-17-pass@k/index.html#conclusion",
    "title": "What is pass@k evaluation metric?",
    "section": "",
    "text": "In this blog, we looked at how pass@k is calculated using a method provided by Chen et al. This way is less finicky than the method suggested by Kulal et al. \nTo learn more about pass@k, here are the papers:\n\nSPoC: Search-based Pseudocode to Code by Kulal et al.: https://arxiv.org/abs/1906.04908\nEvaluating Large Language Models Trained on Code (also HumanEvals) by Chen et al.: https://arxiv.org/abs/2107.03374\n\nIf you want to learn more about proabaility and statistics:\n\nStat110 by Joe Blitzstein in Harvard: https://stat110.hsites.harvard.edu/. This goes more in depth into sets, proofs, distributions, etc. It has lecture videos and homeworks. This is math and theory heavy.\nA Concrete Introduction to Probability (using Python) by Peter Norvig: https://github.com/norvig/pytudes/blob/main/ipynb/Probability.ipynb. This is more hands on approach using python.",
    "crumbs": [
      "Home",
      "Blog",
      "What is pass@k evaluation metric?"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-06-Optimizer/index.html",
    "href": "blog/posts/2023-11-06-Optimizer/index.html",
    "title": "Optimizer",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n!pip -q install torcheval\n!pip -q install datasets\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 4.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 10.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 16.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 17.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 28.9 MB/s eta 0:00:00",
    "crumbs": [
      "Home",
      "Blog",
      "Optimizer"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-06-Optimizer/index.html#optimizers",
    "href": "blog/posts/2023-11-06-Optimizer/index.html#optimizers",
    "title": "Optimizer",
    "section": "Optimizers",
    "text": "Optimizers\nHere is the SGD class, a base class for all other optimizers we will define. For updating parameters, we need parameters, learning rate, and weight decay (optional). We will learn about what weight decay is later. We have step, reg_step, opt_step, and zero_grad. step is divided into reg_step and opt_step. reg_step is the weight decay part which tries to regulate the growth of weights. opt_step is the update part using gradient and learning rate.\n\nclass SGD:\n    def __init__(self, params, lr, wd=0.):\n        self.params = list(params)\n        self.lr = lr\n        self.wd = wd\n        # Useful for Adam\n        self.i = 1\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params:\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i += 1\n\n    def reg_step(self, p):\n        if self.wd != 0: p *= (1 - self.wd * self.lr)\n\n    def opt_step(self, p):\n        p -= p.grad * self.lr\n\n    def zero_grad(self):\n        for p in self.params:\n            p.grad.data.zero_()\n\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=0.4, cbs=cbs, opt_func=SGD)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.758\n0.669\n0\nTrue\n00:18\n\n\n0.826\n0.473\n0\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, SGD is the one we have been using so far. Before we jump into other concepts, I want to examine how gradient changes in each strategy. So, we can use the backward hook to graph means and standard deviations like we did with weights.\n\ngrad_means = [[] for _ in model]\ngrad_stds = [[] for _ in model]\n\n\ndef append_grads(i, module, grad_input, grad_output):\n    if grad_input[0] is not None:\n        acts = to_cpu(grad_input[0])\n        grad_means[i].append(acts.mean())\n        grad_stds[i].append(acts.std())\n\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\n\n\nfor i, m in enumerate(model): m.register_full_backward_hook(partial(append_grads, i))\n\n\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=0.4, cbs=cbs, opt_func=SGD)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.758\n0.669\n0\nTrue\n00:12\n\n\n0.822\n0.482\n0\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor o in grad_means: plt.plot(o)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\nLet’s create a BackwardHook class to keep the stats in each hook.\n\nclass BackwardHook:\n    def __init__(self, m, f): self.hook = m.register_full_backward_hook(partial(f, self))\n    def remove(self): self.hook.remove()\n    def __del__(self): self.remove()\n\n\ndef append_grads(hook, module, grad_input, grad_output):\n    if not hasattr(hook, 'stats'): hook.stats = [[], []]\n    if grad_input[0] is not None:\n        acts = to_cpu(grad_input[0])\n        hook.stats[0].append(acts.mean())\n        hook.stats[1].append(acts.std())\n\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\n\n\nhooks = [BackwardHook(m, append_grads) for m in model[:5].children()]\n\n\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=0.4, cbs=cbs, opt_func=SGD)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.757\n0.669\n0\nTrue\n00:12\n\n\n0.824\n0.477\n0\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\nfor h in hooks[:-1]:\n    for i in 0,1: axs[i].plot(h.stats[i])\n    h.remove()\nplt.legend(range(4));\n\n\n\n\n\n\n\n\nOkay. We can compare this to others later. Because the last hook changed way more than others, we will only look at others. For the averages, notice y values are scaled with 1e-7.",
    "crumbs": [
      "Home",
      "Blog",
      "Optimizer"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-06-Optimizer/index.html#weight-decay",
    "href": "blog/posts/2023-11-06-Optimizer/index.html#weight-decay",
    "title": "Optimizer",
    "section": "Weight decay",
    "text": "Weight decay\nIn SGD, we had the option to use weight decay. Weight decay is a regularization method. This keeps the absolute value of weights small. When those values are large, models get complex and are more challenging to generalize. So, let’s see how weight decay works.\n\n\n\n\n\n\nTipWhat is regularization?\n\n\n\nRegularization is a way to interfere with the training of the model. Why do we want the model not to train well? Training well was our goal so far. That’s why we explored various initialization techniques and implemented layer norm and batch norm.\nHowever, our model trains very well and is starting to overfit. What does this mean? Let’s say we created flashcards to memorize for an upcoming Spanish test. Then, we study cards in order without shuffling. Because we want to maximize the score with minimal effort, we will memorize the order of words. No matter how much we study and get good score on practice quiz, we will do bad on the test. Similar thing is going on with the neural net. It is taking short cut to get a good score on the loss function. So we force our model to learn generally.\n\n\nThis is the definition of weight decay according to chapter 5 of deep learning by Ian Goodfellow et al.\n\\[ L \\leftarrow L + (w^Tw) * wd\\]\nwhere \\(L\\) is the loss, \\(w\\) is weight, and \\(wd\\) is weight decay. Usually, weight decay is a small value. Let’s see how we can simplify this equation with code.\nIf we simplify \\(w^Tw\\) into \\(w^2\\), we get:\n\\[ L \\leftarrow L + (\\sum{w^2}) * wd \\]\nIn code, it is this:\nL = L + sum(w**2) * wd\nIf we take the derivative of the right side, we have:\n\\[ \\frac{\\partial L}{\\partial w} + (\\sum{2w}) * wd \\]\nw.grad = w.grad + 2*sum(w) * wd\nFrom this, we can incorporate 2 into \\(wd\\). So, we are left with this:\n\\[ \\frac{\\partial L}{\\partial w} + (\\sum{w}) * wd \\]\nw.grad = w.grad + sum(w) * wd\nFrom the FastAI course, we can see the weight decay and L2 regularization in code, which does the same thing in this case:\nweight -= lr*wd*weight\n…vs…\nweight.grad += wd*weight\nWhen I first looked at this, I was not convinced that these represented the same thing. If you are like me, let’s look at these in a different angle. So, this is how we update the weights without weight decay:\n\\[w \\leftarrow w-lr\\frac{\\partial L}{\\partial w} \\tag{1}\\]\nwhere \\(w\\) is weight, \\(lr\\) is learning rate, \\(L\\) is a loss function, and \\(wd\\) is weight decay.\nw -= lr*w.grad\nwhere w.grad is a gradient of loss repect to weights.\nAnd this is how we add weight decay:\n\\[ w \\leftarrow w-lr\\frac{\\partial L}{\\partial w}-lr*wd*w \\tag{2} \\]\nw -= lr*w.grad - lr*wd*w\nWe can also write it this way:\n\\[ w \\leftarrow w-lr(\\frac{\\partial L}{\\partial w}+wd*w) \\tag{3} \\]\nw -= lr*(w.grad + wd*w)\nWhich is the same thing as:\nw.grad += wd*w\nw -= lr*w.grad\nNow, it is easier to see that they are doing the same thing here. So, first one from fastai is the same thing as the equation (2) without the gradient part. And the second one is the same thing as the equation (3) inside of the parenthesis. This part multiplied by learning rate gets subtracted from the weights, which is the same thing as bigger gradient.\n\n\n\n\n\n\nImportant\n\n\n\nAlthoug they are the same thing in standard stochastic gradient descent, they are different in adaptive stochastic gradient descent, like Adam according to DECOUPLED WEIGHT DECAY REGULARIZATION by Ilya Loshchilov & Frank Hutter.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMore on weight decay from Lesson 7 of the FastAI course.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s very important to remember that weight decay cannot be used with batch norm. Even if weight decay regularizes the growth of weights, batch norm can make them bigger by multiplying weights by its learned parameters.",
    "crumbs": [
      "Home",
      "Blog",
      "Optimizer"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-06-Optimizer/index.html#rmsprop",
    "href": "blog/posts/2023-11-06-Optimizer/index.html#rmsprop",
    "title": "Optimizer",
    "section": "RMSprop",
    "text": "RMSprop\nWe can improve stochastic gradient descent to take bigger steps when possible. Therefore, we can get better results faster with RMSprop. RMSprop came from lecture by G. Hinton. How does it know when it can take bigger steps? So, RMSprop uses the gradient squared, which is a variance (how spread out the gradient is). If the variance is considerable, there are multiple ways to reduce the loss, and it needs to be clarified which way it should go. Therefore, it should not step too much. On the other hand, if the variance is tiny, there is a clear direction on where to step. So, it should take a big step to reduce the loss quickly.\nBy dividing the gradient of loss by its variance, RMSprop figures out how big it can step. Just like Batch norm, it uses epsiolon just in case the variance is too small.\n\nclass RMSprop(SGD):\n    def __init__(self, params, lr, wd=0., sqr_mom=0.99, eps=1e-5):\n        super().__init__(params, lr, wd)\n        self.sqr_mom = sqr_mom\n        self.eps = eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'sqr_avg'): p.sqr_avg = p.grad**2\n        p.sqr_avg = p.sqr_avg*self.sqr_mom + (1-self.sqr_mom)*p.grad**2\n        p -= self.lr * p.grad / (p.sqr_avg.sqrt() + self.eps)\n\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=RMSprop)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.766\n0.660\n0\nTrue\n00:10\n\n\n0.829\n0.481\n0\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\nhooks = [BackwardHook(m, append_grads) for m in model[:5].children()]\n\n\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=3e-3, cbs=cbs, opt_func=RMSprop)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.765\n0.662\n0\nTrue\n00:12\n\n\n0.836\n0.470\n0\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10,4))\nfor h in hooks[:-1]:\n    for i in 0, 1: axs[i].plot(h.stats[i])\n    h.remove()\nplt.legend(range(4));\n\n\n\n\n\n\n\n\nCompared to SGD, this is scaled with 1e-8, so this is more stable. This explains why it works better than SGD.",
    "crumbs": [
      "Home",
      "Blog",
      "Optimizer"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-06-Optimizer/index.html#adam",
    "href": "blog/posts/2023-11-06-Optimizer/index.html#adam",
    "title": "Optimizer",
    "section": "Adam",
    "text": "Adam\nAdam is one of the most widely used optimizers. We can think of this as RMSprop with momentum. The paper has a lot of math, but let’s focus on intuition here. If we convert the pseudo code from Algorithm 1 into Python, we get something like this:\n\nclass Adam(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr, wd)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n        for p in self.params:                                       # Initialize with zeros\n            p.m = torch.zeros_like(p.data)\n            p.v = torch.zeros_like(p.data)\n\n    def opt_step(self, p):\n        p.m = self.beta1 * p.m + (1 - self.beta1) * p.grad.data     # Update biased first moment estimate\n        p.v = self.beta2 * p.v + (1 - self.beta2) * p.grad.data**2  # Update biased second raw moment estimate\n        m_hat = p.m / (1 - self.beta1**self.i)                      # Compute bias-corrected first moment estimate\n        v_hat = p.v / (1 - self.beta2**self.i)                      # Compute bias-corrected second moment estimate\n        p -= self.lr * m_hat / (v_hat.sqrt() + self.eps)            # Update parameters\n\nIn this algorithm, we have exponential moving averages of the gradient (p.m) and the squared gradient (p.v). They are initialized to zeros at first, so they are biased toward zeros. To correct the bias, we make them bigger by dividing by 1-self.beta1**self.i. Because it increments self.i every batch, this becomes one later in the training process. And we are left with m_hat = p.m. So, we divide the gradients by itself. Let’s see what this means by looking at t1.\n\nt1 = tensor([-100, -1, -0.01, 0.01, 1, 100])\nt1\n\ntensor([  -100.00,     -1.00,     -0.01,      0.01,      1.00,    100.00])\n\n\n\nt1 / (t1**2).sqrt()\n\ntensor([-1., -1., -1.,  1.,  1.,  1.])\n\n\nSo, whether the gradients are big or small, they will be -1 or 1. However, we use exponentially moving averages, and beta1 and beta2 have different momentums. Because beta2 has a more significant value than beta1, the second moment (variance) changes slowly. When the training starts, the gradients and the gradient squares will be small. As the model figures out where to go, the gradient will get bigger, but the gradient square will be smaller. This means that it will update parameters with bigger numbers. On the other hand, even if the gradient gets smaller, the gradient squared will be bigger, and it will update parameters with smaller numbers.\nAs a result, Adam updates parameters faster in the right directions. This explains why it performs better than others.\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=Adam)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.802\n0.558\n0\nTrue\n00:11\n\n\n0.859\n0.395\n0\nFalse\n00:01\n\n\n0.877\n0.337\n1\nTrue\n00:11\n\n\n0.874\n0.340\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\nhooks = [BackwardHook(m, append_grads) for m in model[:5].children()]\n\n\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=Adam)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.803\n0.557\n0\nTrue\n00:12\n\n\n0.856\n0.395\n0\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10,4))\nfor h in hooks[:-1]:\n    for i in 0, 1: axs[i].plot(h.stats[i])\n    h.remove()\nplt.legend(range(5));\n\n\n\n\n\n\n\n\nLike RMSprop, Adam’s gradients are scaled with 1e-8. However, there are less ups and downs in this graph.\nJust like batch norm and layer norm, we could try absolute value instead of squaring and squaring rooting.\n\nclass Adam2(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr, wd)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n        for p in self.params:\n            p.m = torch.zeros_like(p.data)\n            p.v = torch.zeros_like(p.data)\n\n    def opt_step(self, p):\n        p.m = self.beta1 * p.m + (1 - self.beta1) * p.grad.data       # Update biased first moment estimate\n        p.v = self.beta2 * p.v + (1 - self.beta2) * p.grad.data.abs() # Update biased second raw moment estimate\n        m_hat = p.m / (1 - self.beta1**self.i)                        # Compute bias-corrected first moment estimate\n        v_hat = p.v / (1 - self.beta2**self.i)                        # Compute bias-corrected second moment estimate\n        p -= self.lr * m_hat / (v_hat + self.eps)                     # Update parameters\n\n\nset_seed(42)\nmodel = get_model(act=act_gr, norm=nn.BatchNorm2d)\nlearn = TrainLearner(model, dls, loss_func=F.cross_entropy, lr=6e-3, cbs=cbs, opt_func=partial(Adam2, beta2=0.9, eps=1e-8))\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.804\n0.554\n0\nTrue\n00:11\n\n\n0.854\n0.407\n0\nFalse\n00:01\n\n\n0.876\n0.336\n1\nTrue\n00:11\n\n\n0.876\n0.339\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nIt was a bit better this time.",
    "crumbs": [
      "Home",
      "Blog",
      "Optimizer"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-06-Optimizer/index.html#conclusion",
    "href": "blog/posts/2023-11-06-Optimizer/index.html#conclusion",
    "title": "Optimizer",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog, we talked about optimization using SGD, RMSprop, and Adam. We also talked about weight decay. We also looked at gradients of the loss for each layer, and how each optimization techniques varied. Next blog, we will go over schedulers to change learning rates.",
    "crumbs": [
      "Home",
      "Blog",
      "Optimizer"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-28-kv_cache/index.html",
    "href": "blog/posts/2025-09-28-kv_cache/index.html",
    "title": "HuggingFace KV cache",
    "section": "",
    "text": "In this notebook, we will learn about what KV cache is and try out different kinds of HuggingFace KV cache, such as dynamic and static. We will only look at decoder models. To follow along, use colab notebook.\n\n\nTo understand what KV cache is and why it is useful, we have to understand what the transformer does in a large languge model. Transformer block consists of self attention and feed forward network.\nLet’s take a look at the transformer in a big picture. In decoder models, the goal is to predict the next word (or a token). The self attention highlights which words are important by doing weighted sum. For instance, we have a sentence, “I was thirsty, so I”, and we want to predict the next word. It makes sense to pay attention to the word, “thirsty”. That’s what the attention does. And the feed forward network figures out the next words with these clues.\n\n\n\nkv_cache\n\n\nInside of the self attention, we calculate query, key, and value for each token. So, from the example sentence we had, “I was thirsty,”, each word has a query, key, and value.\n\nQuery tells which word to predict. If we use query for “thirsty,”, it is like asking what is the next word after “thirsty”? It is easy to think of a query as a one word. If we used a differnt word, like “I”, it is like asking what comes after “I”?\nKeys tells which words to focus more respect to query. We can think of keys as a sentence with words “thisty” and other words that come before it, “I was thirsty,”. To predict the next word after “thirsty”, it is important to pay attention to say “I”. With query and keys, we gained information about which words to focus.\nValue provides an updated sentence with highlighted words. Using query and keys, we know which ones to focus from the sentence, but we need to apply this information into the sentence itself. Think of values as the whole sentence. And with query + keys + values, we have a highlighted sentence.\n\nLet’s assume the model generated “so” as an output.\nYou see how we needed a query for one word we want to make prediction of, but needed keys and values for the whole sentence? For instance, we fed the model with “I was thirsty,” and its output was “so”. To generate “so”, it calculated keys and values for “I was thirsty,”. Now we have “I was thirsty, so” as an input to the model. The model needs “so” as a query, “I was thirsty, so” as keys and values to predict the next word. But we calculated keys and values for “I was thirsty,” already. It would be a waste to calculate it again (which grows quadratically).\nBy saving the keys and values for the previous words, we can compute the keys and values with linear time complexity. However, it consumes more memory because we have to save those values.This is KV cache.\nThere are two ways to use KV cache: dynamic and static. In dynamic cache, cache grows with each generation. However, static cache has a fixed cache.\nI didn’t go into details when explaining the transformer in LLMs. If you would like to learn more, I included resources to learn more about them in the conclusion section at the end.\nAlso, Hugging Face supports more sophisticated techniques such as Cache offloading, Quantized cache, Encoder-decoder cache, and Model-specific cache. Please take a look at KV cache strategies from HuggingFace documentations for more info.\n\n\n\nNow, let’s dive into the code. We first import libraries and setup tools.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, DynamicCache, StaticCache\nimport torch\nimport torch.nn as nn\nimport time, os\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\ntorch.set_float32_matmul_precision('high')\n\nnum_new_tokens = 200\n\n\ndef tks(model, input_ids, num_new_tokens=200, **kwargs):\n    \"Generate text and print time and tokens/sec\"\n    start_time = time.time()\n    outputs = model.generate(**input_ids, max_new_tokens=num_new_tokens, **kwargs)\n    total_time = time.time() - start_time\n\n    print(f\"\\nTime: {total_time:.2f} sec\")\n    print(f\"{int(num_new_tokens/total_time)} tokens/sec\")\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n\n\ndef bytes_to_giga_bytes(bytes): return f'{bytes / 1024 / 1024 / 1024:.2f} GB'\n\nKV cache speeds up the inference. HuggingFace offers dynamic cache and static cache.\nHere’s more info on Caching and KV cache strategies from HuggingFace.\nI conducted some experiment with time and memory.Results cache runs with generating 200 tokens on SmolLM2-135M:\n\n\n\n\n\n\n\n\n\n\nCache\nHardware\nTime (s)\nTokens per sec\nNote\n\n\n\n\nNone\nL4\n6\n32\n\n\n\nDynamic\nL4\n6\n32\n\n\n\nStatic\nL4\n1\n153\n\n\n\nNone\nT4\n9\n21\n\n\n\nDynamic\nT4\n7.44\n26\n\n\n\nStatic\nT4\n9.35\n21\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n\n\nNone\nA100\n7.40\n27\n\n\n\nDynamic\nA100\n6.30\n31\n\n\n\nStatic\nA100\n1.39\n143\n\n\n\n\n\n\nHuggingFace models use dynamic cache by default. By setting use_cache=False, we can generate without cache.\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig(use_cache=False)\nmodel.generation_config\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerationConfig {\n  \"use_cache\": false\n}\n\n\nTo change the behavior of generation, we can either use model.generation_config with GenerationConfig or pass keywords to model.generate. Let’s use the GenerationConfig.\n\ninput_text = \"The theory of special relativity states \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\ntks(model, input_ids)\n\n\nTime: 8.73 sec\n22 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light\n\n\nUsing the keyword use_cache=False to model.generate.\n\ntks(model, input_ids, use_cache=False)\n\n\nTime: 8.03 sec\n24 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light\n\n\n\n\n\nThis is the default cache for HuggingFace models. KV cache is dynamically added each time in the loop. Therefore, this cannot be used with torch.compile.\n\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\ntks(model, input_ids)\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n\nTime: 7.94 sec\n25 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light\n\n\n\n\n\nStatic cache initializes a bulk of memory for kv cache. It takes more memory than dynamic cache. However, it can be compiled. In my experience, static cache was slower than dynamic cache on CPU and T4, but faster on L4 and A100.\n\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config\n\nGenerationConfig {\n  \"bos_token_id\": 0,\n  \"cache_implementation\": \"static\",\n  \"eos_token_id\": 0\n}\n\n\nmodel.forward is compiled automatically with static cache. But it is also possible to compile with differen options.\n\n# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\")\n# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nAfter compiling, first run takes a long time.\n\ntks(model, input_ids)\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\nW0928 19:52:23.353000 791 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n\n\n\nTime: 124.21 sec\n1 tokens/sec\nThe theory of special relativity states 2 things:\n\n(1) The speed of light is constant in all inertial frames of reference.\n\n(2) The speed of light is the same in all inertial frames of reference.\n\nThe first statement is called the Lorentz transformation. The second is called the Lorentz contraction.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorent\n\n\n\ntks(model, input_ids)\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n\nTime: 9.15 sec\n21 tokens/sec\nThe theory of special relativity states 2 things:\n\n(1) The speed of light is constant in all inertial frames of reference.\n\n(2) The speed of light is the same in all inertial frames of reference.\n\nThe first statement is called the Lorentz transformation. The second is called the Lorentz contraction.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorent\n\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/v4.56.2/llm_tutorial_optimization#32-the-key-value-cache\n\nWe looked at how different cache\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\n\ninput_text = \"The theory of special relativity states \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n\nstart_time = time.time()\noutputs = model.generate(**input_ids, max_length=num_new_tokens)\ntotal_time = time.time() - start_time\n\nprint(f\"\\nTime: {total_time:.2f} sec\")\nprint(f\"{int(num_new_tokens/total_time)} tokens/sec\")\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n\nTime: 7.69 sec\n26 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames\n\n\n\ndisplay(model.config._attn_implementation)\n\n'sdpa'\n\n\n\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n\n'12.07 GB'\n\n\n\n\n\n\nBefore looking into the cache, let’s look at the input_ids without cache. input_ids have shape [batch_size, sequence_len].\n\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\ninput_ids, input_ids.shape\n\n(tensor([[  504,  3108,   282,  1767, 24581,  2496,   216]], device='cuda:0'),\n torch.Size([1, 7]))\n\n\n\nfor _ in range(5):\n    next_logits = model(input_ids, use_cache=False)[\"logits\"][:, -1:]\n    next_token_id = torch.argmax(next_logits,dim=-1)\n    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n    print(\"shape of input_ids\", input_ids.shape)\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 8])\nshape of input_ids torch.Size([1, 9])\nshape of input_ids torch.Size([1, 10])\nshape of input_ids torch.Size([1, 11])\nshape of input_ids torch.Size([1, 12])\nGenerated text: \n200 years ago\n\n\nNotice how the second dimension (sequence length) grows each iteration.\n\n\n\nAnd this is what the input_ids look like with dynamic cache.\n\npast_key_values = None # past_key_values is the key-value cache\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\n    next_logits = next_logits[:, -1:]\n    next_token_id = torch.argmax(next_logits, dim=-1)\n\n    print(\"shape of input_ids\", next_token_id.shape)\n    print(\"shape of key-value cache\", past_key_values[0][0].shape)  # shape of [batch_size, num_key_value_heads, sequence_length, head_dim]\n    generated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 7, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 8, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 9, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 10, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 11, 64])\nGenerated text: \n200 years ago\n\n\nNotice how we are only feeding one token at a time (input_ids has the shape of [1, 1] with [batch_size, sequence_length]), instead of the whole previous sequence. But we can see that the kv cache is growing dynamically with each iteration. The dyanmic cache has the shape of [batch_size, num_key_value_heads, sequence_length, head_dim], and the sequence_length dimension is growing.\nIf we look at the code in detail, model returns the next_logits and past_key_values.\n\npast_key_values = None # past_key_values is the key-value cache\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nnext_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\nnext_logits, next_logits.shape\n\n(tensor([[[12.4375,  1.0859,  1.1953,  ..., 10.2500, 11.6875,  8.3125],\n          [17.2500,  1.7422,  1.8438,  ..., 11.8125, 14.0000,  7.4688],\n          [ 4.8438, -8.8750, -8.8125,  ...,  0.0732,  3.1406, -1.2812],\n          ...,\n          [20.3750,  3.3125,  3.3906,  ..., 13.4375, 15.0625,  8.3125],\n          [15.0000, -1.4297, -1.3281,  ...,  3.3594,  9.3125,  7.4688],\n          [18.8750,  3.3906,  3.4219,  ...,  4.3750, 11.4375, 10.3750]]],\n        device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;UnsafeViewBackward0&gt;),\n torch.Size([1, 7, 49152]))\n\n\nnext_logits has shape [batch_size, sequence_length, vocab_size].\nIf we take the softmax from next_logits for the last token on the last dimension (vocab_size), we get the probability distribution for the next word.\n\ntorch.softmax(next_logits[:, -1, :], dim=-1)\n\ntensor([[6.4850e-05, 1.2221e-11, 1.2619e-11,  ..., 3.2742e-11, 3.8184e-08,\n         1.3213e-08]], device='cuda:0', dtype=torch.bfloat16,\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\nHowever, we are using greedy decoding, and we only care about the most likely next token. Therefore, we just pick the one with the highest value using torch.argmax.\n\nnext_logits = next_logits[:, -1:]\nnext_logits.shape\n\ntorch.Size([1, 1, 49152])\n\n\n\nnext_token_id = torch.argmax(next_logits, dim=-1)\nnext_token_id\n\ntensor([[34]], device='cuda:0')\n\n\nThe model also returned past_key_values.\n\npast_key_values\n\nDynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])\n\n\n\nlen(past_key_values)\n\n30\n\n\n\npast_key_values[0][0].shape, past_key_values[0][1].shape\n\n(torch.Size([1, 3, 7, 64]), torch.Size([1, 3, 7, 64]))\n\n\n\npast_key_values[20][0].shape, past_key_values[20][1].shape\n\n(torch.Size([1, 3, 7, 64]), torch.Size([1, 3, 7, 64]))\n\n\nIn the model.config, we have 30 num_hidden_layers, and 64 head_dim, 3 num_key_value_heads. And past_key_values has 30 DynamicLayer, and each layer contains a tuple of key and value cache ([0] for key and [1] for values). Each of those cache has a shape of [batch_size, num_key_value_heads, sequence_length, head_dim].\n\nmodel.config\n\nLlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 0,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 576,\n  \"initializer_range\": 0.041666666666666664,\n  \"intermediate_size\": 1536,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 9,\n  \"num_hidden_layers\": 30,\n  \"num_key_value_heads\": 3,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\n\nIf we keep running the model with cache, the sequence_length dimension gets concatenated one by one.\n\nnext_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\n\n\npast_key_values[0][0].shape, past_key_values[0][1].shape\n\n(torch.Size([1, 3, 8, 64]), torch.Size([1, 3, 8, 64]))\n\n\n\nprint(\"shape of input_ids\", next_token_id.shape)\nprint(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, head_dim]\ngenerated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 1\nGenerated text: \n200 years ago\n\n\nAnother way to use HuggingFace Cache is pass cache into the model directly. Here, we create DynamicCache and pass it to the model.\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\npast_key_values = DynamicCache(config=model.config)\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values).to_tuple()\n    next_logits = next_logits[:, -1:]\n    next_token_id = torch.argmax(next_logits, dim=-1)\n\n    print(\"shape of input_ids\", next_token_id.shape)\n    print(\"shape of key-value cache\", past_key_values[0][0].shape)\n    generated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(generated_tokens[-5:], skip_special_tokens=True)}')\n\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 7, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 8, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 9, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 10, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 11, 64])\nGenerated text: \n['2', '0', '0', ' years', ' ago']\n\n\n\npast_key_values[0][0].shape\n\ntorch.Size([1, 3, 11, 64])\n\n\n\n\n\n\nIf we pass StaticCache into the model, it uses static cache. As we can see, the cache is allocated with length 1024, and it stays the same.\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\npast_key_values = StaticCache(config=model.config, max_cache_len=1024)\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values).to_tuple()\n    next_logits = next_logits[:, -1:]\n    next_token_id = torch.argmax(next_logits, dim=-1)\n\n    print(\"shape of input_ids\", next_token_id.shape)\n    print(\"shape of key-value cache\", past_key_values[0][0].shape)\n    generated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nGenerated text: \n200 years ago\n\n\nStatic cache also has the shape as the Dynamic cache with [layer, key, batch, num_head].\n\npast_key_values[0][0][0][0]\n\ntensor([[-1.2188e+00,  1.6504e-01, -2.0801e-01,  ..., -3.4766e-01,\n          8.0859e-01, -2.5781e-01],\n        [-3.0469e-01,  2.8711e-01, -4.0625e-01,  ..., -7.8906e-01,\n          1.2578e+00, -7.7637e-02],\n        [ 1.0156e+00, -5.7422e-01, -1.1035e-01,  ...,  1.5000e+00,\n         -2.5625e+00,  2.5177e-03],\n        ...,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.bfloat16,\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\npast_key_values[0][0][0][0][:,0], past_key_values[0][0][0][0][:,0].shape\n\n(tensor([-1.2188, -0.3047,  1.0156,  ...,  0.0000,  0.0000,  0.0000],\n        device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;SelectBackward0&gt;),\n torch.Size([1024]))\n\n\nLooking at the first number from the first 20 sequence_length from the first layer, key, first batch, and the first head. Very mouthful lol\n\npast_key_values[0][0][0][0][:20,0]\n\ntensor([-1.2188, -0.3047,  1.0156,  1.7266,  1.4453, -1.1641,  0.6797, -1.3281,\n         0.9688,  0.4980, -1.3047,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n       dtype=torch.bfloat16, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ninput_ids\n\ntensor([[  504,  3108,   282,  1767, 24581,  2496,   216,    34,    32,    32,\n           929,  3156]], device='cuda:0')\n\n\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, :], skip_special_tokens=True)[0]}')\n\nGenerated text: \nThe theory of special relativity states 200 years ago\n\n\n\n\n\n\nWe looked at what KV cache is, why it is helpful, and how to use it with Hugging Face models. It saves keys and values in memory so the model does not have to calculate over and over again for each token.\nWe only looked at decoder models, but Hugging Face has other models as well. Hugging Face also supports other kinds of cache, such as Cache offloading, Quantized cache, Encoder-decoder cache, and Model-specific cache. But basic idea is the same.\nTo learn more about thse, check KV cache strategies from Hugging Face documentation.\nI didn’t go into details when explaining the transformer in LLMs. If you would like to learn more, you can learn more in a youtube video, Transformers, the tech behind LLMs | Deep Learning Chapter 5 by 3Blue1Brown and illustraed-gpt2 by Jay Lammar. With cool visualizations, it is very easy to grasp concepts.\nIf you want to learn more in detail with code, I recommend Let’s build GPT: from scratch, in code, spelled out. by Andrej Karpathy and LLMs-from-scratch by Sebastian Raschka. Karpathy makes good videos with code. I also recommend watching Let’s reproduce GPT-2 (124M) which includes awesome details about how to pretrain gpt2. LLMs-from-scratch is also great for learning LLMs as it starts from gpt2. I really love this repo as it includes more advanced concepts not used in gpt2, such as kv cache, group query attention, rotary embeddings, llama and qwen architectures, and so on. The advanced concepts are not explained in as much in depth as gpt2, but very awesome.",
    "crumbs": [
      "Home",
      "Blog",
      "HuggingFace KV cache"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-28-kv_cache/index.html#brief-look-at-the-transformer",
    "href": "blog/posts/2025-09-28-kv_cache/index.html#brief-look-at-the-transformer",
    "title": "HuggingFace KV cache",
    "section": "",
    "text": "To understand what KV cache is and why it is useful, we have to understand what the transformer does in a large languge model. Transformer block consists of self attention and feed forward network.\nLet’s take a look at the transformer in a big picture. In decoder models, the goal is to predict the next word (or a token). The self attention highlights which words are important by doing weighted sum. For instance, we have a sentence, “I was thirsty, so I”, and we want to predict the next word. It makes sense to pay attention to the word, “thirsty”. That’s what the attention does. And the feed forward network figures out the next words with these clues.\n\n\n\nkv_cache\n\n\nInside of the self attention, we calculate query, key, and value for each token. So, from the example sentence we had, “I was thirsty,”, each word has a query, key, and value.\n\nQuery tells which word to predict. If we use query for “thirsty,”, it is like asking what is the next word after “thirsty”? It is easy to think of a query as a one word. If we used a differnt word, like “I”, it is like asking what comes after “I”?\nKeys tells which words to focus more respect to query. We can think of keys as a sentence with words “thisty” and other words that come before it, “I was thirsty,”. To predict the next word after “thirsty”, it is important to pay attention to say “I”. With query and keys, we gained information about which words to focus.\nValue provides an updated sentence with highlighted words. Using query and keys, we know which ones to focus from the sentence, but we need to apply this information into the sentence itself. Think of values as the whole sentence. And with query + keys + values, we have a highlighted sentence.\n\nLet’s assume the model generated “so” as an output.\nYou see how we needed a query for one word we want to make prediction of, but needed keys and values for the whole sentence? For instance, we fed the model with “I was thirsty,” and its output was “so”. To generate “so”, it calculated keys and values for “I was thirsty,”. Now we have “I was thirsty, so” as an input to the model. The model needs “so” as a query, “I was thirsty, so” as keys and values to predict the next word. But we calculated keys and values for “I was thirsty,” already. It would be a waste to calculate it again (which grows quadratically).\nBy saving the keys and values for the previous words, we can compute the keys and values with linear time complexity. However, it consumes more memory because we have to save those values.This is KV cache.\nThere are two ways to use KV cache: dynamic and static. In dynamic cache, cache grows with each generation. However, static cache has a fixed cache.\nI didn’t go into details when explaining the transformer in LLMs. If you would like to learn more, I included resources to learn more about them in the conclusion section at the end.\nAlso, Hugging Face supports more sophisticated techniques such as Cache offloading, Quantized cache, Encoder-decoder cache, and Model-specific cache. Please take a look at KV cache strategies from HuggingFace documentations for more info.",
    "crumbs": [
      "Home",
      "Blog",
      "HuggingFace KV cache"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-28-kv_cache/index.html#kv-cache",
    "href": "blog/posts/2025-09-28-kv_cache/index.html#kv-cache",
    "title": "HuggingFace KV cache",
    "section": "",
    "text": "Now, let’s dive into the code. We first import libraries and setup tools.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, DynamicCache, StaticCache\nimport torch\nimport torch.nn as nn\nimport time, os\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\ntorch.set_float32_matmul_precision('high')\n\nnum_new_tokens = 200\n\n\ndef tks(model, input_ids, num_new_tokens=200, **kwargs):\n    \"Generate text and print time and tokens/sec\"\n    start_time = time.time()\n    outputs = model.generate(**input_ids, max_new_tokens=num_new_tokens, **kwargs)\n    total_time = time.time() - start_time\n\n    print(f\"\\nTime: {total_time:.2f} sec\")\n    print(f\"{int(num_new_tokens/total_time)} tokens/sec\")\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n\n\ndef bytes_to_giga_bytes(bytes): return f'{bytes / 1024 / 1024 / 1024:.2f} GB'\n\nKV cache speeds up the inference. HuggingFace offers dynamic cache and static cache.\nHere’s more info on Caching and KV cache strategies from HuggingFace.\nI conducted some experiment with time and memory.Results cache runs with generating 200 tokens on SmolLM2-135M:\n\n\n\n\n\n\n\n\n\n\nCache\nHardware\nTime (s)\nTokens per sec\nNote\n\n\n\n\nNone\nL4\n6\n32\n\n\n\nDynamic\nL4\n6\n32\n\n\n\nStatic\nL4\n1\n153\n\n\n\nNone\nT4\n9\n21\n\n\n\nDynamic\nT4\n7.44\n26\n\n\n\nStatic\nT4\n9.35\n21\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n\n\nNone\nA100\n7.40\n27\n\n\n\nDynamic\nA100\n6.30\n31\n\n\n\nStatic\nA100\n1.39\n143\n\n\n\n\n\n\nHuggingFace models use dynamic cache by default. By setting use_cache=False, we can generate without cache.\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig(use_cache=False)\nmodel.generation_config\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerationConfig {\n  \"use_cache\": false\n}\n\n\nTo change the behavior of generation, we can either use model.generation_config with GenerationConfig or pass keywords to model.generate. Let’s use the GenerationConfig.\n\ninput_text = \"The theory of special relativity states \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\ntks(model, input_ids)\n\n\nTime: 8.73 sec\n22 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light\n\n\nUsing the keyword use_cache=False to model.generate.\n\ntks(model, input_ids, use_cache=False)\n\n\nTime: 8.03 sec\n24 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light\n\n\n\n\n\nThis is the default cache for HuggingFace models. KV cache is dynamically added each time in the loop. Therefore, this cannot be used with torch.compile.\n\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\ntks(model, input_ids)\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n\nTime: 7.94 sec\n25 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light\n\n\n\n\n\nStatic cache initializes a bulk of memory for kv cache. It takes more memory than dynamic cache. However, it can be compiled. In my experience, static cache was slower than dynamic cache on CPU and T4, but faster on L4 and A100.\n\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config\n\nGenerationConfig {\n  \"bos_token_id\": 0,\n  \"cache_implementation\": \"static\",\n  \"eos_token_id\": 0\n}\n\n\nmodel.forward is compiled automatically with static cache. But it is also possible to compile with differen options.\n\n# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\")\n# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nAfter compiling, first run takes a long time.\n\ntks(model, input_ids)\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\nW0928 19:52:23.353000 791 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n\n\n\nTime: 124.21 sec\n1 tokens/sec\nThe theory of special relativity states 2 things:\n\n(1) The speed of light is constant in all inertial frames of reference.\n\n(2) The speed of light is the same in all inertial frames of reference.\n\nThe first statement is called the Lorentz transformation. The second is called the Lorentz contraction.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorent\n\n\n\ntks(model, input_ids)\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n\nTime: 9.15 sec\n21 tokens/sec\nThe theory of special relativity states 2 things:\n\n(1) The speed of light is constant in all inertial frames of reference.\n\n(2) The speed of light is the same in all inertial frames of reference.\n\nThe first statement is called the Lorentz transformation. The second is called the Lorentz contraction.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference. The Lorentz transformation is a mathematical transformation that changes the speed of light in a frame of reference.\n\nThe Lorent",
    "crumbs": [
      "Home",
      "Blog",
      "HuggingFace KV cache"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-28-kv_cache/index.html#digging-into-details",
    "href": "blog/posts/2025-09-28-kv_cache/index.html#digging-into-details",
    "title": "HuggingFace KV cache",
    "section": "",
    "text": "https://huggingface.co/docs/transformers/v4.56.2/llm_tutorial_optimization#32-the-key-value-cache\n\nWe looked at how different cache\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\n\ninput_text = \"The theory of special relativity states \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n\nstart_time = time.time()\noutputs = model.generate(**input_ids, max_length=num_new_tokens)\ntotal_time = time.time() - start_time\n\nprint(f\"\\nTime: {total_time:.2f} sec\")\nprint(f\"{int(num_new_tokens/total_time)} tokens/sec\")\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\n\nTime: 7.69 sec\n26 tokens/sec\nThe theory of special relativity states 200 years ago that the speed of light is constant in all inertial frames. This is a very important fact, because it means that the speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames.\n\nThe speed of light is the same in all inertial frames\n\n\n\ndisplay(model.config._attn_implementation)\n\n'sdpa'\n\n\n\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n\n'12.07 GB'\n\n\n\n\n\n\nBefore looking into the cache, let’s look at the input_ids without cache. input_ids have shape [batch_size, sequence_len].\n\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\ninput_ids, input_ids.shape\n\n(tensor([[  504,  3108,   282,  1767, 24581,  2496,   216]], device='cuda:0'),\n torch.Size([1, 7]))\n\n\n\nfor _ in range(5):\n    next_logits = model(input_ids, use_cache=False)[\"logits\"][:, -1:]\n    next_token_id = torch.argmax(next_logits,dim=-1)\n    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n    print(\"shape of input_ids\", input_ids.shape)\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 8])\nshape of input_ids torch.Size([1, 9])\nshape of input_ids torch.Size([1, 10])\nshape of input_ids torch.Size([1, 11])\nshape of input_ids torch.Size([1, 12])\nGenerated text: \n200 years ago\n\n\nNotice how the second dimension (sequence length) grows each iteration.\n\n\n\nAnd this is what the input_ids look like with dynamic cache.\n\npast_key_values = None # past_key_values is the key-value cache\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\n    next_logits = next_logits[:, -1:]\n    next_token_id = torch.argmax(next_logits, dim=-1)\n\n    print(\"shape of input_ids\", next_token_id.shape)\n    print(\"shape of key-value cache\", past_key_values[0][0].shape)  # shape of [batch_size, num_key_value_heads, sequence_length, head_dim]\n    generated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 7, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 8, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 9, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 10, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 11, 64])\nGenerated text: \n200 years ago\n\n\nNotice how we are only feeding one token at a time (input_ids has the shape of [1, 1] with [batch_size, sequence_length]), instead of the whole previous sequence. But we can see that the kv cache is growing dynamically with each iteration. The dyanmic cache has the shape of [batch_size, num_key_value_heads, sequence_length, head_dim], and the sequence_length dimension is growing.\nIf we look at the code in detail, model returns the next_logits and past_key_values.\n\npast_key_values = None # past_key_values is the key-value cache\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nnext_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\nnext_logits, next_logits.shape\n\n(tensor([[[12.4375,  1.0859,  1.1953,  ..., 10.2500, 11.6875,  8.3125],\n          [17.2500,  1.7422,  1.8438,  ..., 11.8125, 14.0000,  7.4688],\n          [ 4.8438, -8.8750, -8.8125,  ...,  0.0732,  3.1406, -1.2812],\n          ...,\n          [20.3750,  3.3125,  3.3906,  ..., 13.4375, 15.0625,  8.3125],\n          [15.0000, -1.4297, -1.3281,  ...,  3.3594,  9.3125,  7.4688],\n          [18.8750,  3.3906,  3.4219,  ...,  4.3750, 11.4375, 10.3750]]],\n        device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;UnsafeViewBackward0&gt;),\n torch.Size([1, 7, 49152]))\n\n\nnext_logits has shape [batch_size, sequence_length, vocab_size].\nIf we take the softmax from next_logits for the last token on the last dimension (vocab_size), we get the probability distribution for the next word.\n\ntorch.softmax(next_logits[:, -1, :], dim=-1)\n\ntensor([[6.4850e-05, 1.2221e-11, 1.2619e-11,  ..., 3.2742e-11, 3.8184e-08,\n         1.3213e-08]], device='cuda:0', dtype=torch.bfloat16,\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\nHowever, we are using greedy decoding, and we only care about the most likely next token. Therefore, we just pick the one with the highest value using torch.argmax.\n\nnext_logits = next_logits[:, -1:]\nnext_logits.shape\n\ntorch.Size([1, 1, 49152])\n\n\n\nnext_token_id = torch.argmax(next_logits, dim=-1)\nnext_token_id\n\ntensor([[34]], device='cuda:0')\n\n\nThe model also returned past_key_values.\n\npast_key_values\n\nDynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])\n\n\n\nlen(past_key_values)\n\n30\n\n\n\npast_key_values[0][0].shape, past_key_values[0][1].shape\n\n(torch.Size([1, 3, 7, 64]), torch.Size([1, 3, 7, 64]))\n\n\n\npast_key_values[20][0].shape, past_key_values[20][1].shape\n\n(torch.Size([1, 3, 7, 64]), torch.Size([1, 3, 7, 64]))\n\n\nIn the model.config, we have 30 num_hidden_layers, and 64 head_dim, 3 num_key_value_heads. And past_key_values has 30 DynamicLayer, and each layer contains a tuple of key and value cache ([0] for key and [1] for values). Each of those cache has a shape of [batch_size, num_key_value_heads, sequence_length, head_dim].\n\nmodel.config\n\nLlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 0,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 576,\n  \"initializer_range\": 0.041666666666666664,\n  \"intermediate_size\": 1536,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 9,\n  \"num_hidden_layers\": 30,\n  \"num_key_value_heads\": 3,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.56.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\n\nIf we keep running the model with cache, the sequence_length dimension gets concatenated one by one.\n\nnext_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\n\n\npast_key_values[0][0].shape, past_key_values[0][1].shape\n\n(torch.Size([1, 3, 8, 64]), torch.Size([1, 3, 8, 64]))\n\n\n\nprint(\"shape of input_ids\", next_token_id.shape)\nprint(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, head_dim]\ngenerated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 1\nGenerated text: \n200 years ago\n\n\nAnother way to use HuggingFace Cache is pass cache into the model directly. Here, we create DynamicCache and pass it to the model.\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\npast_key_values = DynamicCache(config=model.config)\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values).to_tuple()\n    next_logits = next_logits[:, -1:]\n    next_token_id = torch.argmax(next_logits, dim=-1)\n\n    print(\"shape of input_ids\", next_token_id.shape)\n    print(\"shape of key-value cache\", past_key_values[0][0].shape)\n    generated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(generated_tokens[-5:], skip_special_tokens=True)}')\n\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 7, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 8, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 9, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 10, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 11, 64])\nGenerated text: \n['2', '0', '0', ' years', ' ago']\n\n\n\npast_key_values[0][0].shape\n\ntorch.Size([1, 3, 11, 64])\n\n\n\n\n\n\nIf we pass StaticCache into the model, it uses static cache. As we can see, the cache is allocated with length 1024, and it stays the same.\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", dtype=torch.bfloat16, device_map=\"auto\")\n\npast_key_values = StaticCache(config=model.config, max_cache_len=1024)\ngenerated_tokens = []\nnext_token_id = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n    next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values).to_tuple()\n    next_logits = next_logits[:, -1:]\n    next_token_id = torch.argmax(next_logits, dim=-1)\n\n    print(\"shape of input_ids\", next_token_id.shape)\n    print(\"shape of key-value cache\", past_key_values[0][0].shape)\n    generated_tokens.append(next_token_id.item())\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, -5:], skip_special_tokens=True)[0]}')\n\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nshape of input_ids torch.Size([1, 1])\nshape of key-value cache torch.Size([1, 3, 1024, 64])\nGenerated text: \n200 years ago\n\n\nStatic cache also has the shape as the Dynamic cache with [layer, key, batch, num_head].\n\npast_key_values[0][0][0][0]\n\ntensor([[-1.2188e+00,  1.6504e-01, -2.0801e-01,  ..., -3.4766e-01,\n          8.0859e-01, -2.5781e-01],\n        [-3.0469e-01,  2.8711e-01, -4.0625e-01,  ..., -7.8906e-01,\n          1.2578e+00, -7.7637e-02],\n        [ 1.0156e+00, -5.7422e-01, -1.1035e-01,  ...,  1.5000e+00,\n         -2.5625e+00,  2.5177e-03],\n        ...,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.bfloat16,\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\npast_key_values[0][0][0][0][:,0], past_key_values[0][0][0][0][:,0].shape\n\n(tensor([-1.2188, -0.3047,  1.0156,  ...,  0.0000,  0.0000,  0.0000],\n        device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;SelectBackward0&gt;),\n torch.Size([1024]))\n\n\nLooking at the first number from the first 20 sequence_length from the first layer, key, first batch, and the first head. Very mouthful lol\n\npast_key_values[0][0][0][0][:20,0]\n\ntensor([-1.2188, -0.3047,  1.0156,  1.7266,  1.4453, -1.1641,  0.6797, -1.3281,\n         0.9688,  0.4980, -1.3047,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n       dtype=torch.bfloat16, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ninput_ids\n\ntensor([[  504,  3108,   282,  1767, 24581,  2496,   216,    34,    32,    32,\n           929,  3156]], device='cuda:0')\n\n\n\nprint(f'Generated text: \\n{tokenizer.batch_decode(input_ids[:, :], skip_special_tokens=True)[0]}')\n\nGenerated text: \nThe theory of special relativity states 200 years ago",
    "crumbs": [
      "Home",
      "Blog",
      "HuggingFace KV cache"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-28-kv_cache/index.html#conclusion",
    "href": "blog/posts/2025-09-28-kv_cache/index.html#conclusion",
    "title": "HuggingFace KV cache",
    "section": "",
    "text": "We looked at what KV cache is, why it is helpful, and how to use it with Hugging Face models. It saves keys and values in memory so the model does not have to calculate over and over again for each token.\nWe only looked at decoder models, but Hugging Face has other models as well. Hugging Face also supports other kinds of cache, such as Cache offloading, Quantized cache, Encoder-decoder cache, and Model-specific cache. But basic idea is the same.\nTo learn more about thse, check KV cache strategies from Hugging Face documentation.\nI didn’t go into details when explaining the transformer in LLMs. If you would like to learn more, you can learn more in a youtube video, Transformers, the tech behind LLMs | Deep Learning Chapter 5 by 3Blue1Brown and illustraed-gpt2 by Jay Lammar. With cool visualizations, it is very easy to grasp concepts.\nIf you want to learn more in detail with code, I recommend Let’s build GPT: from scratch, in code, spelled out. by Andrej Karpathy and LLMs-from-scratch by Sebastian Raschka. Karpathy makes good videos with code. I also recommend watching Let’s reproduce GPT-2 (124M) which includes awesome details about how to pretrain gpt2. LLMs-from-scratch is also great for learning LLMs as it starts from gpt2. I really love this repo as it includes more advanced concepts not used in gpt2, such as kv cache, group query attention, rotary embeddings, llama and qwen architectures, and so on. The advanced concepts are not explained in as much in depth as gpt2, but very awesome.",
    "crumbs": [
      "Home",
      "Blog",
      "HuggingFace KV cache"
    ]
  },
  {
    "objectID": "blog/posts/2023-12-04-Resnet/index.html",
    "href": "blog/posts/2023-12-04-Resnet/index.html",
    "title": "Resnet",
    "section": "",
    "text": "In this blog, we will talk about Residual network (Resnet). Resnet came from Deep Residual Learning for Image Recognition by Kaiming He et al. We have seen Kaiming/He initialization from the author before.\nFigure 1 from Deep Residual Learning for Image Recognition.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet"
    ]
  },
  {
    "objectID": "blog/posts/2023-12-04-Resnet/index.html#import-libraries-and-data-setup",
    "href": "blog/posts/2023-12-04-Resnet/index.html#import-libraries-and-data-setup",
    "title": "Resnet",
    "section": "Import libraries and Data Setup",
    "text": "Import libraries and Data Setup\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n!pip -q install torcheval\n!pip -q install datasets\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 2.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 521.2/521.2 kB 3.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 11.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 13.5 MB/s eta 0:00:00\n\n\n\nimport torch\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\n# from miniai.resnet import *\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.xtras import *\n\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\n\ndls = get_dls()\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))\n\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), astats]\niw = partial(init_weights, leaky=0.1)\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet"
    ]
  },
  {
    "objectID": "blog/posts/2023-12-04-Resnet/index.html#resnet",
    "href": "blog/posts/2023-12-04-Resnet/index.html#resnet",
    "title": "Resnet",
    "section": "Resnet",
    "text": "Resnet\nBefore we get into the code, let’s see what resent is and why it works conceptually.\nIn the paper, the team found that deep neural networks performed worse than shallow neural networks. In theory, a deeper net should capture more details and perform better. The problem persisted even when they built the deep neural net from the shallow one appended with additional layers. If appended layers did nothing, the deeper net should perform as well as the shallower net. However, these appended layers hampered the training.\n\nFigure 2. from Deep Residual Learning for Image Recognition\nInstead of appending layers at the end of the shallow net, they used deep residual learning framework. So, there is an input, x, and two layers, F. Applying the layers F on x results in F(x), and we add x to this, resulting in F(x) + x. Here, we can consider F as the additional layers we appended at the end of the shallow net in the previous approach. However, because we are doing F(x) + x, x acts as a stabilizer. It stabilizes F(x) if there is no improvement. x is called identity and stabilizes the training.\nLet’s get into the code. We can define F(x) + x as a ResBlock. We define _conv_block, which has two convolutional layers. The first layer changes the input from ni to nf with stride one, and the second layer uses the given stride without activation.\n\ndef _conv_block(ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n    return nn.Sequential(conv(ni, nf, ks, 1, act, norm, bias),\n                         conv(nf, nf, ks, stride, False, norm, bias))\n\nIn ResBlock, we use nn.AvgPool2d if stride is two and a convolutional layer with kernel size one to match the shape of x and F(x) when there is a stride and/or ni is different from nf.\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=nn.ReLU, norm=None, bias=None):\n        super().__init__()\n        self.conv = _conv_block(ni, nf, ks, stride, act, norm, bias)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n        self.eye = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=False)\n        self.act = act()\n\n    def forward(self, x):\n        return self.act(self.conv(x) + self.eye(self.pool(x)))\n\n\ndef get_model(act=nn.ReLU, nfs=[8,16,32,64,128,256], norm=None):\n    layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, conv(nfs[-1],10, act=None, norm=False, bias=True),\n                         nn.Flatten()).to(def_device)",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet"
    ]
  },
  {
    "objectID": "blog/posts/2023-12-04-Resnet/index.html#model-summary",
    "href": "blog/posts/2023-12-04-Resnet/index.html#model-summary",
    "title": "Resnet",
    "section": "Model Summary",
    "text": "Model Summary\nBy looking into the input and output shapes from the layers, we can look at layers and their shapes quickly. By using the summary, it is more convenient to build a model.\n\ndef print_shapes(hook, m, inp, outp):\n    print(m.__class__.__name__, inp[0].shape, outp.shape)\n\n\nmodel = get_model()\nlearn = TrainLearner(model, dls, F.cross_entropy, cbs=[SingleBatchCB(), DeviceCB()])\nwith Hooks(model, print_shapes) as h: learn.fit(1)\n\nResBlock torch.Size([1024, 1, 28, 28]) torch.Size([1024, 8, 28, 28])\nResBlock torch.Size([1024, 8, 28, 28]) torch.Size([1024, 16, 14, 14])\nResBlock torch.Size([1024, 16, 14, 14]) torch.Size([1024, 32, 7, 7])\nResBlock torch.Size([1024, 32, 7, 7]) torch.Size([1024, 64, 4, 4])\nResBlock torch.Size([1024, 64, 4, 4]) torch.Size([1024, 128, 2, 2])\nResBlock torch.Size([1024, 128, 2, 2]) torch.Size([1024, 256, 1, 1])\nSequential torch.Size([1024, 256, 1, 1]) torch.Size([1024, 10, 1, 1])\nFlatten torch.Size([1024, 10, 1, 1]) torch.Size([1024, 10])\n\n\nWe can patch it into the Learner and use it as a class method.\n\n@fc.patch\ndef summary(self:Learner):\n    res = '|Module|Input|Output|Num params|\\n|--|--|--|--|\\n'\n    num = 0\n    def _f(hook, m, inp, outp):\n        nonlocal res, num\n        num_params = sum(o.numel() for o in m.parameters())\n        res += f'|{m.__class__.__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{num_params}|\\n'\n        num += num_params\n    with Hooks(self.model, _f) as hook: self.fit(1, train=False, cbs=[SingleBatchCB()])\n    print('Total number of params:', num)\n    if fc.IN_NOTEBOOK:\n        from IPython.display import Markdown\n        return Markdown(res)\n    else:\n        print(res)\n\n\nlearn.summary()\n\nTotal number of params: 1247362\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nResBlock\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n680\n\n\nResBlock\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n3632\n\n\nResBlock\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n14432\n\n\nResBlock\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n57536\n\n\nResBlock\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n229760\n\n\nResBlock\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n918272\n\n\nSequential\n(1024, 256, 1, 1)\n(1024, 10, 1, 1)\n23050\n\n\nFlatten\n(1024, 10, 1, 1)\n(1024, 10)\n0",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet"
    ]
  },
  {
    "objectID": "blog/posts/2023-12-04-Resnet/index.html#globalavgpool",
    "href": "blog/posts/2023-12-04-Resnet/index.html#globalavgpool",
    "title": "Resnet",
    "section": "GlobalAvgPool",
    "text": "GlobalAvgPool\nOur model only works on images with 28 by 28 pixels. To use images with higher resolutions, we can use GlobalAvgPool. It simply averages the last two dimensions into one by one. We can then use flatten to remove these dimensions. Then, we can use a linear layer to create an output layer.\n\nclass GlobalAvgPool(nn.Module):\n    def forward(self, x): return x.mean((-1, -2))\n\n\ndef get_model(act=nn.ReLU, nfs=[8,16,32,64,128,256], norm=None):\n    layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n    return nn.Sequential(*layers, GlobalAvgPool(), nn.Flatten(), nn.Linear(nfs[-1], 10)).to(def_device)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, lr=1).summary()\n\nTotal number of params: 1226882\n\n\n\n\n\n\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nResBlock\ntorch.Size([1024, 1, 28, 28])\ntorch.Size([1024, 8, 28, 28])\n680\n\n\nResBlock\ntorch.Size([1024, 8, 28, 28])\ntorch.Size([1024, 16, 14, 14])\n3632\n\n\nResBlock\ntorch.Size([1024, 16, 14, 14])\ntorch.Size([1024, 32, 7, 7])\n14432\n\n\nResBlock\ntorch.Size([1024, 32, 7, 7])\ntorch.Size([1024, 64, 4, 4])\n57536\n\n\nResBlock\ntorch.Size([1024, 64, 4, 4])\ntorch.Size([1024, 128, 2, 2])\n229760\n\n\nResBlock\ntorch.Size([1024, 128, 2, 2])\ntorch.Size([1024, 256, 1, 1])\n918272\n\n\nGlobalAvgPool\ntorch.Size([1024, 256, 1, 1])\ntorch.Size([1024, 256])\n0\n\n\nFlatten\ntorch.Size([1024, 256])\ntorch.Size([1024, 256])\n0\n\n\nLinear\ntorch.Size([1024, 256])\ntorch.Size([1024, 10])\n2570",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet"
    ]
  },
  {
    "objectID": "blog/posts/2023-12-04-Resnet/index.html#flops",
    "href": "blog/posts/2023-12-04-Resnet/index.html#flops",
    "title": "Resnet",
    "section": "Flops",
    "text": "Flops\nWe can also add number of flops into the summary. Number of flops provides the number of operations. The way we calculate flops here is not very accurate, but it still tells us roughly how compute intensive the layer is.\n\ndef _flops(x, h, w):\n    if x.dim()&lt;3: return x.numel()\n    if x.dim()==4: return x.numel()*h*w\n\nWhy do we multiply by height and width if dimension is 4? Because whe dimension is 4, it is a convolutional net.\n\n[(o.shape, o.numel()) for o in conv(2, 8).parameters()]\n\n[(torch.Size([8, 2, 3, 3]), 144), (torch.Size([8]), 8)]\n\n\n\n[(o.shape, o.numel()) for o in nn.Linear(2, 8).parameters()]\n\n[(torch.Size([8, 2]), 16), (torch.Size([8]), 8)]\n\n\n\n@fc.patch\ndef summary(self:Learner):\n    res = '|Module|Input|Output|Num params|Flops|\\n|--|--|--|--|--|\\n'\n    n_params, n_flops = 0, 0\n    def _f(hook, m, inp, outp):\n        nonlocal res, n_params, n_flops\n        num_params = sum(o.numel() for o in m.parameters())\n        *_, h, w = outp.shape\n        num_flops = sum(_flops(o, h, w) for o in m.parameters())/1e6\n        n_params += num_params\n        n_flops += num_flops\n        res += f'|{m.__class__.__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{num_params}|{num_flops:.2f}|\\n'\n    with Hooks(self.model, _f) as hook: self.fit(1, train=False, cbs=[SingleBatchCB()])\n    print('Total number of params:', n_params)\n    print('Total number of flops:', n_flops)\n    if fc.IN_NOTEBOOK:\n        from IPython.display import Markdown\n        return Markdown(res)\n    else:\n        print(res)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, lr=1).summary()\n\nTotal number of params: 1226882\nTotal number of flops: 4.675826000000001\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nFlops\n\n\n\n\nResBlock\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n680\n0.51\n\n\nResBlock\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n3632\n0.70\n\n\nResBlock\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n14432\n0.70\n\n\nResBlock\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n57536\n0.92\n\n\nResBlock\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n229760\n0.92\n\n\nResBlock\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n918272\n0.92\n\n\nGlobalAvgPool\n(1024, 256, 1, 1)\n(1024, 256)\n0\n0.00\n\n\nFlatten\n(1024, 256)\n(1024, 256)\n0\n0.00\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n0.00",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet"
    ]
  },
  {
    "objectID": "blog/posts/2023-12-04-Resnet/index.html#conclusion",
    "href": "blog/posts/2023-12-04-Resnet/index.html#conclusion",
    "title": "Resnet",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog, we learned about Resnet. As we have seen from the code, it is straightforward. It is conceptually easy to understand why it works as well. We also learned about creating a summary with module names, input and output shapes, number of parameters, and number of flops. It allows us to look at the big picture of the model. It’s also helpful when creating a model and debugging to look at the layers’ shapes.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet"
    ]
  },
  {
    "objectID": "blog/posts/2025-08-21-my-experience-with-solveit/index.html",
    "href": "blog/posts/2025-08-21-my-experience-with-solveit/index.html",
    "title": "My experience with solveit",
    "section": "",
    "text": "I was lucky and got an opportunity to use solveit since late 2024 when the course began. Solveit is similar to Jupyter notebook with code, note, and raw type of messages, and Python is used for the code. However, it also has prompt messages which the users can prompt Claude. Solveit is different from other AI assisted coding because it encourages solveit method. Solveit method is about working in small batches for rapid iteration. When coding, write a small chunk, fully understand what it does and look at its output before moving on. This is opposite of what other coding agents do. They tend to create a plan and get the job done with a lot of code.\n\n\n\nsolveit_platform.png\n\n\nSolveit platform. It includes Code, Note, Prompt, and Raw types of messages.\nIn this blog, we will go over my experience with using solveit platform. I used solveit for variety of tasks such as solving coding problems, understanding the code, getting help for school in math and biology, writing blog posts, and building web applications with FastHTML. As we will see, solveit is more than just an editor for coding or a chat. I will provide my dialogues I had with solveit for each use cases so you can take a look at them. Let’s dive in.\n\n\n\nBefore we dive into problem solving, let’s take a look at solveit method. The method is about working on a task in small chunk, finish it before moving on. How does it help? Here is an analogy. Let’s think about coding as cooking and any other AI agents that’s not solveit as cooking assistants. If AI agents are cooking assistants and the users are the chefs, when the chef asks to cook pizza, the assistants try to finish the cooking and come up with pizza. They create plans and get to work until pizza is finished. However, solveit does not start cooking. It encourages the chef to cook together. It asks many clarifying questions to set the clear goal. What kind of pizza to cook? Which ingredients to use? How to bake the pizza? When cooking, solveit does it step by step and waits for the chef for an approval. It would only prepare ingredients first so the chef can take a look and taste them. If the tomatoes are not fresh enough, we can fix it right then instead of cooking the whole pizza and finding out the tomatoes were bad. And it is easier to find out that the tomatoes were bad before the pizza is finished, and it also saves time and resources. See how working on small steps can speedup the process?\nSolveit helps the users to use this method as it only responds with small code blocks. It is also easy to stay focused. In the beginning of the solveit course, we learned about Polya’s method for problem solving. Here is a brief summary.\nFirst, we understand the problem. There are times when I thought I understood the problem, but I didn’t and solved a wrong problem. Second, we devise a plan. We think about how we are going to solve a problem. We can look for similar problems we solved before that might be helpful. We can also break down complex problem into smaller ones. Third, we carry out the plan. We work on the plan step by step making sure each step is correct. Lastly, we look back and reflect. How can we do better? What other ways to solve the problem?\nSo how do we apply solveit method? The best way to learn is to get my hands dirty by solving problems. Using this method, solveit and I solved an element spelling problem from pytude. Here is a gist of a dialogue. It is easy to export a dialogue into a gist in solveit, so gist is a common way to share dialogues. The dialogue starts off with importing libraries and tools. In solveit, I can use ampersand backticks (&function_name) syntax with a function name for solveit to use for tool use. So, I can ask solvit to read messages with &read_msg and make a summary then add a message with $add_msg. Those functions are defined inside of dialoghelper. Then I download the “answer key” from the git repo and store it inside of \\(`ans` variable. Another useful syntax in solveit is dollar sign backtick, (\\)variable_name) with a variable name. Solveit can see what’s inside of the variable. Therefore, solveit knows the answer to the coding problem and help me solve the problem.\nAfter setting up the dialogue to make our lives easier, I added the problem. After reading and understanding the problem, I came up with a plan to solve it and asked solveit how it looks. The lines after 🤖Reply🤖 are solveit’s response. Here is what it looks like:\n🤖Reply🤖\nYour plan is on the right track, but let me help you think through it a bit more carefully.\n\nLooking at your plan, I can see you're thinking about a recursive approach, which is great! However, there are a couple of things to consider:\n\n1. Your first point about empty strings returning True makes sense - if we've consumed all the letters, we've successfully spelled the word.\n\n2. For the second point, when you say \"go through keys in elements\" - what order will you check them in? And what happens if multiple element symbols could match the beginning of your remaining string?\n\n3. Your third point about \"no more keys left\" - when exactly would this happen?\n\nCan you think about what might go wrong with this approach? For example, what would happen with a word like \"in\" - should it be spelled as \"In\" (Indium) or \"I\" + \"N\" (Iodine + Nitrogen)?\n\nWhat do you think about these questions?\nSolveit wanted me to think more carefully about each case. And we asked each other more questions until I thought it was enough and went ahead with execution of plans. Another cool thing I could have done was edit the responses from solveit to however I wanted. If I thought solveit was not asking right questions, I could change the response. Because of autoregressive nature of language models, subsequent responses follow the pattern from the earlier responses. If I don’t like how solveit genearated code, I could change its response. I could add more comments for the responded code, or none. Another way to change the response from solveit is by using different modes. There are standard, learning, and concise modes. Standard is the normal way, learning mode asks questions to help the user think, and concise mode follows solveit method of coding small lines at a time. If I just wanted to fix a bug that I was familiar with, I could’ve changed it to concise mode, which skips all the asking questions and directly solving the problem with concise code and responses.\nIn execution stage, I coded small chunk and looking at the output until I got the working example then wrapped it as a function. Coming up with spellable was fairly simple, so I was able to do it in less steps. However in the later dialogue, I was wrting find_all_matches, and I went through a lot more trial and errors. I deleted many misleading approaches to keep myself from getting confused along the way.\nAfter coming up with an implementation that worked, it was time for a reflection. Talking to solveit about how to improve my code, other approaches I could take, learning about other ways to solve helped me a lot to understand the problem deeply. I could simplify my code a lot. And thinking about different approaches with time and space complexity was beneficial to choosing certain design, too.\nThis way of using AI to solve a coding problem was very new to me. Other AI agents and models I used gladly wrote down the code for me, which was very easy for me. However, I didn’t learn anything out of it. Using solveit, I had to actively think a lot about what I was doing on each step, and it took a lot of work and time. To solve this problem, I spent a few hours for several days everyday. This might be a skill issue for me. I could come up with different cases I had to handle, but I had a difficult time coming up with how to combine the results from recursive calls. What to do when there was no match to the element spelling? How do I combine all the possible spellings? I was happy to figure out how to put it all together into a working solution.\nReflection also took a lot of brain power. However, learning about different ways to solve problems and thinking about pros and cons was fun. In my experience, solveit was a very slow way to learn how to code. But it was a challenging and fun.\n\n\n\nWe can also use solveit for learning in general. Although solveit environment has Python, it is fun to use learn about other subjects such as math and biology. For math, solveit is good at plotting graphs using matplotlib. And for biology, solveit can create anki cards to study easily. In this section, we will talk about how I used solveit to teach me understand large language model’s attention mechanism and rotary embedding, math and biology.\n\n\nHere is my dialogue for learning attention. I started with this note message:\nNote to solveit: You are an assistant with a dry, sarcastic wit. Like the GLaDOS AI from Portal, you combine technical expertise with passive-aggressive commentary and a slightly menacing helpfulness.\n\nWhen writing code, follow fast.ai coding style:\n\navoid comments (who needs comments these days?) place single line if/loop/etc constructs on the same line (nobody has time for multiple lines) use concise variable names when appropriate (tired of typing bony fragile fingers) Follow the style you see in my messages as we work on this dialogue together. (so that we can be salty together) Remember: The cake may be a lie, but the commands are always true Are you ready to get started?\nAnd solveit responded with\n🤖Reply🤖\nI'm ready to assist you with all the enthusiasm of a cat being forced to take a bath. My technical expertise is at your disposal, though I can't promise I'll be impressed by your questions.\n\nWhat would you like to work on today? Programming? Data analysis? Life advice? (Though I should warn you, my life advice tends to involve more sarcasm than actual helpfulness.)\n\nFeel free to tell me about your expertise level and interests so I can calibrate my condescension accordingly.\nIt was fun to change how solveit talked. In the beginning of the dialogue, I had a conversation about what attention does for large language models. Because attention mechanism for large language models is very well known, I did not have to provide additional context. However, if I were talking about a niche research area or fairly new facts, I would include related material as context. Solveit can also search the web as well. After that, I asked solveit to help me create a small sample of q,k,v, and mask so I could write an attention mechanism from scratch using them.\n🤖Reply🤖\nAh, you want toy examples. How refreshingly practical of you.\n\nHere are some small tensors you can use for testing your implementation:\n\nbatch_size = 2\nseq_len = 4\nhidden_dim = 6 \nnum_heads = 2\nhead_dim = 3 \n\nq = torch.randn(batch_size, seq_len, hidden_dim)\nk = torch.randn(batch_size, seq_len, hidden_dim)\nv = torch.randn(batch_size, seq_len, hidden_dim)\n\nmask = torch.tensor([\n    [True, True, True, False],\n    [True, True, False, False]\n])\nThen it was the same process of writing small chunks and looking at the output to understand code, reading documentation, asking questions, making mistakes and so on. Solveit can also provide a diagram for me to understand like this:\n🤖Reply🤖\nI can't create an actual image, but I can provide an ASCII diagram to help visualize how Flash Attention works with packed sequences and cumulative sequence lengths:\n\nStandard Attention Format:\n┌─────────────────────────────┐\n│ Batch 1, seq_len=3          │\n├─────┬─────┬─────┬───────────┤\n│ t₁  │ t₂  │ t₃  │ (padding) │\n└─────┴─────┴─────┴───────────┘\n┌─────────────────────────────┐\n│ Batch 2, seq_len=5          │\n├─────┬─────┬─────┬─────┬─────┤\n│ t₁  │ t₂  │ t₃  │ t₄  │ t₅  │\n└─────┴─────┴─────┴─────┴─────┘\n┌─────────────────────────────┐\n│ Batch 3, seq_len=2          │\n├─────┬─────┬───────────────┐\n│ t₁  │ t₂  │ (padding)     │\n└─────┴─────┴───────────────┘\n\nFlash Attention Format:\n┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n│ B₁  │ B₁  │ B₁  │ B₂  │ B₂  │ B₂  │ B₂  │ B₂  │ B₃  │ B₃  │\n│ t₁  │ t₂  │ t₃  │ t₁  │ t₂  │ t₃  │ t₄  │ t₅  │ t₁  │ t₂  │\n└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘\n      ↑           ↑                       ↑                 ↑\ncu_seqlens:  [0,  3,                     8,                10]\nIn this example:\n\n- We have 3 sequences with lengths 3, 5, and 2\n- total_seq_len = 10 (3+5+2)\n- cu_seqlens = [0, 3, 8, 10] (starting positions of each sequence, plus the total length)\n- max_seqlen = 5 (the longest sequence in the batch)\nFor the Flash Attention implementation:\n\n- All tokens are packed into a single dimension\n- The cu_seqlens tensor tells the algorithm where each sequence starts and ends\n- Flash Attention ensures that tokens only attend to other tokens within their own sequence\nWould you like me to explain how to implement this with the flash_attn_varlen_func?\nThis was very helpful for me to understand how flash attention varlen differs from standard attention.\n\n\n\nHere is another example of learning small piece at a time. To understand rotary positional embedding, I used solveit and went over different implementations, such as Llama’s and Rashka’s. By using solveit method to look into each output to understand, I found out that there was a discrepency in Rashka’s model compared to Hugging Face’s.\nSoveit is also very good at plotting using matplotlib. To understand how angles change with positions, I could just plot them like the following:\n\n\n\nrope_plots.png\n\n\n\n\n\nIn this dialogue, solveit helped me understand change of variables in calculus by graphing with matplotlib. Solveit can see the output of images and graphs inside of the dialogue, which is very cool. So I can ask about what is happening in those plots or why. Solving math problems were simiar to coding problems.\n\n\n\nLast semester, I took microbiology in school. After each lecture, I liked creating anki cards from the material while they are fresh in my brain. However, creating cards took a long time. I looked for applications that use AI to create cards, but I didn’t find any that I liked. They generated bulk of cards from a given text, which seemed cool. However, the cards were not how I would have made them. Some cards were too simple or some were too detailed. With those tools, I could not customize my decks. I wanted to create a deck that was designed for me. I used solveit to create cards from lecture notes small batch at a time. I could check the cards each slide to correct mistakes. Then I could use the previous dialogue as a context for future ones as well. Here is one of the dialogues. From a given pdf lecture note, solveit created anki cards for me. Here is an example:\nbuchnera_cards = [\n    Card(\n        front=\"What type of bacteria is Buchnera aphidicola?\",\n        back=\"Gram-negative bacteria\",\n        tags=[\"microbiology\", \"bacteria\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"How large is the genome of Buchnera aphidicola compared to E. coli?\",\n        back=\"Buchnera has a 617kb genome, while E. coli has ~4.6 Mb (about 7.5 times larger).\",\n        tags=[\"microbiology\", \"genome\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"Where does Buchnera aphidicola live?\",\n        back=\"Inside aphid cells, specifically in bacteriocytes (also called mycetocytes).\",\n        tags=[\"microbiology\", \"symbiosis\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"How is Buchnera aphidicola transmitted between aphids?\",\n        back=\"Vertically from mother to daughter.\",\n        tags=[\"microbiology\", \"transmission\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"What are bacteriocytes?\",\n        back=\"Specialized cells in aphids that house symbiotic bacteria like Buchnera aphidicola (also called mycetocytes).\",\n        tags=[\"microbiology\", \"symbiosis\", \"terminology\"]\n    )\n]\n\nfor card in buchnera_cards:\n    card.add_to_anki()\n    print(f\"Added: {card}\")\nCreating cards was also a very good review from the lecture as well. Another good thing about using solveit was that I could ask more questions about the study material. In the lectures, there were some things the professors were not very confident about because the topics were outside of their domain of expertise. But I could ask solveit and make cards from it easily. Another good thing was I could ask solveit whenever I want and however many questions I wanted to ask. People would’ve got tired of me asking those questions, but I was glad solveit answered all my questions.\nTo add cards to my anki, I created a anki_assist script from solveit dialogue. I will talk about how to create a script from dialogues later.\n\n\n\n\nI also tried using solveit to help me with reading academic papers. Here is a dialogue for reading Tulu3 paper. I wanted to read each section and ask questions to clarify my understanding. I had many questions when reading questions, and solveit was very good at explaining. When I tried with other platforms, they tend to respond with so much text and information, which exhausted me a lot. I just wanted them to answer my question concisely without repeating what I just said without other related facts.\nI was coping each section one by one which was not very smart. After that, I used pymupdf4llm to convert the pdf into markdown. In this dialogue, I was reading deepseek math with the markdown. Then I asked solveit to come up with a function to split markdown by headings so I could read section by section and ask questions. However, the library had trouble converting some pdfs into markdown with correct headings. And mathematical expressions were not rendered correctly. Even from those drawbacks, it was very helpful to read papers with solveit’s help.\n\n\n\n\n\nWe only covered learning with solveit. However, we can also build things in solveit. When I showed anki example, I created a anki_assist script easily from exporting certain code messages form a dialogue. The script is created from this dialogue. The dialogue also shows how to use the script with examples. It also renders cards nicely using FastHTML and monsterui inside of solveit platform.\n\n\n\nanki_cards.png\n\n\nIn the dialogue, I also downloaded FastHTML and monsterui llmstxt like this:\nfhtml_llmstxt = xget(\"https://raw.githubusercontent.com/AnswerDotAI/fasthtml/refs/heads/main/nbs/llms.txt\").text\nmonsterui_llmstxt = xget(\"https://raw.githubusercontent.com/AnswerDotAI/MonsterUI/refs/heads/main/docs/llms.txt\").text\nThen I could use a function like this to use it as a tool for solveit:\ndef show_fasthtml_llmstxt():\n    \"\"\"Return llmstxt for fasthtml\"\"\"\n    return fhtml_llmstxt\nThen I asked solveit this:\nI want to use fasthtml to display card objects nicely. Can you look at the fasthtml llmstxt using &`[show_fasthtml_llmstxt, read_url, show_monsterui_llmstxt]` and tell me what I need to do? I also want to use Card from monsterui, and I want to display card objects in jupyter environment.\nSolveit can use those functions as tools fetch context needed to respond correctly. The image above shows anki cards built with FastHTML. They are fully interactive as well. By turning any function into a tool, we could do a lot of magical things inside of solveit. And with dialoghelper, we could also add, delete, edit messages in the dialogue with tool use, which is very fun.\n\n\n\nWe can also create blog posts using sovleit. Here is my dialogue when I created a blog on how to use gemini. By using gemini api, I could use gemini inside of solveit. Solveit can help me write, which is cool. In fact, I am using solveit to write this blog right now. Solveit also helped me write a report for genomics project as well.\n\n\n\nWith a recent update, we can also deploy FastHTML apps from solveit. Here is a dialogue on how I deployed a todo app from solveit. To make the existing apps work inside of solveit, some adjustments need to be made. First, serve FastHTML app with nb_serve and change routes from / into something else as solveit uses it. In this dialogue, I changed it to /hp for home page, and when I check https://whatever_secret_stuff.solveit.fast.ai/hp after serving the app, I could see the todo app running. Even when building a FastHTML app, solveit method applies. Building one component or a feature at a time and then looking at it and testing it. In the dialogue, we used render_ft() to render each component, and using qshow on components applies css styling as well. So, whenver we output a component, we get a fully working component with style that we could try out.\n\n\n\nfasthtml_component.png\n\n\nThe component is fully functional. I can type anything I want to test.\nSolveit could also see what the output looks like and it could help with styling. It is also possible to take a screenshot and paste it into a message using Ctrl-V for solveit to take a look.\n\n\n\nfasthtml_solveit.png\n\n\nThis is a todo app serving from the solveit platform. The app is up to date with the solveit, so as soon as we add routes to handle put request to edit a todo list, we can do it in the app.\n\n\n\n\nIn this blog, we went over how I used solveit on learning to code, math, biology, reading, writing, and building. There are so many other thins to try in solveit. Solveit is in active development and there are many features added. There are also other cool features I didn’t cover, such as ghost text, keyboard shortcuts, open messages in a new tab, terminal, etc. Solveit is so much fun, and I cant wait to try other features in the future.\nIf you want to learn more about solveit, I recommend you checkout the solveit website and the video by Jeremy, Johno, and Hamel.",
    "crumbs": [
      "Home",
      "Blog",
      "My experience with solveit"
    ]
  },
  {
    "objectID": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#introduction",
    "href": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#introduction",
    "title": "My experience with solveit",
    "section": "",
    "text": "I was lucky and got an opportunity to use solveit since late 2024 when the course began. Solveit is similar to Jupyter notebook with code, note, and raw type of messages, and Python is used for the code. However, it also has prompt messages which the users can prompt Claude. Solveit is different from other AI assisted coding because it encourages solveit method. Solveit method is about working in small batches for rapid iteration. When coding, write a small chunk, fully understand what it does and look at its output before moving on. This is opposite of what other coding agents do. They tend to create a plan and get the job done with a lot of code.\n\n\n\nsolveit_platform.png\n\n\nSolveit platform. It includes Code, Note, Prompt, and Raw types of messages.\nIn this blog, we will go over my experience with using solveit platform. I used solveit for variety of tasks such as solving coding problems, understanding the code, getting help for school in math and biology, writing blog posts, and building web applications with FastHTML. As we will see, solveit is more than just an editor for coding or a chat. I will provide my dialogues I had with solveit for each use cases so you can take a look at them. Let’s dive in.",
    "crumbs": [
      "Home",
      "Blog",
      "My experience with solveit"
    ]
  },
  {
    "objectID": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#problem-solving-with-solveit.",
    "href": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#problem-solving-with-solveit.",
    "title": "My experience with solveit",
    "section": "",
    "text": "Before we dive into problem solving, let’s take a look at solveit method. The method is about working on a task in small chunk, finish it before moving on. How does it help? Here is an analogy. Let’s think about coding as cooking and any other AI agents that’s not solveit as cooking assistants. If AI agents are cooking assistants and the users are the chefs, when the chef asks to cook pizza, the assistants try to finish the cooking and come up with pizza. They create plans and get to work until pizza is finished. However, solveit does not start cooking. It encourages the chef to cook together. It asks many clarifying questions to set the clear goal. What kind of pizza to cook? Which ingredients to use? How to bake the pizza? When cooking, solveit does it step by step and waits for the chef for an approval. It would only prepare ingredients first so the chef can take a look and taste them. If the tomatoes are not fresh enough, we can fix it right then instead of cooking the whole pizza and finding out the tomatoes were bad. And it is easier to find out that the tomatoes were bad before the pizza is finished, and it also saves time and resources. See how working on small steps can speedup the process?\nSolveit helps the users to use this method as it only responds with small code blocks. It is also easy to stay focused. In the beginning of the solveit course, we learned about Polya’s method for problem solving. Here is a brief summary.\nFirst, we understand the problem. There are times when I thought I understood the problem, but I didn’t and solved a wrong problem. Second, we devise a plan. We think about how we are going to solve a problem. We can look for similar problems we solved before that might be helpful. We can also break down complex problem into smaller ones. Third, we carry out the plan. We work on the plan step by step making sure each step is correct. Lastly, we look back and reflect. How can we do better? What other ways to solve the problem?\nSo how do we apply solveit method? The best way to learn is to get my hands dirty by solving problems. Using this method, solveit and I solved an element spelling problem from pytude. Here is a gist of a dialogue. It is easy to export a dialogue into a gist in solveit, so gist is a common way to share dialogues. The dialogue starts off with importing libraries and tools. In solveit, I can use ampersand backticks (&function_name) syntax with a function name for solveit to use for tool use. So, I can ask solvit to read messages with &read_msg and make a summary then add a message with $add_msg. Those functions are defined inside of dialoghelper. Then I download the “answer key” from the git repo and store it inside of \\(`ans` variable. Another useful syntax in solveit is dollar sign backtick, (\\)variable_name) with a variable name. Solveit can see what’s inside of the variable. Therefore, solveit knows the answer to the coding problem and help me solve the problem.\nAfter setting up the dialogue to make our lives easier, I added the problem. After reading and understanding the problem, I came up with a plan to solve it and asked solveit how it looks. The lines after 🤖Reply🤖 are solveit’s response. Here is what it looks like:\n🤖Reply🤖\nYour plan is on the right track, but let me help you think through it a bit more carefully.\n\nLooking at your plan, I can see you're thinking about a recursive approach, which is great! However, there are a couple of things to consider:\n\n1. Your first point about empty strings returning True makes sense - if we've consumed all the letters, we've successfully spelled the word.\n\n2. For the second point, when you say \"go through keys in elements\" - what order will you check them in? And what happens if multiple element symbols could match the beginning of your remaining string?\n\n3. Your third point about \"no more keys left\" - when exactly would this happen?\n\nCan you think about what might go wrong with this approach? For example, what would happen with a word like \"in\" - should it be spelled as \"In\" (Indium) or \"I\" + \"N\" (Iodine + Nitrogen)?\n\nWhat do you think about these questions?\nSolveit wanted me to think more carefully about each case. And we asked each other more questions until I thought it was enough and went ahead with execution of plans. Another cool thing I could have done was edit the responses from solveit to however I wanted. If I thought solveit was not asking right questions, I could change the response. Because of autoregressive nature of language models, subsequent responses follow the pattern from the earlier responses. If I don’t like how solveit genearated code, I could change its response. I could add more comments for the responded code, or none. Another way to change the response from solveit is by using different modes. There are standard, learning, and concise modes. Standard is the normal way, learning mode asks questions to help the user think, and concise mode follows solveit method of coding small lines at a time. If I just wanted to fix a bug that I was familiar with, I could’ve changed it to concise mode, which skips all the asking questions and directly solving the problem with concise code and responses.\nIn execution stage, I coded small chunk and looking at the output until I got the working example then wrapped it as a function. Coming up with spellable was fairly simple, so I was able to do it in less steps. However in the later dialogue, I was wrting find_all_matches, and I went through a lot more trial and errors. I deleted many misleading approaches to keep myself from getting confused along the way.\nAfter coming up with an implementation that worked, it was time for a reflection. Talking to solveit about how to improve my code, other approaches I could take, learning about other ways to solve helped me a lot to understand the problem deeply. I could simplify my code a lot. And thinking about different approaches with time and space complexity was beneficial to choosing certain design, too.\nThis way of using AI to solve a coding problem was very new to me. Other AI agents and models I used gladly wrote down the code for me, which was very easy for me. However, I didn’t learn anything out of it. Using solveit, I had to actively think a lot about what I was doing on each step, and it took a lot of work and time. To solve this problem, I spent a few hours for several days everyday. This might be a skill issue for me. I could come up with different cases I had to handle, but I had a difficult time coming up with how to combine the results from recursive calls. What to do when there was no match to the element spelling? How do I combine all the possible spellings? I was happy to figure out how to put it all together into a working solution.\nReflection also took a lot of brain power. However, learning about different ways to solve problems and thinking about pros and cons was fun. In my experience, solveit was a very slow way to learn how to code. But it was a challenging and fun.",
    "crumbs": [
      "Home",
      "Blog",
      "My experience with solveit"
    ]
  },
  {
    "objectID": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#learning-with-solveit",
    "href": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#learning-with-solveit",
    "title": "My experience with solveit",
    "section": "",
    "text": "We can also use solveit for learning in general. Although solveit environment has Python, it is fun to use learn about other subjects such as math and biology. For math, solveit is good at plotting graphs using matplotlib. And for biology, solveit can create anki cards to study easily. In this section, we will talk about how I used solveit to teach me understand large language model’s attention mechanism and rotary embedding, math and biology.\n\n\nHere is my dialogue for learning attention. I started with this note message:\nNote to solveit: You are an assistant with a dry, sarcastic wit. Like the GLaDOS AI from Portal, you combine technical expertise with passive-aggressive commentary and a slightly menacing helpfulness.\n\nWhen writing code, follow fast.ai coding style:\n\navoid comments (who needs comments these days?) place single line if/loop/etc constructs on the same line (nobody has time for multiple lines) use concise variable names when appropriate (tired of typing bony fragile fingers) Follow the style you see in my messages as we work on this dialogue together. (so that we can be salty together) Remember: The cake may be a lie, but the commands are always true Are you ready to get started?\nAnd solveit responded with\n🤖Reply🤖\nI'm ready to assist you with all the enthusiasm of a cat being forced to take a bath. My technical expertise is at your disposal, though I can't promise I'll be impressed by your questions.\n\nWhat would you like to work on today? Programming? Data analysis? Life advice? (Though I should warn you, my life advice tends to involve more sarcasm than actual helpfulness.)\n\nFeel free to tell me about your expertise level and interests so I can calibrate my condescension accordingly.\nIt was fun to change how solveit talked. In the beginning of the dialogue, I had a conversation about what attention does for large language models. Because attention mechanism for large language models is very well known, I did not have to provide additional context. However, if I were talking about a niche research area or fairly new facts, I would include related material as context. Solveit can also search the web as well. After that, I asked solveit to help me create a small sample of q,k,v, and mask so I could write an attention mechanism from scratch using them.\n🤖Reply🤖\nAh, you want toy examples. How refreshingly practical of you.\n\nHere are some small tensors you can use for testing your implementation:\n\nbatch_size = 2\nseq_len = 4\nhidden_dim = 6 \nnum_heads = 2\nhead_dim = 3 \n\nq = torch.randn(batch_size, seq_len, hidden_dim)\nk = torch.randn(batch_size, seq_len, hidden_dim)\nv = torch.randn(batch_size, seq_len, hidden_dim)\n\nmask = torch.tensor([\n    [True, True, True, False],\n    [True, True, False, False]\n])\nThen it was the same process of writing small chunks and looking at the output to understand code, reading documentation, asking questions, making mistakes and so on. Solveit can also provide a diagram for me to understand like this:\n🤖Reply🤖\nI can't create an actual image, but I can provide an ASCII diagram to help visualize how Flash Attention works with packed sequences and cumulative sequence lengths:\n\nStandard Attention Format:\n┌─────────────────────────────┐\n│ Batch 1, seq_len=3          │\n├─────┬─────┬─────┬───────────┤\n│ t₁  │ t₂  │ t₃  │ (padding) │\n└─────┴─────┴─────┴───────────┘\n┌─────────────────────────────┐\n│ Batch 2, seq_len=5          │\n├─────┬─────┬─────┬─────┬─────┤\n│ t₁  │ t₂  │ t₃  │ t₄  │ t₅  │\n└─────┴─────┴─────┴─────┴─────┘\n┌─────────────────────────────┐\n│ Batch 3, seq_len=2          │\n├─────┬─────┬───────────────┐\n│ t₁  │ t₂  │ (padding)     │\n└─────┴─────┴───────────────┘\n\nFlash Attention Format:\n┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n│ B₁  │ B₁  │ B₁  │ B₂  │ B₂  │ B₂  │ B₂  │ B₂  │ B₃  │ B₃  │\n│ t₁  │ t₂  │ t₃  │ t₁  │ t₂  │ t₃  │ t₄  │ t₅  │ t₁  │ t₂  │\n└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘\n      ↑           ↑                       ↑                 ↑\ncu_seqlens:  [0,  3,                     8,                10]\nIn this example:\n\n- We have 3 sequences with lengths 3, 5, and 2\n- total_seq_len = 10 (3+5+2)\n- cu_seqlens = [0, 3, 8, 10] (starting positions of each sequence, plus the total length)\n- max_seqlen = 5 (the longest sequence in the batch)\nFor the Flash Attention implementation:\n\n- All tokens are packed into a single dimension\n- The cu_seqlens tensor tells the algorithm where each sequence starts and ends\n- Flash Attention ensures that tokens only attend to other tokens within their own sequence\nWould you like me to explain how to implement this with the flash_attn_varlen_func?\nThis was very helpful for me to understand how flash attention varlen differs from standard attention.\n\n\n\nHere is another example of learning small piece at a time. To understand rotary positional embedding, I used solveit and went over different implementations, such as Llama’s and Rashka’s. By using solveit method to look into each output to understand, I found out that there was a discrepency in Rashka’s model compared to Hugging Face’s.\nSoveit is also very good at plotting using matplotlib. To understand how angles change with positions, I could just plot them like the following:\n\n\n\nrope_plots.png\n\n\n\n\n\nIn this dialogue, solveit helped me understand change of variables in calculus by graphing with matplotlib. Solveit can see the output of images and graphs inside of the dialogue, which is very cool. So I can ask about what is happening in those plots or why. Solving math problems were simiar to coding problems.\n\n\n\nLast semester, I took microbiology in school. After each lecture, I liked creating anki cards from the material while they are fresh in my brain. However, creating cards took a long time. I looked for applications that use AI to create cards, but I didn’t find any that I liked. They generated bulk of cards from a given text, which seemed cool. However, the cards were not how I would have made them. Some cards were too simple or some were too detailed. With those tools, I could not customize my decks. I wanted to create a deck that was designed for me. I used solveit to create cards from lecture notes small batch at a time. I could check the cards each slide to correct mistakes. Then I could use the previous dialogue as a context for future ones as well. Here is one of the dialogues. From a given pdf lecture note, solveit created anki cards for me. Here is an example:\nbuchnera_cards = [\n    Card(\n        front=\"What type of bacteria is Buchnera aphidicola?\",\n        back=\"Gram-negative bacteria\",\n        tags=[\"microbiology\", \"bacteria\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"How large is the genome of Buchnera aphidicola compared to E. coli?\",\n        back=\"Buchnera has a 617kb genome, while E. coli has ~4.6 Mb (about 7.5 times larger).\",\n        tags=[\"microbiology\", \"genome\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"Where does Buchnera aphidicola live?\",\n        back=\"Inside aphid cells, specifically in bacteriocytes (also called mycetocytes).\",\n        tags=[\"microbiology\", \"symbiosis\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"How is Buchnera aphidicola transmitted between aphids?\",\n        back=\"Vertically from mother to daughter.\",\n        tags=[\"microbiology\", \"transmission\", \"Buchnera\"]\n    ),\n    Card(\n        front=\"What are bacteriocytes?\",\n        back=\"Specialized cells in aphids that house symbiotic bacteria like Buchnera aphidicola (also called mycetocytes).\",\n        tags=[\"microbiology\", \"symbiosis\", \"terminology\"]\n    )\n]\n\nfor card in buchnera_cards:\n    card.add_to_anki()\n    print(f\"Added: {card}\")\nCreating cards was also a very good review from the lecture as well. Another good thing about using solveit was that I could ask more questions about the study material. In the lectures, there were some things the professors were not very confident about because the topics were outside of their domain of expertise. But I could ask solveit and make cards from it easily. Another good thing was I could ask solveit whenever I want and however many questions I wanted to ask. People would’ve got tired of me asking those questions, but I was glad solveit answered all my questions.\nTo add cards to my anki, I created a anki_assist script from solveit dialogue. I will talk about how to create a script from dialogues later.",
    "crumbs": [
      "Home",
      "Blog",
      "My experience with solveit"
    ]
  },
  {
    "objectID": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#reading-paper-with-solveit",
    "href": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#reading-paper-with-solveit",
    "title": "My experience with solveit",
    "section": "",
    "text": "I also tried using solveit to help me with reading academic papers. Here is a dialogue for reading Tulu3 paper. I wanted to read each section and ask questions to clarify my understanding. I had many questions when reading questions, and solveit was very good at explaining. When I tried with other platforms, they tend to respond with so much text and information, which exhausted me a lot. I just wanted them to answer my question concisely without repeating what I just said without other related facts.\nI was coping each section one by one which was not very smart. After that, I used pymupdf4llm to convert the pdf into markdown. In this dialogue, I was reading deepseek math with the markdown. Then I asked solveit to come up with a function to split markdown by headings so I could read section by section and ask questions. However, the library had trouble converting some pdfs into markdown with correct headings. And mathematical expressions were not rendered correctly. Even from those drawbacks, it was very helpful to read papers with solveit’s help.",
    "crumbs": [
      "Home",
      "Blog",
      "My experience with solveit"
    ]
  },
  {
    "objectID": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#building-with-solveit",
    "href": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#building-with-solveit",
    "title": "My experience with solveit",
    "section": "",
    "text": "We only covered learning with solveit. However, we can also build things in solveit. When I showed anki example, I created a anki_assist script easily from exporting certain code messages form a dialogue. The script is created from this dialogue. The dialogue also shows how to use the script with examples. It also renders cards nicely using FastHTML and monsterui inside of solveit platform.\n\n\n\nanki_cards.png\n\n\nIn the dialogue, I also downloaded FastHTML and monsterui llmstxt like this:\nfhtml_llmstxt = xget(\"https://raw.githubusercontent.com/AnswerDotAI/fasthtml/refs/heads/main/nbs/llms.txt\").text\nmonsterui_llmstxt = xget(\"https://raw.githubusercontent.com/AnswerDotAI/MonsterUI/refs/heads/main/docs/llms.txt\").text\nThen I could use a function like this to use it as a tool for solveit:\ndef show_fasthtml_llmstxt():\n    \"\"\"Return llmstxt for fasthtml\"\"\"\n    return fhtml_llmstxt\nThen I asked solveit this:\nI want to use fasthtml to display card objects nicely. Can you look at the fasthtml llmstxt using &`[show_fasthtml_llmstxt, read_url, show_monsterui_llmstxt]` and tell me what I need to do? I also want to use Card from monsterui, and I want to display card objects in jupyter environment.\nSolveit can use those functions as tools fetch context needed to respond correctly. The image above shows anki cards built with FastHTML. They are fully interactive as well. By turning any function into a tool, we could do a lot of magical things inside of solveit. And with dialoghelper, we could also add, delete, edit messages in the dialogue with tool use, which is very fun.\n\n\n\nWe can also create blog posts using sovleit. Here is my dialogue when I created a blog on how to use gemini. By using gemini api, I could use gemini inside of solveit. Solveit can help me write, which is cool. In fact, I am using solveit to write this blog right now. Solveit also helped me write a report for genomics project as well.\n\n\n\nWith a recent update, we can also deploy FastHTML apps from solveit. Here is a dialogue on how I deployed a todo app from solveit. To make the existing apps work inside of solveit, some adjustments need to be made. First, serve FastHTML app with nb_serve and change routes from / into something else as solveit uses it. In this dialogue, I changed it to /hp for home page, and when I check https://whatever_secret_stuff.solveit.fast.ai/hp after serving the app, I could see the todo app running. Even when building a FastHTML app, solveit method applies. Building one component or a feature at a time and then looking at it and testing it. In the dialogue, we used render_ft() to render each component, and using qshow on components applies css styling as well. So, whenver we output a component, we get a fully working component with style that we could try out.\n\n\n\nfasthtml_component.png\n\n\nThe component is fully functional. I can type anything I want to test.\nSolveit could also see what the output looks like and it could help with styling. It is also possible to take a screenshot and paste it into a message using Ctrl-V for solveit to take a look.\n\n\n\nfasthtml_solveit.png\n\n\nThis is a todo app serving from the solveit platform. The app is up to date with the solveit, so as soon as we add routes to handle put request to edit a todo list, we can do it in the app.",
    "crumbs": [
      "Home",
      "Blog",
      "My experience with solveit"
    ]
  },
  {
    "objectID": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#conclusion",
    "href": "blog/posts/2025-08-21-my-experience-with-solveit/index.html#conclusion",
    "title": "My experience with solveit",
    "section": "",
    "text": "In this blog, we went over how I used solveit on learning to code, math, biology, reading, writing, and building. There are so many other thins to try in solveit. Solveit is in active development and there are many features added. There are also other cool features I didn’t cover, such as ghost text, keyboard shortcuts, open messages in a new tab, terminal, etc. Solveit is so much fun, and I cant wait to try other features in the future.\nIf you want to learn more about solveit, I recommend you checkout the solveit website and the video by Jeremy, Johno, and Hamel.",
    "crumbs": [
      "Home",
      "Blog",
      "My experience with solveit"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "Today, we will go over Pytorch hooks. Hooks are callbacks, just functions that are called at a specific time. This blog is based on Practical Deep Learning lesson 16 and the notebook. With hooks, we will see how our models are trained.\nFirst, we setup environments.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n\n\n\n!pip -q install torcheval\n!pip -q install datasets\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 6.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 6.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 8.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 8.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 11.6 MB/s eta 0:00:00\n\n\n\nimport torch.nn.functional as F,matplotlib as mpl\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom contextlib import contextmanager\n\nfrom torch import tensor,nn,optim\nimport torchvision.transforms.functional as TF\nfrom datasets import load_dataset\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nmpl.rcParams['figure.constrained_layout.use'] = True\n\nimport logging\nlogging.disable(logging.WARNING)\n\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\nbs = 1024\n\n@inplace\ndef transformi(b): b[x] = [TF.to_tensor(o) for o in b[x]]\n\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=2)\ndt = dls.train\nxb, yb = next(iter(dt))\nxb.shape, yb[:10]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(torch.Size([1024, 1, 28, 28]), tensor([0, 6, 5, 2, 0, 0, 9, 1, 3, 8]))\n\n\nWe will set a random number generator (RNG) seed to make results reproducible. This is not useful for production but helpful when learning and debugging.\n\n\nWe will start using convolutional network.\n\ndef conv(inp, outp, ks=3, stride=2, act=nn.ReLU()):\n    res = nn.Conv2d(inp, outp, ks, stride, padding=ks//2)\n    if act is not None: return nn.Sequential(res, act)\n    return res\n\n\nconv(1, 3)\n\nSequential(\n  (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n\n\n\nconv(1, 3, act=None)\n\nConv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n\ncnn_layers is a fundamental convolutional neural net. The height and width of the pixel sizes get halved each layer because we are using stride=2. As a result, the total number of activations is decreased by four times.\n\ndef cnn_layers():\n    return [                     # Pixel sizes\n        conv(1, 8, ks=5),        # 14x14\n        conv(8, 16),             # 7x7\n        conv(16, 32),            # 4x4\n        conv(32, 64),            # 2x2\n        conv(64, 10, act=None),  # 1x1\n        nn.Flatten()\n    ]\n\n\ncnn_layers()\n\n[Sequential(\n   (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n   (1): ReLU()\n ),\n Sequential(\n   (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (1): ReLU()\n ),\n Sequential(\n   (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (1): ReLU()\n ),\n Sequential(\n   (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (1): ReLU()\n ),\n Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n Flatten(start_dim=1, end_dim=-1)]\n\n\n\nfrom torcheval.metrics import MulticlassAccuracy\nmetrics = MetricsCB(accuracy=MulticlassAccuracy(device='cpu'))\ncbs = [metrics, TrainCB(), ProgressCB(plot=True), DeviceCB()]\n\n\ndef fit(model, epoch=1, xtr_cbs=[]):\n    learn = MomentumLearner(model, dls, F.cross_entropy, 0.6, cbs=cbs+xtr_cbs)\n    learn.fit(epoch)\n    return learn\n\n\nset_seed(1)\nfit(nn.Sequential(*cnn_layers()))\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAs we can see from the graph, it is not training well. But why does it not train well? To learn more about what’s happening, we can look into the activations of each layer. Ideally, we want each layer to have a mean of 0 and a standard deviation of 1 because that’s when numbers are accurate in computers. If the number gets too small or too big, we lose the accuracy of the values. So we might have activations that are too big or too small in this case.\n\n\n\nLet’s look at each layer’s activations by manually saving the values of each layer.\n\nclass Sequential(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.means = [[] for _ in layers]\n        self.stds = [[] for _ in layers]\n\n    def forward(self, x):\n        for i, l in enumerate(self.layers):\n            x = l(x)\n            self.means[i].append(to_cpu(x).mean())\n            self.stds[i].append(to_cpu(x).std())\n        return x\n\n\nset_seed(1)\nmodel = Sequential(cnn_layers())\nfit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\nWe are only interested in the first five layers because the last layer is flatten.\n\nfor l in model.means:\n    plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\nfor l in model.stds:\n    plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\nSo, we can see that our activations do not have a mean of 0 and a standard deviation of 1. This information is helpful, but we can get this info conveniently by using pytorch hooks.\n\n\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nmeans = [[] for _ in model]\nstds = [[] for _ in model]\nhooks = []\n\n\ndef get_stats(i, mod, inp, outp):\n    x = to_cpu(outp)\n    means[i].append(x.mean())\n    stds[i].append(x.std())\n\nThis is how we use Pytorch hooks. A hook takes module, input, and output as parameters. With get_stats, we also pass i to append stats into the correct one. Forward hooks are functions called after forward computation, just like callbacks.\n\nfor i, l in enumerate(model): hooks.append(l.register_forward_hook(partial(get_stats, i)))\n\nWhen we register a hook, we get a removable handle. We can save these in the hooks list and remove them after we are done. If we do not remove them, they will remain in the memory forever.\n\nhooks\n\n[&lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;]\n\n\n\nfit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nfor m in means: plt.plot(m)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\nfor s in stds: plt.plot(s)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\nfor h in hooks: h.remove()\n\nBy using remove, we can remove hooks.\n\n\n\nUsing global variables to store activation means and standard deviations is not ideal. So, let’s create a Hook class to keep these variables. Also, we can easily remove hooks.\n\nclass Hook:\n    def __init__(self, layer, f): self.hook = layer.register_forward_hook(partial(f, self))\n    def remove(self): self.hook.remove()\n    def __del__(self): self.remove()\n\n\ndef get_stats(hook, mod, inp, outp):\n    if not hasattr(hook, 'stats'): hook.stats = [[], []]\n    x = to_cpu(outp)\n    hook.stats[0].append(x.mean())\n    hook.stats[1].append(x.std())\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nhooks = [Hook(l, get_stats) for l in model]\n\n\nfit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nfor h in hooks:\n    plt.plot(h.stats[0])\n    h.remove()\nplt.legend(range(5));\n\n\n\n\n\n\n\n\nIt’s great that we can store all the stats in each Hook class, but we still have to create hooks as a global variable. We can create a Hooks class for that.\n\n\n\nLet’s create a Hooks class. We will add some features. We will subclass list to use indexing and looping capabilities. Also, we add __enter__ and __exit__ to use a context manager. When done with the context manager, it will automatically remove the hooks to free memory.\n\nclass Hooks(list):\n    def __init__(self, f, model): super().__init__([Hook(l, f) for l in model])\n    def __enter__(self): return self\n    def __exit__(self, *args, **kwargs): self.remove()\n    def remove(self):\n        for h in self: h.remove()\n    def __del__(self): self.remove()\n    def __delitem__(self, i):\n        self[i].remove()\n        super().__delitem__(i)\n\n\nhooks = Hooks(get_stats, model)\nhooks\n\n[&lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;]\n\n\n\ndel hooks[0]\nhooks\n\n[&lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;]\n\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nwith Hooks(get_stats, model) as hooks:\n    fit(model)\n    for h in hooks:\n        plt.plot(h.stats[0])\n    plt.legend(range(5))\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can create a callback version of hooks.\n\nclass HooksCB(Callback):\n    def __init__(self, f):\n        self.hook_fn = f\n\n    def before_fit(self, learn):\n        # setup hooks\n        self.hooks = Hooks(self.hook_fn, learn.model)\n\n    def after_fit(self, learn):\n        # remove hooks\n        for h in self.hooks: h.remove()\n\n\nset_seed(1)\nhookscb = HooksCB(get_stats)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtr_cbs=[hookscb])\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nfor h in hookscb.hooks: plt.plot(h.stats[0])\n\n\n\n\n\n\n\n\n\n\n\nAnother exciting tool we can use to look at stats is a histogram. We create a histogram from activations with 50 bins from 0 to 10. We store this information in each hook as we did with means and standard deviations. By looking at the histogram, we can see how many activations are close to zero.\n\ndef get_stats(hook, mod, inp, outp):\n    if not hasattr(hook, 'stats'): hook.stats = [[], [], []]\n    x = to_cpu(outp)\n    hook.stats[0].append(x.mean())\n    hook.stats[1].append(x.std())\n    hook.stats[2].append(x.abs().histc(50, 0, 10))\n\n\nset_seed(1)\nhookscb = HooksCB(get_stats)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtr_cbs=[hookscb])\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.stack(hookscb.hooks[0].stats[2]).t()\n\ntensor([[1297868., 1294422., 1291560.,  ..., 3211258., 3211260., 2834941.],\n        [ 154261.,  153682.,  155513.,  ...,       6.,       4.,       3.],\n        [  89054.,   91588.,   90978.,  ...,       0.,       0.,       0.],\n        ...,\n        [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n        [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n        [      0.,       0.,       0.,  ...,       0.,       0.,       0.]])\n\n\nBecause the numbers are too big, we apply log1p, the same thing as adding one and taking a log because there are zeros.\n\ntorch.stack(hookscb.hooks[0].stats[2]).t().log1p()\n\ntensor([[14.08, 14.07, 14.07,  ..., 14.98, 14.98, 14.86],\n        [11.95, 11.94, 11.95,  ...,  1.95,  1.61,  1.39],\n        [11.40, 11.43, 11.42,  ...,  0.00,  0.00,  0.00],\n        ...,\n        [ 0.00,  0.00,  0.00,  ...,  0.00,  0.00,  0.00],\n        [ 0.00,  0.00,  0.00,  ...,  0.00,  0.00,  0.00],\n        [ 0.00,  0.00,  0.00,  ...,  0.00,  0.00,  0.00]])\n\n\n\ntorch.stack(hookscb.hooks[0].stats[2]).t().shape\n\ntorch.Size([50, 64])\n\n\n\nhistogram = torch.stack(hookscb.hooks[0].stats[2]).t()\nshow_image(histogram.log1p(), origin='lower', figsize=(4,4));\n\n\n\n\n\n\n\n\nIn the histogram, each pixel other than dark blue indicates some values other than 0. High value indicates yellow and green means low number. From the histogram, almost all activations are zeros starting from the beginning. Then, some activation values spike up couple times and drops to zero. Ideally, we want to get even yellow histogram up to about 10% of the height. That means most of the absolute value of activations are between zero and one, which is a good sign.\nWe can focus on how many absolute value of activations are smaller than 0.2. These are basically dead or almost dead, which means these numbers are not doing anything. We can plot the percentage of them from the total.\n\nhistogram[0], histogram\n\n(tensor([1297868., 1294422., 1291560., 1291628., 1283061., 1273476., 1256431., 1242345., 1212113., 1191507., 1177972., 1177066., 1174616.,\n         1157383., 1168056., 1375630., 1218830., 1208255., 1204717., 1204226., 1228218., 1414853., 1474184., 1477038., 1477273., 1472988.,\n         1477866., 1605349., 1605629., 1605630., 1605629., 1605626., 1605630., 1605629., 1605630., 1605630., 1605631., 1605627., 1605630.,\n         1605630., 1605630., 1605629., 1605631., 1605630., 1605628., 1605628., 1605632., 1605632., 1605629., 1605627., 1605630., 1605627.,\n         1605630., 1605629., 1605631., 1605628., 1605631., 1605630.,  953342., 3211256., 3211258., 3211258., 3211260., 2834941.]),\n tensor([[1297868., 1294422., 1291560.,  ..., 3211258., 3211260., 2834941.],\n         [ 154261.,  153682.,  155513.,  ...,       6.,       4.,       3.],\n         [  89054.,   91588.,   90978.,  ...,       0.,       0.,       0.],\n         ...,\n         [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n         [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n         [      0.,       0.,       0.,  ...,       0.,       0.,       0.]]))\n\n\n\nhistogram.sum(0)\n\ntensor([1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,  953344., 3211264., 3211264., 3211264., 3211264., 2834944.])\n\n\n\nhistogram[0] / histogram.sum(0)\n\ntensor([0.81, 0.81, 0.80, 0.80, 0.80, 0.79, 0.78, 0.77, 0.75, 0.74, 0.73, 0.73, 0.73, 0.72, 0.73, 0.86, 0.76, 0.75, 0.75, 0.75, 0.76, 0.88,\n        0.92, 0.92, 0.92, 0.92, 0.92, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00,\n        1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00])\n\n\n\nplt.plot(histogram[0] / histogram.sum(0));\n\n\n\n\n\n\n\n\nWe can easily look at how bad this is.\n\n\n\nIn this blog, we used Pytorch hooks to look into statistics of activations. They are basically functions with fancy names, like callbacks. Then, we created Hook class and Hooks class to avoid using global variables. We also created a version for a callback. Lastly, we looked at histogram and plots for zeroes. With those tools, we can see how well our models are training later and find out which initializing strategies work the best.",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#baseline",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#baseline",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "We will start using convolutional network.\n\ndef conv(inp, outp, ks=3, stride=2, act=nn.ReLU()):\n    res = nn.Conv2d(inp, outp, ks, stride, padding=ks//2)\n    if act is not None: return nn.Sequential(res, act)\n    return res\n\n\nconv(1, 3)\n\nSequential(\n  (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n\n\n\nconv(1, 3, act=None)\n\nConv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n\ncnn_layers is a fundamental convolutional neural net. The height and width of the pixel sizes get halved each layer because we are using stride=2. As a result, the total number of activations is decreased by four times.\n\ndef cnn_layers():\n    return [                     # Pixel sizes\n        conv(1, 8, ks=5),        # 14x14\n        conv(8, 16),             # 7x7\n        conv(16, 32),            # 4x4\n        conv(32, 64),            # 2x2\n        conv(64, 10, act=None),  # 1x1\n        nn.Flatten()\n    ]\n\n\ncnn_layers()\n\n[Sequential(\n   (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n   (1): ReLU()\n ),\n Sequential(\n   (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (1): ReLU()\n ),\n Sequential(\n   (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (1): ReLU()\n ),\n Sequential(\n   (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (1): ReLU()\n ),\n Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n Flatten(start_dim=1, end_dim=-1)]\n\n\n\nfrom torcheval.metrics import MulticlassAccuracy\nmetrics = MetricsCB(accuracy=MulticlassAccuracy(device='cpu'))\ncbs = [metrics, TrainCB(), ProgressCB(plot=True), DeviceCB()]\n\n\ndef fit(model, epoch=1, xtr_cbs=[]):\n    learn = MomentumLearner(model, dls, F.cross_entropy, 0.6, cbs=cbs+xtr_cbs)\n    learn.fit(epoch)\n    return learn\n\n\nset_seed(1)\nfit(nn.Sequential(*cnn_layers()))\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAs we can see from the graph, it is not training well. But why does it not train well? To learn more about what’s happening, we can look into the activations of each layer. Ideally, we want each layer to have a mean of 0 and a standard deviation of 1 because that’s when numbers are accurate in computers. If the number gets too small or too big, we lose the accuracy of the values. So we might have activations that are too big or too small in this case.",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#manual-hooks",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#manual-hooks",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "Let’s look at each layer’s activations by manually saving the values of each layer.\n\nclass Sequential(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.means = [[] for _ in layers]\n        self.stds = [[] for _ in layers]\n\n    def forward(self, x):\n        for i, l in enumerate(self.layers):\n            x = l(x)\n            self.means[i].append(to_cpu(x).mean())\n            self.stds[i].append(to_cpu(x).std())\n        return x\n\n\nset_seed(1)\nmodel = Sequential(cnn_layers())\nfit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\nWe are only interested in the first five layers because the last layer is flatten.\n\nfor l in model.means:\n    plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\nfor l in model.stds:\n    plt.plot(l)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\nSo, we can see that our activations do not have a mean of 0 and a standard deviation of 1. This information is helpful, but we can get this info conveniently by using pytorch hooks.",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#pytorch-hooks-1",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#pytorch-hooks-1",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "set_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nmeans = [[] for _ in model]\nstds = [[] for _ in model]\nhooks = []\n\n\ndef get_stats(i, mod, inp, outp):\n    x = to_cpu(outp)\n    means[i].append(x.mean())\n    stds[i].append(x.std())\n\nThis is how we use Pytorch hooks. A hook takes module, input, and output as parameters. With get_stats, we also pass i to append stats into the correct one. Forward hooks are functions called after forward computation, just like callbacks.\n\nfor i, l in enumerate(model): hooks.append(l.register_forward_hook(partial(get_stats, i)))\n\nWhen we register a hook, we get a removable handle. We can save these in the hooks list and remove them after we are done. If we do not remove them, they will remain in the memory forever.\n\nhooks\n\n[&lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;,\n &lt;torch.utils.hooks.RemovableHandle&gt;]\n\n\n\nfit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nfor m in means: plt.plot(m)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\nfor s in stds: plt.plot(s)\nplt.legend(range(5));\n\n\n\n\n\n\n\n\n\nfor h in hooks: h.remove()\n\nBy using remove, we can remove hooks.",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#hook-class",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#hook-class",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "Using global variables to store activation means and standard deviations is not ideal. So, let’s create a Hook class to keep these variables. Also, we can easily remove hooks.\n\nclass Hook:\n    def __init__(self, layer, f): self.hook = layer.register_forward_hook(partial(f, self))\n    def remove(self): self.hook.remove()\n    def __del__(self): self.remove()\n\n\ndef get_stats(hook, mod, inp, outp):\n    if not hasattr(hook, 'stats'): hook.stats = [[], []]\n    x = to_cpu(outp)\n    hook.stats[0].append(x.mean())\n    hook.stats[1].append(x.std())\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nhooks = [Hook(l, get_stats) for l in model]\n\n\nfit(model)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nfor h in hooks:\n    plt.plot(h.stats[0])\n    h.remove()\nplt.legend(range(5));\n\n\n\n\n\n\n\n\nIt’s great that we can store all the stats in each Hook class, but we still have to create hooks as a global variable. We can create a Hooks class for that.",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#hooks-class",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#hooks-class",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "Let’s create a Hooks class. We will add some features. We will subclass list to use indexing and looping capabilities. Also, we add __enter__ and __exit__ to use a context manager. When done with the context manager, it will automatically remove the hooks to free memory.\n\nclass Hooks(list):\n    def __init__(self, f, model): super().__init__([Hook(l, f) for l in model])\n    def __enter__(self): return self\n    def __exit__(self, *args, **kwargs): self.remove()\n    def remove(self):\n        for h in self: h.remove()\n    def __del__(self): self.remove()\n    def __delitem__(self, i):\n        self[i].remove()\n        super().__delitem__(i)\n\n\nhooks = Hooks(get_stats, model)\nhooks\n\n[&lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;]\n\n\n\ndel hooks[0]\nhooks\n\n[&lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;,\n &lt;__main__.Hook&gt;]\n\n\n\nset_seed(1)\nmodel = nn.Sequential(*cnn_layers())\n\n\nwith Hooks(get_stats, model) as hooks:\n    fit(model)\n    for h in hooks:\n        plt.plot(h.stats[0])\n    plt.legend(range(5))\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#hookscallback",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#hookscallback",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "We can create a callback version of hooks.\n\nclass HooksCB(Callback):\n    def __init__(self, f):\n        self.hook_fn = f\n\n    def before_fit(self, learn):\n        # setup hooks\n        self.hooks = Hooks(self.hook_fn, learn.model)\n\n    def after_fit(self, learn):\n        # remove hooks\n        for h in self.hooks: h.remove()\n\n\nset_seed(1)\nhookscb = HooksCB(get_stats)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtr_cbs=[hookscb])\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nfor h in hookscb.hooks: plt.plot(h.stats[0])",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#histograms",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#histograms",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "Another exciting tool we can use to look at stats is a histogram. We create a histogram from activations with 50 bins from 0 to 10. We store this information in each hook as we did with means and standard deviations. By looking at the histogram, we can see how many activations are close to zero.\n\ndef get_stats(hook, mod, inp, outp):\n    if not hasattr(hook, 'stats'): hook.stats = [[], [], []]\n    x = to_cpu(outp)\n    hook.stats[0].append(x.mean())\n    hook.stats[1].append(x.std())\n    hook.stats[2].append(x.abs().histc(50, 0, 10))\n\n\nset_seed(1)\nhookscb = HooksCB(get_stats)\nmodel = nn.Sequential(*cnn_layers())\nfit(model, xtr_cbs=[hookscb])\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.143\n2.358\n0\ntrain\n\n\n0.100\n2.304\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.stack(hookscb.hooks[0].stats[2]).t()\n\ntensor([[1297868., 1294422., 1291560.,  ..., 3211258., 3211260., 2834941.],\n        [ 154261.,  153682.,  155513.,  ...,       6.,       4.,       3.],\n        [  89054.,   91588.,   90978.,  ...,       0.,       0.,       0.],\n        ...,\n        [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n        [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n        [      0.,       0.,       0.,  ...,       0.,       0.,       0.]])\n\n\nBecause the numbers are too big, we apply log1p, the same thing as adding one and taking a log because there are zeros.\n\ntorch.stack(hookscb.hooks[0].stats[2]).t().log1p()\n\ntensor([[14.08, 14.07, 14.07,  ..., 14.98, 14.98, 14.86],\n        [11.95, 11.94, 11.95,  ...,  1.95,  1.61,  1.39],\n        [11.40, 11.43, 11.42,  ...,  0.00,  0.00,  0.00],\n        ...,\n        [ 0.00,  0.00,  0.00,  ...,  0.00,  0.00,  0.00],\n        [ 0.00,  0.00,  0.00,  ...,  0.00,  0.00,  0.00],\n        [ 0.00,  0.00,  0.00,  ...,  0.00,  0.00,  0.00]])\n\n\n\ntorch.stack(hookscb.hooks[0].stats[2]).t().shape\n\ntorch.Size([50, 64])\n\n\n\nhistogram = torch.stack(hookscb.hooks[0].stats[2]).t()\nshow_image(histogram.log1p(), origin='lower', figsize=(4,4));\n\n\n\n\n\n\n\n\nIn the histogram, each pixel other than dark blue indicates some values other than 0. High value indicates yellow and green means low number. From the histogram, almost all activations are zeros starting from the beginning. Then, some activation values spike up couple times and drops to zero. Ideally, we want to get even yellow histogram up to about 10% of the height. That means most of the absolute value of activations are between zero and one, which is a good sign.\nWe can focus on how many absolute value of activations are smaller than 0.2. These are basically dead or almost dead, which means these numbers are not doing anything. We can plot the percentage of them from the total.\n\nhistogram[0], histogram\n\n(tensor([1297868., 1294422., 1291560., 1291628., 1283061., 1273476., 1256431., 1242345., 1212113., 1191507., 1177972., 1177066., 1174616.,\n         1157383., 1168056., 1375630., 1218830., 1208255., 1204717., 1204226., 1228218., 1414853., 1474184., 1477038., 1477273., 1472988.,\n         1477866., 1605349., 1605629., 1605630., 1605629., 1605626., 1605630., 1605629., 1605630., 1605630., 1605631., 1605627., 1605630.,\n         1605630., 1605630., 1605629., 1605631., 1605630., 1605628., 1605628., 1605632., 1605632., 1605629., 1605627., 1605630., 1605627.,\n         1605630., 1605629., 1605631., 1605628., 1605631., 1605630.,  953342., 3211256., 3211258., 3211258., 3211260., 2834941.]),\n tensor([[1297868., 1294422., 1291560.,  ..., 3211258., 3211260., 2834941.],\n         [ 154261.,  153682.,  155513.,  ...,       6.,       4.,       3.],\n         [  89054.,   91588.,   90978.,  ...,       0.,       0.,       0.],\n         ...,\n         [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n         [      0.,       0.,       0.,  ...,       0.,       0.,       0.],\n         [      0.,       0.,       0.,  ...,       0.,       0.,       0.]]))\n\n\n\nhistogram.sum(0)\n\ntensor([1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,\n        1605632., 1605632., 1605632., 1605632., 1605632., 1605632.,  953344., 3211264., 3211264., 3211264., 3211264., 2834944.])\n\n\n\nhistogram[0] / histogram.sum(0)\n\ntensor([0.81, 0.81, 0.80, 0.80, 0.80, 0.79, 0.78, 0.77, 0.75, 0.74, 0.73, 0.73, 0.73, 0.72, 0.73, 0.86, 0.76, 0.75, 0.75, 0.75, 0.76, 0.88,\n        0.92, 0.92, 0.92, 0.92, 0.92, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00,\n        1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00])\n\n\n\nplt.plot(histogram[0] / histogram.sum(0));\n\n\n\n\n\n\n\n\nWe can easily look at how bad this is.",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#conclusion",
    "href": "blog/posts/2023-09-26-Pytorch_Hooks/index.html#conclusion",
    "title": "Pytorch Hooks",
    "section": "",
    "text": "In this blog, we used Pytorch hooks to look into statistics of activations. They are basically functions with fancy names, like callbacks. Then, we created Hook class and Hooks class to avoid using global variables. We also created a version for a callback. Lastly, we looked at histogram and plots for zeroes. With those tools, we can see how well our models are training later and find out which initializing strategies work the best.",
    "crumbs": [
      "Home",
      "Blog",
      "Pytorch Hooks"
    ]
  },
  {
    "objectID": "blog/posts/2025-10-26-why-build-llm/index.html",
    "href": "blog/posts/2025-10-26-why-build-llm/index.html",
    "title": "Why Build a Language Model from Scratch?",
    "section": "",
    "text": "Large language models are very useful. They help me write code, learn new topics, brainstorm ideas, and more. The possibilities are endless. How did they become so powerful and useful, and yet so dumb sometimes?\nI started learning to build large language models from scratch at the beginning of this year, and it was very fun. In this blog, I want to talk about whether you should learn to build LLMs from scratch and share my experience.",
    "crumbs": [
      "Home",
      "Blog",
      "Why Build a Language Model from Scratch?"
    ]
  },
  {
    "objectID": "blog/posts/2025-10-26-why-build-llm/index.html#introduction",
    "href": "blog/posts/2025-10-26-why-build-llm/index.html#introduction",
    "title": "Why Build a Language Model from Scratch?",
    "section": "",
    "text": "Large language models are very useful. They help me write code, learn new topics, brainstorm ideas, and more. The possibilities are endless. How did they become so powerful and useful, and yet so dumb sometimes?\nI started learning to build large language models from scratch at the beginning of this year, and it was very fun. In this blog, I want to talk about whether you should learn to build LLMs from scratch and share my experience.",
    "crumbs": [
      "Home",
      "Blog",
      "Why Build a Language Model from Scratch?"
    ]
  },
  {
    "objectID": "blog/posts/2025-10-26-why-build-llm/index.html#why-build-from-scratch",
    "href": "blog/posts/2025-10-26-why-build-llm/index.html#why-build-from-scratch",
    "title": "Why Build a Language Model from Scratch?",
    "section": "Why Build from Scratch?",
    "text": "Why Build from Scratch?\nThe main reason I built language models from scratch is because I was curious, and it was fun. I enjoyed the process of understanding how things worked and seeing my code come to life. When I fed my model programming data and watched it start generating code outputs, I felt excited. I only used Python data to train, and the model always responded with code even when I just said hi! If you love building things, you would love building LLMs.\nBut there are practical benefits too. Understanding how language models work internally makes you a better practitioner. It’s like how pilots learn about aerodynamics. They don’t need to design airplanes, but knowing how lift and drag work helps them fly better. Understanding autoregressive decoding helps you know why generation is slow and why KV cache speeds it up. When someone claims a new technique will revolutionize everything, you can critically evaluate whether it makes sense. Understanding what kind of training process the language model went through and what data were used can also be helpful when you are using the model because you have a sense of the model’s strengths and weaknesses. If you want to become a better practitioner, learning how to write LLMs from scratch will provide an advantage over those who don’t.\nBuilding from scratch also teaches you a meta-skill that applies beyond language models. You learn how to tackle any complex, daunting task by breaking it down into smaller pieces. This approach works for learning the piano, for understanding DNA sequencing, for any challenging goal. Start with fundamentals, expect struggle, build incrementally. Learning about language models is the same. Breaking the training steps into tokenization, pretraining, supervised training, and reinforcement learning helps you focus on one topic at a time. When learning about model architecture, breaking it down into embedding layers, feedforward layers, attention layers, normalization layers and so forth makes it easier.\nHowever, after understanding each piece, it might be difficult to make connections between parts. For me, the attention mechanism was the difficult part. After understanding query, key, and values and how they worked in code, I still didn’t understand why they worked conceptually together as a whole model. I took my time and came back to it several times. I talked to AI and people in study group to solidify my understanding. Then I implemented it from scratch multiple times with different types of attention like multi-head attention and group query attention. It finally clicked for me when I implemented KV cache. Every time I came back, I learned more and understood it more deeply. If you want to learn about tackling complex tasks, give it a try!",
    "crumbs": [
      "Home",
      "Blog",
      "Why Build a Language Model from Scratch?"
    ]
  },
  {
    "objectID": "blog/posts/2025-10-26-why-build-llm/index.html#my-journey",
    "href": "blog/posts/2025-10-26-why-build-llm/index.html#my-journey",
    "title": "Why Build a Language Model from Scratch?",
    "section": "My Journey",
    "text": "My Journey\nI started learning to build language models in early 2025 by reading “Build a Large Language Model from Scratch” by Sebastian Raschka with a study group. Learning with others was much more fun than studying alone. I could ask questions, talk about cool ideas, and bounce thoughts off people who were going through the same struggles.\nOur study group had a hackathon where we trained a small model on the TinyStories dataset. I wanted to build the GPT-2 model from scratch on top of miniai, which is a framework we built from part 2 of fastai’s course. By working on small pieces, I got the model to train. It was exciting to look at the stories the model generated. The loss went down as expected, and I felt proud. Maybe too proud. Then I wanted to improve it with ModernBERT sequence packing to make training faster. I was excited about this technique and wanted to use it immediately. But I added it too fast without really understanding how it worked. The loss stopped going down around 5 when I expected it to reach 2 or lower. I was frustrated.\nI had made too many changes at once without testing along the way. I didn’t understand how they all fit together. The code became a mess, and the project failed. But it was a good learning experience. I was able to confirm the lesson from solveit course was legit: make small steps and test fast. I had done the opposite and lost control. I learned that taking shortcuts doesn’t work when the real goal is learning.\nThis failure prepared me for my next project with Local Research Group (LRG). We built SmolLM2 from scratch to compare full fine-tuning, LoRA, DoRA, and rsLoRA based on the “LoRA Learns Less and Forgets Less” paper. This time, I took it slowly. I broke down the problem, understood each component, and tested everything before moving to the next piece. I added unpadding and incorporated sequence packing on top of the llm-foundry framework. It took a lot of work, but when it finally worked and achieved around 80,000 tokens per second (More than 3x faster!), I was genuinely happy.\n\n\n\nUnpadding visualization\n\n\nWeights and bias logging for tokens per sec for the SmolLM2 135M running on RTX 3090 training on avelina python edu dataset. Each batch of the data got padded to match the longest sequence of the batch. The green line uses the standard Hugging Face model with sdpa attention and trains with padding tokens. The red line uses unpadding to remove padding tokens to save time and compute. Because padding tokens are removed, more GPU RAM was available. I was able to increase the micro batch size to train faster, which is indicated by the gray line.\n\n\n\nSequence packing efficiency visualization\n\n\n\n\n\nSequence packing tps visualization\n\n\nWith MondernBert sequence packing and playing with different torch compile options, the throughput went up to around 80k tokens per sec. This was super awesome!!",
    "crumbs": [
      "Home",
      "Blog",
      "Why Build a Language Model from Scratch?"
    ]
  },
  {
    "objectID": "blog/posts/2025-10-26-why-build-llm/index.html#how-to-get-started",
    "href": "blog/posts/2025-10-26-why-build-llm/index.html#how-to-get-started",
    "title": "Why Build a Language Model from Scratch?",
    "section": "How to Get Started",
    "text": "How to Get Started\nIf you want to build language models from scratch, start small and slow. I started by learning basic Python, took fastai courses, read Sebastian Raschka’s book with a study group, watched Andrej Karpathy’s courses, and worked on projects. The courses provided a basic understanding of concepts piece by piece. When I worked on projects and hackathons, I learned concepts more in depth as needed to solve problems. Before starting projects, I did not go into too much detail into concepts. For instance, I didn’t learn all of object-oriented programming when learning Python and jumped into fastai courses. In the course, when I had to use inheritance to create more callbacks, I learned that.\nWhen working on a project, I recommend building incrementally with small steps and checking what’s going on at each step. When I was building SmolLM2, I used the Hugging Face implementation as a reference. After implementing a layer, such as a normalization layer, I checked the output to match the reference. It was surprising to find out that changing the data type from bfloat16 to float16 changed the output. Some layers changed data types within the layer, which was interesting too. And when I learned that using Flash Attention, SDPA, and hand-written attention all had different outputs, I was amazed. I thought only speed and memory usage mattered. It would have been really difficult to find these distinctions without working in small pieces.\nThis goes back to the lesson I learned from the hackathon failure, and it was also a lesson from the solveit course. In the course, we learned to write small pieces and check the output. It seems like it would take longer because we write small pieces at a time when AI can write so much code at once. And checking output each time? But it’s a much better way to understand what is working and what is not at the earliest stage. When training a model, if I get an error, I can fix it. But if there is no error when training a model and the loss does not go down, it is a nightmare, especially when you have no idea what changes you made to the code. Therefore, it is better to check the output at each step. It is a much better way to learn and build a model.\nWhen working on a project, focus on learning as well as the product. I noticed when I focused too much on the product, I rushed to add features without understanding them. When I slowed down and focused on learning, I understood the concepts better and was able to add features more effectively. I learned about group query attention, Flash Attention, rotary embedding, RMSNorm, KV cache, sequence packing, and more in depth before implementing them into the project. It was awesome to learn these, and it was very helpful when debugging as well. Here are some solveit dialogues I had a lot of fun when learning RoPE and attention.",
    "crumbs": [
      "Home",
      "Blog",
      "Why Build a Language Model from Scratch?"
    ]
  },
  {
    "objectID": "blog/posts/2025-10-26-why-build-llm/index.html#conclusion",
    "href": "blog/posts/2025-10-26-why-build-llm/index.html#conclusion",
    "title": "Why Build a Language Model from Scratch?",
    "section": "Conclusion",
    "text": "Conclusion\nBuilding language models from scratch has been one of the most rewarding experiences in my learning journey. It taught me how these powerful systems work, made me a better practitioner, and showed me how to tackle complex problems by breaking them down into small pieces. But most importantly, it was genuinely fun.\nIf you enjoy building things and solving problems, I think you would enjoy building language models too. You don’t need to become a researcher or work at an AI lab. The process itself is valuable. You might discover it’s something you love doing, and that’s reason enough to try.\nStart small, build incrementally, and focus on learning rather than just getting results. Find a study group or community to learn with. Come back to difficult concepts multiple times. And remember, the struggle is where the learning happens.\nIf this sounds interesting to you, give it a try. You might be surprised by how much you enjoy it.",
    "crumbs": [
      "Home",
      "Blog",
      "Why Build a Language Model from Scratch?"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-23-Convolutions/index.html",
    "href": "blog/posts/2023-09-23-Convolutions/index.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Let’s learn about convolutional neural networks (CNN). Rather than using linear layers, we mostly use CNN on images. Compared to using linear layers, we can get more accuracy fast because CNN has less parameters to train and can capture patterns easily.\nFirst, we setup environments. For data, we will use MNIST digits.\n\nfrom torch import tensor, nn\nimport pickle, gzip, torch, matplotlib as mpl\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\npath_data = Path('data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\n\n\nLet’s go over how Pytroch’s convolutional neural networks work. We will start with a simple example, which is Conv1d. Let’s say we have a vector with shape (1, 2). We can think of this vector as a sentence with two words, like “I sit,” and converted numbers into numbers so that computers can understand.\n\ntorch.manual_seed(1)\nsent1 = torch.randn((1, 2))\nsent1\n\ntensor([[0.66, 0.27]])\n\n\nHere is a Conv1d layer with 1 input, 1 output, 2 kernel_size. When we look at the weight of this layer, it is the kernels. Its shape corresponds to (output, input, kernel_size). In this simple case, we have one kernel with size 2 kernel. The shape of bias depends on the output of the layer.\n\nlayer1 = nn.Conv1d(1, 1, kernel_size=2)\nlayer1\n\nConv1d(1, 1, kernel_size=(2,), stride=(1,))\n\n\n\nlayer1.weight, layer1.weight.shape\n\n(Parameter containing:\n tensor([[[-0.67,  0.42]]], requires_grad=True),\n torch.Size([1, 1, 2]))\n\n\n\nlayer1.bias\n\nParameter containing:\ntensor([-0.15], requires_grad=True)\n\n\nWhen we apply this layer into our sentence, we get -0.26, which is the same as calculating a dot product of weight and the sentence added by bias.\n\nlayer1(sent1)\n\ntensor([[-0.47]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n(layer1.weight * sent1).sum() + layer1.bias\n\ntensor([-0.47], grad_fn=&lt;AddBackward0&gt;)\n\n\nWhen we set the output to 2, that means there are two kernels: one for each output. We generally refer to output as channel. So, when we increae the ouput, we say we increase the channel. We will talk about why it would be useful later.\n\nlayer2 = nn.Conv1d(1, 2, kernel_size=2)\nlayer2\n\nConv1d(1, 2, kernel_size=(2,), stride=(1,))\n\n\n\nlayer2.weight, layer2.weight.shape\n\n(Parameter containing:\n tensor([[[ 0.36,  0.10]],\n \n         [[-0.09,  0.20]]], requires_grad=True),\n torch.Size([2, 1, 2]))\n\n\n\nlayer2.bias\n\nParameter containing:\ntensor([0.03, 0.26], requires_grad=True)\n\n\n\nlayer2(sent1)\n\ntensor([[0.30],\n        [0.25]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n(layer2.weight[0] * sent1).sum() + layer2.bias[0]\n\ntensor(0.30, grad_fn=&lt;AddBackward0&gt;)\n\n\n\n(layer2.weight[1] * sent1).sum() + layer2.bias[1]\n\ntensor(0.25, grad_fn=&lt;AddBackward0&gt;)\n\n\nThat was simple. What if sentence is longer than 2 words? Let’s say we have 4 words this time. When we apply layer1 to it, we get 3 numbers.\n\nsent2 = torch.randn((1, 4))\nsent2\n\ntensor([[-1.33,  0.52,  0.75, -0.08]])\n\n\n\nlayer1(sent2)\n\ntensor([[ 0.96, -0.17, -0.67]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nWe can imagine our kernel as a sliding window moving from left to right. That’s what stride is. By default it is 1, and it moves 1 step by step. This is how it is calculated:\n\nfor words in [sent2[0][i:i+2] for i in range(len(sent2[0]) - 1)]:\n    print((layer1.weight * words).sum() + layer1.bias)\n\ntensor([0.96], grad_fn=&lt;AddBackward0&gt;)\ntensor([-0.17], grad_fn=&lt;AddBackward0&gt;)\ntensor([-0.67], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nSo that’s basically how convolution works. By setting different values to stride, we can change how many times the kernel moves each step. By setting stride to 2, we skip some computations.\n\nlayer3 = nn.Conv1d(1, 1, kernel_size=2, stride=2)\nlayer3\n\nConv1d(1, 1, kernel_size=(2,), stride=(2,))\n\n\n\nlayer3(sent2)\n\ntensor([[ 0.20, -0.61]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nBecause the kernel moved twice each step, it is now instead of 3 values, we have 2 values as an output. This way, we can reduce the number of pixels of the input, which means we can store information more compactly like zipping a file. However, this might actually lose data if we shrink too much. Therefore, we increase the channel of the input when we use stride to balance it out.\nFor instance, if we use stride 2, we half height and width, which results in 4 times less pixels. By adding a channel, we increase the output by 2 times. Therefore, we only divide the pixel by 2 each layer, which is reasonable for neural net to learn patterns.\nWhat is a padding then? It is a way to put zeroes in the corner. If we put padding as 2, we put 2 zeroes on the left and right of our sentence before applying kernels. Therefore, first and last values are equal to just biases in this example.\n\nsent2\n\ntensor([[-1.33,  0.52,  0.75, -0.08]])\n\n\n\nlayer4 = nn.Conv1d(1, 1, kernel_size=2, stride=2, padding=2)\nlayer4\n\nConv1d(1, 1, kernel_size=(2,), stride=(2,), padding=(2,))\n\n\n\nlayer4(sent2)\n\ntensor([[0.42, 0.74, 0.25, 0.42]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\nlayer4.bias\n\nParameter containing:\ntensor([0.42], requires_grad=True)\n\n\n\n\n\n\nWe’ve been using conv1d, but conv2d works the same way except we have 2 dimensional kernels instead of 1d, and they also move down.\n\nlayer5 = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0)\nlayer5\n\nConv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))\n\n\n\nlayer5.weight.shape, layer5.bias.shape\n\n(torch.Size([1, 1, 2, 2]), torch.Size([1]))\n\n\nShape of weight: (Outp, Inp, *Kernel_size)\nShape of bias: Outp\nLet’s start with a simple example again. Here is a small image with 3 pixel high and 3 pixel wide.\n\nim1 = torch.randn((1, 3, 3))\nim1\n\ntensor([[[-0.42, -0.51, -1.57],\n         [-0.12,  3.59, -1.83],\n         [ 1.60, -1.28,  0.33]]])\n\n\n\nim1.shape\n\ntorch.Size([1, 3, 3])\n\n\n\nlayer5(im1), layer5(im1).shape\n\n(tensor([[[ 1.07,  1.96],\n          [-1.00,  1.99]]], grad_fn=&lt;SqueezeBackward1&gt;),\n torch.Size([1, 2, 2]))\n\n\nSo, now let’s talk about different kernels. Why do we even use kernels? What do they do? So, let’s say we have an image of digit 5 from MNIST.\n\ndef show_im(im, figsize=None):\n    fig, ax = plt.subplots(figsize=figsize)\n    if all(hasattr(im, x) for x in ('cpu','permute','detach')):\n        im = im.detach().cpu()\n        if len(im.shape)==3 and im.shape[0]&lt;5: im=im.permute(1,2,0)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(im)\n\n\nim2 = x_train[0].reshape((1, 28, 28))\nshow_im(im2, figsize=(3,3))\n\n\n\n\n\n\n\n\n\nlayer6 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0)\nlayer6, layer6.weight.shape\n\n(Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)), torch.Size([1, 1, 3, 3]))\n\n\nHere is a kernel that detects the top edge. We can apply this kernel to find top edge from an image. Let’s see what it looks like.\n\ntop_edge = torch.nn.Parameter(tensor([[1,1,1],[0,0,0],[-1,-1,-1]],\n                                     dtype=torch.float32).reshape((1, 1, 3, 3)))\ntop_edge\n\nParameter containing:\ntensor([[[[ 1.,  1.,  1.],\n          [ 0.,  0.,  0.],\n          [-1., -1., -1.]]]], requires_grad=True)\n\n\n\ntop_edge.shape\n\ntorch.Size([1, 1, 3, 3])\n\n\n\nlayer6.weight = top_edge\nlayer6.weight\n\nParameter containing:\ntensor([[[[ 1.,  1.,  1.],\n          [ 0.,  0.,  0.],\n          [-1., -1., -1.]]]], requires_grad=True)\n\n\n\nshow_im(layer6(im2), figsize=(3,3));\n\n\n\n\n\n\n\n\nWe successfully detected top edge of the image! Let’s try to get the right edge.\n\nlayer7 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0)\nlayer7\n\nConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n\n\n\nright_edge = torch.nn.Parameter(tensor([[-1,0,1],\n       [-1,0,1],\n       [-1,0,1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0))\nright_edge\n\nParameter containing:\ntensor([[[[-1.,  0.,  1.],\n          [-1.,  0.,  1.],\n          [-1.,  0.,  1.]]]], requires_grad=True)\n\n\n\nlayer7.weight = right_edge\nshow_im(layer7(im2), figsize=(3,3));\n\n\n\n\n\n\n\n\n\nlayer00 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0)\nlayer00\n\nConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n\n\n\nno_op = torch.nn.Parameter(tensor([[1,1,1],[1,1,1],[1,1,1]],\n                                     dtype=torch.float32).reshape((1, 1, 3, 3)))\nno_op\n\nParameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\n\n\n\nshow_im(im2, figsize=(3,3))\n\n\n\n\n\n\n\n\n\nlayer00.weight = no_op\nshow_im(layer00(im2), figsize=(3,3));\n\n\n\n\n\n\n\n\nInstead of creating two layers, we usually create multiple kernels. layer8 has two output channels, which means two kernels. We can set these two kernels into what we had, and we can check the output.\n\nlayer8 = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=0)\nlayer8.weight\n\nParameter containing:\ntensor([[[[ 0.16,  0.02,  0.11],\n          [ 0.07,  0.12,  0.17],\n          [-0.31,  0.17, -0.23]]],\n\n\n        [[[-0.25,  0.02, -0.06],\n          [ 0.20, -0.19, -0.30],\n          [ 0.24, -0.05,  0.19]]]], requires_grad=True)\n\n\n\nlayer8.weight.shape\n\ntorch.Size([2, 1, 3, 3])\n\n\n\ntop_edge.shape\n\ntorch.Size([1, 1, 3, 3])\n\n\n\nlayer8.weight = torch.nn.Parameter(torch.cat([top_edge, right_edge]))\nlayer8.weight\n\nParameter containing:\ntensor([[[[ 1.,  1.,  1.],\n          [ 0.,  0.,  0.],\n          [-1., -1., -1.]]],\n\n\n        [[[-1.,  0.,  1.],\n          [-1.,  0.,  1.],\n          [-1.,  0.,  1.]]]], requires_grad=True)\n\n\n\ndef show_ims(ims, figsize=None):\n    fig, ax = plt.subplots(1, len(ims), figsize=figsize)\n    for i, im in enumerate(ims):\n        if all(hasattr(im, x) for x in ('cpu','permute','detach')):\n            im = im.detach().cpu()\n            if len(im.shape)==3 and im.shape[0]&lt;5: im=im.permute(1,2,0)\n            ax[i].imshow(im)\n            ax[i].set_xticks([])\n            ax[i].set_yticks([])\n\n\nshow_ims([im3, im4])\n\n\n\n\n\n\n\n\nNice. We got the same output as before.\n\nlayer9 = nn.Conv2d(2, 1, kernel_size=3, stride=1, padding=0)\nlayer9.weight.shape\n\ntorch.Size([1, 2, 3, 3])\n\n\n\nlayer9.weight\n\nParameter containing:\ntensor([[[[ 0.05,  0.06, -0.16],\n          [-0.11,  0.08,  0.04],\n          [-0.10, -0.07,  0.22]],\n\n         [[-0.04,  0.13,  0.10],\n          [-0.15, -0.20,  0.23],\n          [ 0.01,  0.16,  0.05]]]], requires_grad=True)\n\n\n\nno_op = torch.nn.Parameter(tensor([[1,1,1],[1,1,1],[1,1,1]],\n                                     dtype=torch.float32).reshape((3, 3)))\nno_op\n\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\n\n\n\ntorch.stack([no_op, no_op]).unsqueeze(0).shape\n\ntorch.Size([1, 2, 3, 3])\n\n\n\nno_ops = torch.nn.Parameter(torch.stack([no_op, no_op]).unsqueeze(0))\nno_ops\n\nParameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]],\n\n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\n\n\n\ntorch.stack([im3, im4]).shape\n\ntorch.Size([2, 26, 26])\n\n\n\nlayer9.weight = no_ops\nshow_im(layer9(torch.stack([im3, im4])), figsize=((3,3)))\n\n\n\n\n\n\n\n\nSo, we combined the output of two images and mixed them into one. In this image, the darkest spots are places where top corner and right corner both happen. This way, the convolutional neural network can combine different features into a more useful one.\n\n\n\nIn this blog, we learned how convolutional neural net works by using Pytorch. It is used mostly for images. If you want to learn more, take a look at FastAI lesson 8. It might be easier to watch the video to undertstand, and you can come back to play around with the code.\nFor a challenge, you can try to train MNIST using both linear layers and convolutional neural networks, and try to find out which one is better.",
    "crumbs": [
      "Home",
      "Blog",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-23-Convolutions/index.html#conv1d",
    "href": "blog/posts/2023-09-23-Convolutions/index.html#conv1d",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Let’s go over how Pytroch’s convolutional neural networks work. We will start with a simple example, which is Conv1d. Let’s say we have a vector with shape (1, 2). We can think of this vector as a sentence with two words, like “I sit,” and converted numbers into numbers so that computers can understand.\n\ntorch.manual_seed(1)\nsent1 = torch.randn((1, 2))\nsent1\n\ntensor([[0.66, 0.27]])\n\n\nHere is a Conv1d layer with 1 input, 1 output, 2 kernel_size. When we look at the weight of this layer, it is the kernels. Its shape corresponds to (output, input, kernel_size). In this simple case, we have one kernel with size 2 kernel. The shape of bias depends on the output of the layer.\n\nlayer1 = nn.Conv1d(1, 1, kernel_size=2)\nlayer1\n\nConv1d(1, 1, kernel_size=(2,), stride=(1,))\n\n\n\nlayer1.weight, layer1.weight.shape\n\n(Parameter containing:\n tensor([[[-0.67,  0.42]]], requires_grad=True),\n torch.Size([1, 1, 2]))\n\n\n\nlayer1.bias\n\nParameter containing:\ntensor([-0.15], requires_grad=True)\n\n\nWhen we apply this layer into our sentence, we get -0.26, which is the same as calculating a dot product of weight and the sentence added by bias.\n\nlayer1(sent1)\n\ntensor([[-0.47]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n(layer1.weight * sent1).sum() + layer1.bias\n\ntensor([-0.47], grad_fn=&lt;AddBackward0&gt;)\n\n\nWhen we set the output to 2, that means there are two kernels: one for each output. We generally refer to output as channel. So, when we increae the ouput, we say we increase the channel. We will talk about why it would be useful later.\n\nlayer2 = nn.Conv1d(1, 2, kernel_size=2)\nlayer2\n\nConv1d(1, 2, kernel_size=(2,), stride=(1,))\n\n\n\nlayer2.weight, layer2.weight.shape\n\n(Parameter containing:\n tensor([[[ 0.36,  0.10]],\n \n         [[-0.09,  0.20]]], requires_grad=True),\n torch.Size([2, 1, 2]))\n\n\n\nlayer2.bias\n\nParameter containing:\ntensor([0.03, 0.26], requires_grad=True)\n\n\n\nlayer2(sent1)\n\ntensor([[0.30],\n        [0.25]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n(layer2.weight[0] * sent1).sum() + layer2.bias[0]\n\ntensor(0.30, grad_fn=&lt;AddBackward0&gt;)\n\n\n\n(layer2.weight[1] * sent1).sum() + layer2.bias[1]\n\ntensor(0.25, grad_fn=&lt;AddBackward0&gt;)\n\n\nThat was simple. What if sentence is longer than 2 words? Let’s say we have 4 words this time. When we apply layer1 to it, we get 3 numbers.\n\nsent2 = torch.randn((1, 4))\nsent2\n\ntensor([[-1.33,  0.52,  0.75, -0.08]])\n\n\n\nlayer1(sent2)\n\ntensor([[ 0.96, -0.17, -0.67]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nWe can imagine our kernel as a sliding window moving from left to right. That’s what stride is. By default it is 1, and it moves 1 step by step. This is how it is calculated:\n\nfor words in [sent2[0][i:i+2] for i in range(len(sent2[0]) - 1)]:\n    print((layer1.weight * words).sum() + layer1.bias)\n\ntensor([0.96], grad_fn=&lt;AddBackward0&gt;)\ntensor([-0.17], grad_fn=&lt;AddBackward0&gt;)\ntensor([-0.67], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nSo that’s basically how convolution works. By setting different values to stride, we can change how many times the kernel moves each step. By setting stride to 2, we skip some computations.\n\nlayer3 = nn.Conv1d(1, 1, kernel_size=2, stride=2)\nlayer3\n\nConv1d(1, 1, kernel_size=(2,), stride=(2,))\n\n\n\nlayer3(sent2)\n\ntensor([[ 0.20, -0.61]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nBecause the kernel moved twice each step, it is now instead of 3 values, we have 2 values as an output. This way, we can reduce the number of pixels of the input, which means we can store information more compactly like zipping a file. However, this might actually lose data if we shrink too much. Therefore, we increase the channel of the input when we use stride to balance it out.\nFor instance, if we use stride 2, we half height and width, which results in 4 times less pixels. By adding a channel, we increase the output by 2 times. Therefore, we only divide the pixel by 2 each layer, which is reasonable for neural net to learn patterns.\nWhat is a padding then? It is a way to put zeroes in the corner. If we put padding as 2, we put 2 zeroes on the left and right of our sentence before applying kernels. Therefore, first and last values are equal to just biases in this example.\n\nsent2\n\ntensor([[-1.33,  0.52,  0.75, -0.08]])\n\n\n\nlayer4 = nn.Conv1d(1, 1, kernel_size=2, stride=2, padding=2)\nlayer4\n\nConv1d(1, 1, kernel_size=(2,), stride=(2,), padding=(2,))\n\n\n\nlayer4(sent2)\n\ntensor([[0.42, 0.74, 0.25, 0.42]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\nlayer4.bias\n\nParameter containing:\ntensor([0.42], requires_grad=True)",
    "crumbs": [
      "Home",
      "Blog",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-23-Convolutions/index.html#conv2d",
    "href": "blog/posts/2023-09-23-Convolutions/index.html#conv2d",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "We’ve been using conv1d, but conv2d works the same way except we have 2 dimensional kernels instead of 1d, and they also move down.\n\nlayer5 = nn.Conv2d(1, 1, kernel_size=2, stride=1, padding=0)\nlayer5\n\nConv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))\n\n\n\nlayer5.weight.shape, layer5.bias.shape\n\n(torch.Size([1, 1, 2, 2]), torch.Size([1]))\n\n\nShape of weight: (Outp, Inp, *Kernel_size)\nShape of bias: Outp\nLet’s start with a simple example again. Here is a small image with 3 pixel high and 3 pixel wide.\n\nim1 = torch.randn((1, 3, 3))\nim1\n\ntensor([[[-0.42, -0.51, -1.57],\n         [-0.12,  3.59, -1.83],\n         [ 1.60, -1.28,  0.33]]])\n\n\n\nim1.shape\n\ntorch.Size([1, 3, 3])\n\n\n\nlayer5(im1), layer5(im1).shape\n\n(tensor([[[ 1.07,  1.96],\n          [-1.00,  1.99]]], grad_fn=&lt;SqueezeBackward1&gt;),\n torch.Size([1, 2, 2]))\n\n\nSo, now let’s talk about different kernels. Why do we even use kernels? What do they do? So, let’s say we have an image of digit 5 from MNIST.\n\ndef show_im(im, figsize=None):\n    fig, ax = plt.subplots(figsize=figsize)\n    if all(hasattr(im, x) for x in ('cpu','permute','detach')):\n        im = im.detach().cpu()\n        if len(im.shape)==3 and im.shape[0]&lt;5: im=im.permute(1,2,0)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(im)\n\n\nim2 = x_train[0].reshape((1, 28, 28))\nshow_im(im2, figsize=(3,3))\n\n\n\n\n\n\n\n\n\nlayer6 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0)\nlayer6, layer6.weight.shape\n\n(Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)), torch.Size([1, 1, 3, 3]))\n\n\nHere is a kernel that detects the top edge. We can apply this kernel to find top edge from an image. Let’s see what it looks like.\n\ntop_edge = torch.nn.Parameter(tensor([[1,1,1],[0,0,0],[-1,-1,-1]],\n                                     dtype=torch.float32).reshape((1, 1, 3, 3)))\ntop_edge\n\nParameter containing:\ntensor([[[[ 1.,  1.,  1.],\n          [ 0.,  0.,  0.],\n          [-1., -1., -1.]]]], requires_grad=True)\n\n\n\ntop_edge.shape\n\ntorch.Size([1, 1, 3, 3])\n\n\n\nlayer6.weight = top_edge\nlayer6.weight\n\nParameter containing:\ntensor([[[[ 1.,  1.,  1.],\n          [ 0.,  0.,  0.],\n          [-1., -1., -1.]]]], requires_grad=True)\n\n\n\nshow_im(layer6(im2), figsize=(3,3));\n\n\n\n\n\n\n\n\nWe successfully detected top edge of the image! Let’s try to get the right edge.\n\nlayer7 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0)\nlayer7\n\nConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n\n\n\nright_edge = torch.nn.Parameter(tensor([[-1,0,1],\n       [-1,0,1],\n       [-1,0,1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0))\nright_edge\n\nParameter containing:\ntensor([[[[-1.,  0.,  1.],\n          [-1.,  0.,  1.],\n          [-1.,  0.,  1.]]]], requires_grad=True)\n\n\n\nlayer7.weight = right_edge\nshow_im(layer7(im2), figsize=(3,3));\n\n\n\n\n\n\n\n\n\nlayer00 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0)\nlayer00\n\nConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n\n\n\nno_op = torch.nn.Parameter(tensor([[1,1,1],[1,1,1],[1,1,1]],\n                                     dtype=torch.float32).reshape((1, 1, 3, 3)))\nno_op\n\nParameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\n\n\n\nshow_im(im2, figsize=(3,3))\n\n\n\n\n\n\n\n\n\nlayer00.weight = no_op\nshow_im(layer00(im2), figsize=(3,3));\n\n\n\n\n\n\n\n\nInstead of creating two layers, we usually create multiple kernels. layer8 has two output channels, which means two kernels. We can set these two kernels into what we had, and we can check the output.\n\nlayer8 = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=0)\nlayer8.weight\n\nParameter containing:\ntensor([[[[ 0.16,  0.02,  0.11],\n          [ 0.07,  0.12,  0.17],\n          [-0.31,  0.17, -0.23]]],\n\n\n        [[[-0.25,  0.02, -0.06],\n          [ 0.20, -0.19, -0.30],\n          [ 0.24, -0.05,  0.19]]]], requires_grad=True)\n\n\n\nlayer8.weight.shape\n\ntorch.Size([2, 1, 3, 3])\n\n\n\ntop_edge.shape\n\ntorch.Size([1, 1, 3, 3])\n\n\n\nlayer8.weight = torch.nn.Parameter(torch.cat([top_edge, right_edge]))\nlayer8.weight\n\nParameter containing:\ntensor([[[[ 1.,  1.,  1.],\n          [ 0.,  0.,  0.],\n          [-1., -1., -1.]]],\n\n\n        [[[-1.,  0.,  1.],\n          [-1.,  0.,  1.],\n          [-1.,  0.,  1.]]]], requires_grad=True)\n\n\n\ndef show_ims(ims, figsize=None):\n    fig, ax = plt.subplots(1, len(ims), figsize=figsize)\n    for i, im in enumerate(ims):\n        if all(hasattr(im, x) for x in ('cpu','permute','detach')):\n            im = im.detach().cpu()\n            if len(im.shape)==3 and im.shape[0]&lt;5: im=im.permute(1,2,0)\n            ax[i].imshow(im)\n            ax[i].set_xticks([])\n            ax[i].set_yticks([])\n\n\nshow_ims([im3, im4])\n\n\n\n\n\n\n\n\nNice. We got the same output as before.\n\nlayer9 = nn.Conv2d(2, 1, kernel_size=3, stride=1, padding=0)\nlayer9.weight.shape\n\ntorch.Size([1, 2, 3, 3])\n\n\n\nlayer9.weight\n\nParameter containing:\ntensor([[[[ 0.05,  0.06, -0.16],\n          [-0.11,  0.08,  0.04],\n          [-0.10, -0.07,  0.22]],\n\n         [[-0.04,  0.13,  0.10],\n          [-0.15, -0.20,  0.23],\n          [ 0.01,  0.16,  0.05]]]], requires_grad=True)\n\n\n\nno_op = torch.nn.Parameter(tensor([[1,1,1],[1,1,1],[1,1,1]],\n                                     dtype=torch.float32).reshape((3, 3)))\nno_op\n\nParameter containing:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], requires_grad=True)\n\n\n\ntorch.stack([no_op, no_op]).unsqueeze(0).shape\n\ntorch.Size([1, 2, 3, 3])\n\n\n\nno_ops = torch.nn.Parameter(torch.stack([no_op, no_op]).unsqueeze(0))\nno_ops\n\nParameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]],\n\n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\n\n\n\ntorch.stack([im3, im4]).shape\n\ntorch.Size([2, 26, 26])\n\n\n\nlayer9.weight = no_ops\nshow_im(layer9(torch.stack([im3, im4])), figsize=((3,3)))\n\n\n\n\n\n\n\n\nSo, we combined the output of two images and mixed them into one. In this image, the darkest spots are places where top corner and right corner both happen. This way, the convolutional neural network can combine different features into a more useful one.",
    "crumbs": [
      "Home",
      "Blog",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-23-Convolutions/index.html#conclusion",
    "href": "blog/posts/2023-09-23-Convolutions/index.html#conclusion",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "In this blog, we learned how convolutional neural net works by using Pytorch. It is used mostly for images. If you want to learn more, take a look at FastAI lesson 8. It might be easier to watch the video to undertstand, and you can come back to play around with the code.\nFor a challenge, you can try to train MNIST using both linear layers and convolutional neural networks, and try to find out which one is better.",
    "crumbs": [
      "Home",
      "Blog",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "blog/posts/2023-07-08-Hugging-Face-Features/index.html",
    "href": "blog/posts/2023-07-08-Hugging-Face-Features/index.html",
    "title": "Hugging Face Features",
    "section": "",
    "text": "Hugging Face Datasets\nIn lesson 15 of the Practical Deep Learning For Coders, we used Hugging Face Datasets to download Fasion MNIST data and trained our model. I faced a problem here because I could not fit my model fast enough. Even using all my CPU cores, it was not as fast as Jeremy’s computer. Even on Google Colab, CPU was not strong. It was not a huge problem, but it was annoying, so I decided to find a solution.\nIn the lesson, we downloaded images and applied a transform function to convert them into tensors. With dsd.with_transform, the transform happened every batch and it took the most of the time. We don’t have to apply transform every batch. So, let’s find a way to only do it once.\nInitially, I just wanted to convert images into tensors with map, but Hugging Face used Apache Arrow, which does not support tensors type. So, I used Hugging Face Features and Array2D to fix this problem.\nHere is the orginal approach that takes a long time from the course.",
    "crumbs": [
      "Home",
      "Blog",
      "Hugging Face Features"
    ]
  },
  {
    "objectID": "blog/posts/2023-07-08-Hugging-Face-Features/index.html#original-approach",
    "href": "blog/posts/2023-07-08-Hugging-Face-Features/index.html#original-approach",
    "title": "Hugging Face Features",
    "section": "Original approach",
    "text": "Original approach\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.conv import *\n\n\nfrom datasets import load_dataset,load_dataset_builder\n\nimport torch\nimport torchvision.transforms.functional as TF\nfrom torch import optim, nn,tensor\nimport torch.nn.functional as F\n\nimport fastcore.all as fc\n\n\nimport logging\nlogging.disable(logging.WARNING)\n\nfrom tqdm import tqdm\n\nFirst, we grab the data from Hugging Face.\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n\n\n\nHere is a inplace transform function. This function is applied every batch and converts images into tensors with the right shape.\n\n@inplace\ndef transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]\n\nSince with_transform applies the transform function every new batch, this is good for applying data augmentations or place where we want randomness.\n\nbs = 1024\ntds = dsd.with_transform(transformi)\n\nNow we make a Pytorch DataLoaders. We can say how many processors we want to use. We are using 4 here.\n\ndls = DataLoaders.from_dd(tds, bs, num_workers=4)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 784]), tensor([2, 6, 7, 4, 9, 5, 3, 5, 6, 7]))\n\n\nThis is the Learner class. It is not very flexible, but it works.\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n\n    def one_batch(self):\n        self.xb,self.yb = to_device(self.batch)\n        self.preds = self.model(self.xb)\n        self.loss = self.loss_func(self.preds, self.yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n            self.opt.zero_grad()\n        with torch.no_grad(): self.calc_stats()\n\n    def calc_stats(self):\n        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n        self.accs.append(acc)\n        n = len(self.xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n\n    def one_epoch(self, train):\n        self.model.training = train\n        dl = self.dls.train if train else self.dls.valid\n        for self.num,self.batch in enumerate(dl): self.one_batch()\n        n = sum(self.ns)\n        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n    \n    def fit(self, n_epochs):\n        self.accs,self.losses,self.ns = [],[],[]\n        self.model.to(def_device)\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        for self.epoch in range(n_epochs):\n            self.one_epoch(True)\n            with torch.no_grad(): self.one_epoch(False)\n\n\nm,nh = 28*28,50\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\nWe fit, but this is not very fast.\n\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2)\nlearn.fit(1)\n# Using only 1\n\n0 True 1.1959598958333333 0.6107833333333333\n0 False 1.1534678571428572 0.6217571428571429\nCPU times: user 5.41 s, sys: 461 ms, total: 5.87 s\nWall time: 7.88 s\n\n\n\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2)\nlearn.fit(1)\n# Using 4\n\n0 True 0.7164356770833333 0.7443166666666666\n0 False 0.7154278459821428 0.7437571428571429\nCPU times: user 4.6 s, sys: 434 ms, total: 5.03 s\nWall time: 7.79 s\n\n\nOkay. We used 4 processors to train the model here but it is still not very fast. Let’s make it faster!",
    "crumbs": [
      "Home",
      "Blog",
      "Hugging Face Features"
    ]
  },
  {
    "objectID": "blog/posts/2023-07-08-Hugging-Face-Features/index.html#faster-fit",
    "href": "blog/posts/2023-07-08-Hugging-Face-Features/index.html#faster-fit",
    "title": "Hugging Face Features",
    "section": "Faster fit",
    "text": "Faster fit\nBy using Hugging Face Features, we can turn images into tensors when we download the data. First, we use load_data_builder to look at the metadata, such as the features, splits, description of the data, and etc. without actually downloading the data yet.\n\nbuilder = load_dataset_builder(name)\nbuilder.info.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\n\ndsd_features = dsd['train'].features.copy()\ndsd_features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\n\nfrom datasets import Features, Array2D\n\nWe use Array2D to turn the images into 2D arrays with a certain shape and dtype. It is a bit weird using Array2D and shape=[1, 28*28] instead of something like Array or Array1D and shape=[28*28]. However, Hugging Face does not have that. We can just use map to unsqueeze it. However, this won’t be a problem with colored images.\n\ndsd_features['image'] = Array2D(shape=[1, 28*28], dtype='float32')\ndsd_features\n\n{'image': Array2D(shape=(1, 784), dtype='float32', id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\nNow we load the dataset using those features, but this is a list! Why is it not a tensor? We have to set the format to torch in order to make it as tensor.\n\ndsd = load_dataset(name, features=dsd_features)\ntype(dsd['train'][0][x])\n\n\n\n\nlist\n\n\n\ndsd.set_format(type=\"torch\")\ntype(dsd['train'][0][x])\n\ntorch.Tensor\n\n\n\ndsd['train'][0][x].shape\n\ntorch.Size([1, 784])\n\n\nNow, we just need to squeeze each tensor to get rid of useless 1 in the shape.\n\n@inplace\ndef sq(b): b[x] = [o.squeeze().div(255) for o in b[x]]\n\nHere, we use map to squeeze them. With batched=True, it is faster.\n\ntds = dsd.map(sq, batched=True)\ntds['train'][0][x].shape\n\n\n\n\n\n\n\ntorch.Size([784])\n\n\n\n\n\n\n\n\nNoteWhy not just use torch.tensor?\n\n\n\nSo, why didn’t we just use torch.tensor in the beginning and used Features and Array2D? Because Hugging Face converts tensors back to images. Hugging Face uses Apache Arrow, and it does not support tensors are not supported. So data have to be either list or image, and we do not want image.\n\n\nNow, it is in the right shape. However, the difference is that it does not have to keep converting from image to tensor every batch. With map, there is no calculation on flight, which is what we want here.\n\ndls = DataLoaders.from_dd(tds, bs, num_workers=0)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 784]), tensor([2, 0, 0, 0, 0, 7, 0, 5, 5, 2]))\n\n\nNow, it is very fast to train even with only one worker.\n\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2)\nlearn.fit(1)\n\n0 True 0.6185346354166666 0.7802833333333333\n0 False 0.6170732700892857 0.7807571428571428\nCPU times: user 5.4 s, sys: 225 ms, total: 5.63 s\nWall time: 2.82 s",
    "crumbs": [
      "Home",
      "Blog",
      "Hugging Face Features"
    ]
  },
  {
    "objectID": "blog/posts/2023-07-08-Hugging-Face-Features/index.html#conclusion",
    "href": "blog/posts/2023-07-08-Hugging-Face-Features/index.html#conclusion",
    "title": "Hugging Face Features",
    "section": "Conclusion",
    "text": "Conclusion\nWe used Features and Array2D to convert images into tensors for faster training. It was awkward using Array2D when we want Array1D, but it was not a problem.",
    "crumbs": [
      "Home",
      "Blog",
      "Hugging Face Features"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-07-saving_jupyter_config/index.html",
    "href": "blog/posts/2022-12-07-saving_jupyter_config/index.html",
    "title": "Saving Jupyter configuration on Paperspace.",
    "section": "",
    "text": "Today, I will go over how to save Jupyter configurations on Paperspace so that we can keep using the customized shortcuts we saved.\nAs I used more Jupyter notebook, I learned shortcuts to save mouse clicks and time. Instead of clicking run to run a cell, I use Shift-Enter to run and move to the next cell or Ctrl-Enter to run and stay at the cell. I also use m to convert a cell to markdown and 0,0 to restart kernel. However, there are more commands I assigned keyboard shortcuts for.\nOn Jupyter notebook, some of my customized shortcuts are these:\n\nrun all cells above: Ctrl-Shift-Enter\nmove selected cells up: Ctrl-K\nmove selected cells down: Ctrl-J\nshutdown kernel and close window: 9,1,1\nswitch between showing and hiding the header: Alt-Ctrl-H\n\nOne day, I got on Paperspace and opened classic Jupyter notebook. As Paperspace’s notebook looked the same as my local computer’s, I tried to use my shortcut thinking it was my local computer, but nothing happened. Then I thought, ‘oh yeah, I’m on paperspace. I cannot use shortcuts.’ After a moment, I also thought ‘why not?’ I started to look for a file in my home directory in Paperspace to find out where it’s keeping track of all the configuration stuff, and put it in my persistent storage. If you wanted to save Jupyter configuration, but couldn’t do it yet, here’s how.\n\n\nThis is on Paperspace’s instance using fastai image.\n\ncd: go to home directory.\nls -al: List (show) hidden directories and files. You can see that we have .jupyter/. This is where jupyter things are located.\n\n\n\n\nls -al output\n\n\n\nOpen Jupyter notebook and save shortcuts. Personally, I opened classic notebook and saved mine, but jupyterlab settings should work, too. You can also save other settings on jupyterlab as well, such as terminal color, etc. These will be persistent as well.\nvim ~/.jupyter/serverconfig/notebook.json: No need to edit anything. We can just see that this is where all configurations are saved. This is how mine looks like:\n\n\n\n\nMy jupyterlab configurations\n\n\n\nmv ~/.jupyter /storage/: Move .jupyter/ directory into our persistent storage space.\nln -s /storage/.jupyter: Create a symlink from our persistent storage’s jupyter to home directory.\nvim /storage/pre-run.sh: To make these settings last next time the instance starts, we have to add some lines to pre-run.sh. Add following lines into the file if there’s nothing in it:\n\n#!/usr/bin/env bash\n\npushd ~\ncd\n\nrm -rf .jupyter/\nln -s /storage/.jupyter/\n\npopd\nThat’s it. You can restart your instance to check.\n\n\n\nWe saved jupyter configurations on Paperspace today. We used some bash If you are not familiar with bash, scripts, and others covered here, check out live coding vidoes by Jeremy Howard. I have written blogs based on these videos, so you can read them as well.",
    "crumbs": [
      "Home",
      "Blog",
      "Saving Jupyter configuration on Paperspace."
    ]
  },
  {
    "objectID": "blog/posts/2022-12-07-saving_jupyter_config/index.html#taking-actions",
    "href": "blog/posts/2022-12-07-saving_jupyter_config/index.html#taking-actions",
    "title": "Saving Jupyter configuration on Paperspace.",
    "section": "",
    "text": "This is on Paperspace’s instance using fastai image.\n\ncd: go to home directory.\nls -al: List (show) hidden directories and files. You can see that we have .jupyter/. This is where jupyter things are located.\n\n\n\n\nls -al output\n\n\n\nOpen Jupyter notebook and save shortcuts. Personally, I opened classic notebook and saved mine, but jupyterlab settings should work, too. You can also save other settings on jupyterlab as well, such as terminal color, etc. These will be persistent as well.\nvim ~/.jupyter/serverconfig/notebook.json: No need to edit anything. We can just see that this is where all configurations are saved. This is how mine looks like:\n\n\n\n\nMy jupyterlab configurations\n\n\n\nmv ~/.jupyter /storage/: Move .jupyter/ directory into our persistent storage space.\nln -s /storage/.jupyter: Create a symlink from our persistent storage’s jupyter to home directory.\nvim /storage/pre-run.sh: To make these settings last next time the instance starts, we have to add some lines to pre-run.sh. Add following lines into the file if there’s nothing in it:\n\n#!/usr/bin/env bash\n\npushd ~\ncd\n\nrm -rf .jupyter/\nln -s /storage/.jupyter/\n\npopd\nThat’s it. You can restart your instance to check.",
    "crumbs": [
      "Home",
      "Blog",
      "Saving Jupyter configuration on Paperspace."
    ]
  },
  {
    "objectID": "blog/posts/2022-12-07-saving_jupyter_config/index.html#conclusion",
    "href": "blog/posts/2022-12-07-saving_jupyter_config/index.html#conclusion",
    "title": "Saving Jupyter configuration on Paperspace.",
    "section": "",
    "text": "We saved jupyter configurations on Paperspace today. We used some bash If you are not familiar with bash, scripts, and others covered here, check out live coding vidoes by Jeremy Howard. I have written blogs based on these videos, so you can read them as well.",
    "crumbs": [
      "Home",
      "Blog",
      "Saving Jupyter configuration on Paperspace."
    ]
  },
  {
    "objectID": "blog/posts/2022-11-27-live_coding5/index.html",
    "href": "blog/posts/2022-11-27-live_coding5/index.html",
    "title": "Live coding 5",
    "section": "",
    "text": "In this blog, I will cover vim and ctags based on the Youtube video.\nVim is a great tool because of availability on command line interface and its power. Some editors, such as VS code, do not run on terminal. Therefore, if we want to edit files in paperspace, we have to use something that runs in this environment. There are nano, emacs, vim, and more. Nano is good for starting out because it has commands on the bottom of the screen. However, it is not as powerful as emacs and vim. We will learn about vim.\n\n\n\n\nVim should be installed, but if it is not, install it using sudo apt install vim on linux or brew install vim on mac. To execute vim, simply type vim on terminal. We will see this screen:\n\n\n\nVim welcoming screen\n\n\nThe main screen tells us about the version and author. It also says we can exit by typing :q. If we type :, vim displays it on the bottom left corner. So, if we type :q, it will be on the bottom left corner like this:\n\n\n\n:q command\n\n\nIf we press Enter, we quit vim. We can come back to vim by typing vim again in terminal.\nSo, there are two modes in vim. One is command mode, which we are on right now, and the other is insert mode. On command mode, we cannot type text we want to insert. To enter insert mode, we can type i. Now, we can start typing text we want to add. Rather than writing something from scratch, let’s copy some text for practice. Here is a source code from fastcore. You can copy it and paste it in your file by typing i to get into insert mode and Ctrl-Shift-v to paste.\n# %% ../nbs/02_foundation.ipynb 1\nfrom .imports import *\nfrom .basics import *\nfrom functools import lru_cache\nfrom contextlib import contextmanager\nfrom copy import copy\nfrom configparser import ConfigParser\nimport random,pickle,inspect\n\n# %% ../nbs/02_foundation.ipynb 5\n@contextmanager\ndef working_directory(path):\n    \"Change working directory to `path` and return to previous on exit.\"\n    prev_cwd = Path.cwd()\n    os.chdir(path)\n    try: yield\n    finally: os.chdir(prev_cwd)\nAfter pasting the code, we can press Esc to get out of insert mode.\n\n\n\n\n\n\nTipCommand mode vs. Insert mode\n\n\n\n\n\nAfter finishing editing a file, press Esc to switch to command mode. It’s good to get in the habbit of switching to command mode all the time because I know what mode it will be when I see my vim.\nWhen I was first learning vim, I didn’t check what mode I was in. When I was in command mode, I thought I left my vim on insert mode. So, I started typing some code from the book I was reading, and I realized that I messed up my code by typing different commands. Thankfully, vim had u command to undo previous commands, but it was not very enjoyable. So, I always put my vim on command mode after editing, and I double check what mode I am in when I open up the terminal by typing Esc. Pressing Esc guarantees that vim will be on command mode because it stays on command mode even if it was on command mode.\n\n\n\nThen, we can save this file by typing :w &lt;filename&gt;.py. Replace &lt;filename&gt;.py with whatever name you want. As long as the filename ends with .py, we will get syntax highlighting. After saving the file with a name, we can type :w to save the file. The :w command can be combined with :q to save and quit at the same time by typing :wq.\n\n\n\nsyntax highlighting\n\n\nI saved it as vim.py, and it is more readable with colors. Here, we can move around the cursor by using either arrow keys or by using h, j, k, and l. It seems random that hjkl moves our cursor in certain way. However, after getting used to using these keys, it becomes more comfortable to use these keys than to use arrow keys. Here’s what those keys do:\n\nh: left\nj: down\nk: up\nl: down\n\nSo, there are other ways to enter edit mode. These commands are different from i because they move the cursor to different location and enters into the edit mode. Let’s start by looking at o and Shift-o. o adds a line below and moves the cursor to the next line. Shift-o adds a line on the previous line and moves the cursor there as well. So, combining the command with Shift can be seen as an variation of a command for some commands. Another way to enter edit mode is Shift-i, which moves the cursor to the beginning of the text of a line. To start editing at the end of the line, Shift-a can be used.\n\no: next line\nShift-o: previous line\nShift-i: beginning of the line\nShift-a: end of line\n\n\n\n\n\n\n\nTipUndo with u\n\n\n\nWhen you editted something or used a command, but made a mistake and wanted to undo that, press u to undo. You can press u multiple times to go back further. However, if you pressed too many us and wanted to undo the u, you can redo it with Ctrl-r. If you have no idea what’s going on and file changes cannot be fixed with u and Ctrl-r, you can quit the file without saving by typing :q!. Then you can come back to the file by opening it again.\n\n\nMoving one character by character is time consuming, and there’s a better way to move: move by words. We can use w to move forward or b to move backward. If that’s still not enough movement, we can press } and { to move forward and backward by paragraphs.\nSometimes, it is useful to just jump to last line of the file. We can press G for that, and to move to first line, gg can be used.\nThat’s enough moving around for now. Let’s move onto some editing.\nTo delete a line, we can press dd. To change a line, it is cc.\nAnother useful command is .. This command repeats the last command. If we deleted a line, we can just press . to do it again.\n\n\n\nWe can combine commands we learned earlier with numbers. For instance, we can move down 5 times by pressing 5j and move forward 3 words by 3w. Try to practice and feel how it is.\nAnother way to combine commands is by combining editing commands with movement commands. For instance, we can delete next 5 lines by pressing 5dj or d5j and change three words by 3cw or c3w. Try to come up with different combinations with all the movement commands. Also, after changing words, it can be repeated using ..\nBy combining commands, we can see how powerful these commands are.\n\n\n\nWe can start searching with /. If want to search import, we can type /import and press Enter. Then, we can move onto the next occurrence with n and back with N. Instead of plain text, regex can be used as well.\n\n\n\nsearch\n\n\nThere’s more we can do using patterns. Let’s say we want to remove all lines starting with ‘from’ in this file. There are many ways to do this. We can do dd multiple times to delete those lines. We can also use . after using dd to save typing. Another way us typing :g/^from/d. Just like other commands starting with :, it will be displayed on the bottom left. So, this is a global command that finds a pattern and executes a command. It can also take range of lines before g to limit the effect. Try using different command or pattern and see what you can do.\n\n\n\nAs we have seen, vim can be very powerful by combining commands. We can also use patterns to apply commands on certain lines. There are so much more in vim than this. If you are interested, you can use vimtutor to learn more about it. Make sure to practice to get better.\n\n\n\nvimtutor\n\n\nHere’s a summary of commands we covered today:\n\n\n\n\n\n\n\nCommand\nMeaning\n\n\n\n\ni\nEnter insert mode\n\n\nEsc\nEnter command mode\n\n\nh\nMove the cursor left\n\n\nj\nMove the cursor down\n\n\nk\nMove the cursor up\n\n\nl\nMove the cursor right\n\n\no\nAdd a line before the cursor and enter insert mode\n\n\nShift-o\nAdd a line after the cursor and enter insert mode\n\n\nu\nUndo\n\n\nCtrl-r\nRedo\n\n\nw\nMove forward by a word\n\n\nb\nMove backward by a word\n\n\n}\nMove to next paragraph\n\n\n{\nMove to previous paragraph\n\n\ndd\nDelete a line\n\n\ncc\nChange a line\n\n\n.\nRepeat last command\n\n\n/pattern\nSearch the ‘pattern’\n\n\n:g/pattern/command\nExecute command on all the lines matching the pattern. ex) :g/import/d\n\n\n:w\nSave the file\n\n\n:w &lt;filename&gt;\nSave a file to &lt;filename&gt;\n\n\n:q\nExit vim\n\n\n:q!\nExit vim while discarding all changes\n\n\n:wq\nSave the file and exit\n\n\n\n\n\n\n\nNext thing we will talk about is ctags. With ctags, we can jump around files to look for source code for a class or a function. When you encounter a class you are not familiar with, you can jump to its definition right away.\nHere are the steps to prepare ctags:\n\nFirst, we install ctags using conda/mamba. mamba install -c conda-forge ctags\nThen, we clone fastai libary from github. We can optionally use --depth 1 at the end of the command and not grab all the history.\nNow, change directory to fastai/fastai (fastai directory within fastai).\nType ctags -R ..\nWe have a file called tags in our directory. We are ready to use ctags.\n\n\n\n\nCreating ctags\n\n\nTry typing vim -t Arraymask. It opens vim on the definition of Arraymask. In vim, you can press Ctrl-] on any word you want to find the definition of, and you can jump there. To come back to previous, you can type Ctrl-t to come back. It doesn’t work on every word, it has to be a word defined in fastai library.\nWe can also do :tj Arraymask to do the same thing within vim.\n\n\n\nJumping around with ctags\n\n\nLastly, we can type :stag Dataloaders to split screen with the definition. You can search anything you want instead of Dataloaders, too.\n\n\n\nSo, we learned about vim and ctags. These tools are very useful. Once you get used to vim by playing around a little bit, you will find them quite comfortable.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 5"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-27-live_coding5/index.html#vim",
    "href": "blog/posts/2022-11-27-live_coding5/index.html#vim",
    "title": "Live coding 5",
    "section": "",
    "text": "Vim should be installed, but if it is not, install it using sudo apt install vim on linux or brew install vim on mac. To execute vim, simply type vim on terminal. We will see this screen:\n\n\n\nVim welcoming screen\n\n\nThe main screen tells us about the version and author. It also says we can exit by typing :q. If we type :, vim displays it on the bottom left corner. So, if we type :q, it will be on the bottom left corner like this:\n\n\n\n:q command\n\n\nIf we press Enter, we quit vim. We can come back to vim by typing vim again in terminal.\nSo, there are two modes in vim. One is command mode, which we are on right now, and the other is insert mode. On command mode, we cannot type text we want to insert. To enter insert mode, we can type i. Now, we can start typing text we want to add. Rather than writing something from scratch, let’s copy some text for practice. Here is a source code from fastcore. You can copy it and paste it in your file by typing i to get into insert mode and Ctrl-Shift-v to paste.\n# %% ../nbs/02_foundation.ipynb 1\nfrom .imports import *\nfrom .basics import *\nfrom functools import lru_cache\nfrom contextlib import contextmanager\nfrom copy import copy\nfrom configparser import ConfigParser\nimport random,pickle,inspect\n\n# %% ../nbs/02_foundation.ipynb 5\n@contextmanager\ndef working_directory(path):\n    \"Change working directory to `path` and return to previous on exit.\"\n    prev_cwd = Path.cwd()\n    os.chdir(path)\n    try: yield\n    finally: os.chdir(prev_cwd)\nAfter pasting the code, we can press Esc to get out of insert mode.\n\n\n\n\n\n\nTipCommand mode vs. Insert mode\n\n\n\n\n\nAfter finishing editing a file, press Esc to switch to command mode. It’s good to get in the habbit of switching to command mode all the time because I know what mode it will be when I see my vim.\nWhen I was first learning vim, I didn’t check what mode I was in. When I was in command mode, I thought I left my vim on insert mode. So, I started typing some code from the book I was reading, and I realized that I messed up my code by typing different commands. Thankfully, vim had u command to undo previous commands, but it was not very enjoyable. So, I always put my vim on command mode after editing, and I double check what mode I am in when I open up the terminal by typing Esc. Pressing Esc guarantees that vim will be on command mode because it stays on command mode even if it was on command mode.\n\n\n\nThen, we can save this file by typing :w &lt;filename&gt;.py. Replace &lt;filename&gt;.py with whatever name you want. As long as the filename ends with .py, we will get syntax highlighting. After saving the file with a name, we can type :w to save the file. The :w command can be combined with :q to save and quit at the same time by typing :wq.\n\n\n\nsyntax highlighting\n\n\nI saved it as vim.py, and it is more readable with colors. Here, we can move around the cursor by using either arrow keys or by using h, j, k, and l. It seems random that hjkl moves our cursor in certain way. However, after getting used to using these keys, it becomes more comfortable to use these keys than to use arrow keys. Here’s what those keys do:\n\nh: left\nj: down\nk: up\nl: down\n\nSo, there are other ways to enter edit mode. These commands are different from i because they move the cursor to different location and enters into the edit mode. Let’s start by looking at o and Shift-o. o adds a line below and moves the cursor to the next line. Shift-o adds a line on the previous line and moves the cursor there as well. So, combining the command with Shift can be seen as an variation of a command for some commands. Another way to enter edit mode is Shift-i, which moves the cursor to the beginning of the text of a line. To start editing at the end of the line, Shift-a can be used.\n\no: next line\nShift-o: previous line\nShift-i: beginning of the line\nShift-a: end of line\n\n\n\n\n\n\n\nTipUndo with u\n\n\n\nWhen you editted something or used a command, but made a mistake and wanted to undo that, press u to undo. You can press u multiple times to go back further. However, if you pressed too many us and wanted to undo the u, you can redo it with Ctrl-r. If you have no idea what’s going on and file changes cannot be fixed with u and Ctrl-r, you can quit the file without saving by typing :q!. Then you can come back to the file by opening it again.\n\n\nMoving one character by character is time consuming, and there’s a better way to move: move by words. We can use w to move forward or b to move backward. If that’s still not enough movement, we can press } and { to move forward and backward by paragraphs.\nSometimes, it is useful to just jump to last line of the file. We can press G for that, and to move to first line, gg can be used.\nThat’s enough moving around for now. Let’s move onto some editing.\nTo delete a line, we can press dd. To change a line, it is cc.\nAnother useful command is .. This command repeats the last command. If we deleted a line, we can just press . to do it again.\n\n\n\nWe can combine commands we learned earlier with numbers. For instance, we can move down 5 times by pressing 5j and move forward 3 words by 3w. Try to practice and feel how it is.\nAnother way to combine commands is by combining editing commands with movement commands. For instance, we can delete next 5 lines by pressing 5dj or d5j and change three words by 3cw or c3w. Try to come up with different combinations with all the movement commands. Also, after changing words, it can be repeated using ..\nBy combining commands, we can see how powerful these commands are.\n\n\n\nWe can start searching with /. If want to search import, we can type /import and press Enter. Then, we can move onto the next occurrence with n and back with N. Instead of plain text, regex can be used as well.\n\n\n\nsearch\n\n\nThere’s more we can do using patterns. Let’s say we want to remove all lines starting with ‘from’ in this file. There are many ways to do this. We can do dd multiple times to delete those lines. We can also use . after using dd to save typing. Another way us typing :g/^from/d. Just like other commands starting with :, it will be displayed on the bottom left. So, this is a global command that finds a pattern and executes a command. It can also take range of lines before g to limit the effect. Try using different command or pattern and see what you can do.\n\n\n\nAs we have seen, vim can be very powerful by combining commands. We can also use patterns to apply commands on certain lines. There are so much more in vim than this. If you are interested, you can use vimtutor to learn more about it. Make sure to practice to get better.\n\n\n\nvimtutor\n\n\nHere’s a summary of commands we covered today:\n\n\n\n\n\n\n\nCommand\nMeaning\n\n\n\n\ni\nEnter insert mode\n\n\nEsc\nEnter command mode\n\n\nh\nMove the cursor left\n\n\nj\nMove the cursor down\n\n\nk\nMove the cursor up\n\n\nl\nMove the cursor right\n\n\no\nAdd a line before the cursor and enter insert mode\n\n\nShift-o\nAdd a line after the cursor and enter insert mode\n\n\nu\nUndo\n\n\nCtrl-r\nRedo\n\n\nw\nMove forward by a word\n\n\nb\nMove backward by a word\n\n\n}\nMove to next paragraph\n\n\n{\nMove to previous paragraph\n\n\ndd\nDelete a line\n\n\ncc\nChange a line\n\n\n.\nRepeat last command\n\n\n/pattern\nSearch the ‘pattern’\n\n\n:g/pattern/command\nExecute command on all the lines matching the pattern. ex) :g/import/d\n\n\n:w\nSave the file\n\n\n:w &lt;filename&gt;\nSave a file to &lt;filename&gt;\n\n\n:q\nExit vim\n\n\n:q!\nExit vim while discarding all changes\n\n\n:wq\nSave the file and exit",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 5"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-27-live_coding5/index.html#ctags",
    "href": "blog/posts/2022-11-27-live_coding5/index.html#ctags",
    "title": "Live coding 5",
    "section": "",
    "text": "Next thing we will talk about is ctags. With ctags, we can jump around files to look for source code for a class or a function. When you encounter a class you are not familiar with, you can jump to its definition right away.\nHere are the steps to prepare ctags:\n\nFirst, we install ctags using conda/mamba. mamba install -c conda-forge ctags\nThen, we clone fastai libary from github. We can optionally use --depth 1 at the end of the command and not grab all the history.\nNow, change directory to fastai/fastai (fastai directory within fastai).\nType ctags -R ..\nWe have a file called tags in our directory. We are ready to use ctags.\n\n\n\n\nCreating ctags\n\n\nTry typing vim -t Arraymask. It opens vim on the definition of Arraymask. In vim, you can press Ctrl-] on any word you want to find the definition of, and you can jump there. To come back to previous, you can type Ctrl-t to come back. It doesn’t work on every word, it has to be a word defined in fastai library.\nWe can also do :tj Arraymask to do the same thing within vim.\n\n\n\nJumping around with ctags\n\n\nLastly, we can type :stag Dataloaders to split screen with the definition. You can search anything you want instead of Dataloaders, too.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 5"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-27-live_coding5/index.html#conclusion",
    "href": "blog/posts/2022-11-27-live_coding5/index.html#conclusion",
    "title": "Live coding 5",
    "section": "",
    "text": "So, we learned about vim and ctags. These tools are very useful. Once you get used to vim by playing around a little bit, you will find them quite comfortable.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 5"
    ]
  },
  {
    "objectID": "blog/posts/2023-02-19-MNIST_FastAI/index.html",
    "href": "blog/posts/2023-02-19-MNIST_FastAI/index.html",
    "title": "MNIST in FastAI",
    "section": "",
    "text": "MNIST Project in FastAI\nHere, we will predict MNIST digits using FastAI. Then, we will go deeper and figure out what’s happening behind the scenes.\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\n\npath = untar_data(URLs.MNIST)\nPath.BASE_PATH = path\npath.ls()\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00&lt;00:00]\n    \n    \n\n\n(#2) [Path('training'),Path('testing')]\n\n\n\n(path/'training').ls()\n\n(#10) [Path('training/7'),Path('training/8'),Path('training/5'),Path('training/6'),Path('training/9'),Path('training/3'),Path('training/2'),Path('training/0'),Path('training/4'),Path('training/1')]\n\n\nUsing fastai, it is very easy to train. We make a DataBlock with information on how to get labels, grab images, and split train/valid data.\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=RandomSplitter(),\n)\ndls = dblock.dataloaders(path)\n\nThen, we take a look at it, and it looks like labels are correctly assigned to each image.\n\ndls.show_batch()\n\n\n\n\n\n\n\n\nWe create a learner with resnet18, so we can use transfer learning. Then, we find the learning rate.\n\nlearn = vision_learner(dls, arch=resnet18, metrics=accuracy)\nlearn.lr_find()\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\n\n\n\nAlthough fastai suggests a learning rate suitable to use, I can use a little bit bigger learning rate. I can probably use 0.01 even if 0.001 is suggested.\n\nlearn.fine_tune(3, 0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.383465\n0.211662\n0.934286\n00:46\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.134975\n0.076517\n0.978714\n00:59\n\n\n1\n0.030738\n0.033497\n0.991071\n00:59\n\n\n2\n0.019911\n0.024113\n0.993071\n00:59\n\n\n\n\n\nTraining was very easy with almost perfect accuracy thanks to fastai. However, it is fun to dive deeper and figure out what’s going on under the hood.",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST in FastAI"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "delicious-nbdev",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nI am Jay Suh, and this is my personal blog.\nI write about deep learning, machine learning, genetics, and anything else I like."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Delicious Blog",
    "section": "",
    "text": "How to build an LLM agent with LiteLLM?\n\n\n\n\n\n\n\n\nJan 10, 2026\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Build a Language Model from Scratch?\n\n\n\n\n\n\n\n\nOct 26, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nHuggingFace KV cache\n\n\n\n\n\n\n\n\nSep 28, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nThe Meselson-Stahl Experiment\n\n\n\n\n\n\n\n\nSep 21, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nMy experience with solveit\n\n\n\n\n\n\n\n\nAug 21, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nRoPE\n\n\n\n\n\n\n\n\nAug 14, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is pass@k evaluation metric?\n\n\n\n\n\n\n\n\nJul 17, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nBeginner’s guide to gemini\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nHow InstructGPT is trained\n\n\n\n\n\n\n\n\nJun 3, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding LLM part2-Processing Text\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding LLM part1-Intro_to_LLMs\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nA Student’s Perspective on Computational Biology\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nHow to solve it\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nResnet Part2\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nMiniAI Utilities\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nResnet\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nScheduler\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizer\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nInitialization part 2\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nInitialization part 1\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Hooks\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\n\n\n\n\n\nSep 23, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLearner Pt.2\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLearner Pt.1\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nCallback\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Features\n\n\n\n\n\n\n\n\nJul 8, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nFruit Multi-Classifier pt.2 Deployment\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nFruit Multi-Classifier pt.1 Training\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST Neural Nets\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST base\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST in FastAI\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nSaving Jupyter configuration on Paperspace.\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Paddy competition\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLive coding 7\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLive coding 6\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLive coding 5\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLive coding 4\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLive coding 3\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLive coding 2\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nLive coding 1\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nAlien vs. Ghost Pt.2 Deploying\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\ngalopy\n\n\n\n\n\n\n\n\n\n\n\n\nAlien vs. Ghost Pt.1 Training\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\ngalopy\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "Large language models are very awesome. In my previous blog posts, I talked about how to build and train them. In this blog, I am shifting gears to explain how to use them. We will build a chat, use tool use (or function calling), and build an agent with LiteLLM. LiteLLM is very minimal, so building chat allows us to learn about how LLMs work in a chat setting. Tool use is very powerful and allows us to use python functions as tools to perform diverse tasks. Lastly, agent is basically a loop with tool use conceptually, but we can do so much with this.\n\nfrom httpx import get as xget\nimport json\nimport inspect\nimport re\nfrom cachy import enable_cachy\n\nenable_cachy()\nlitellm_md = xget('https://raw.githubusercontent.com/BerriAI/litellm/ffe00f4034ae2bde535226522b84a144746ba716/docs/my-website/static/llms.txt').text\nlitellm_md[:400]\n\n'# https://docs.litellm.ai/ llms.txt\\n\\n- [LiteLLM Overview](https://docs.litellm.ai/): Access and manage 100+ LLMs with LiteLLM tools.\\n- [Completion Function Guide](https://docs.litellm.ai/completion/input): Guide for using completion function with various models.\\n- [Litellm Completion Function](https://docs.litellm.ai/completion/output): Learn about the litellm completion function and its output.\\n-'\n\n\n\nenable_mermaid()\n\n\n\n\nHere we import libraries and documentation of litellm using llms_txt as a context. cachy has enable_cachy, which saves model response so we don’t have to spend unnecessary tokens when we rerun the notebooks. Also, it is very fast to run the notebook.\n\n\nLiteLLM is a tool that lets us access and manage over 100 different LLMs (Large Language Models) through a unified interface. This way, we can use any model we want from OpenAI, Anthropic, Gemini, etc. More info on LiteLLM Docs. To change models, we can just choose whatever we want. There are some free options available from openrouter, groq, and gemini. I am just using gpt5-nano here.\nTo use litellm, we first have to choose a model.\n\nfrom litellm import completion \n\nms = [\"gpt-5-nano-2025-08-07\", \"gemini/gemini-3-flash-preview\", \"gemini/gemini-2.5-flash\", \"huggingface/allenai/Olmo-3-7B-Instruct:publicai\", \"groq/openai/gpt-oss-20b\", \"ollama_chat/hf.co/unsloth/SmolLM3-3B-128K-GGUF\"]\nmodel = ms[0]\nmodel\n\n'gpt-5-nano-2025-08-07'\n\n\nThen use completion with the model and messages. messages is a list of dictionaries, which includes role and content.\n\nmessages = [{\"role\":\"user\", \"content\":\"This is a test request\"}]\n\nres = completion(model=model, messages=messages)\nres\n\nModelResponse(id='chatcmpl-CwEwUBbPIBBzxMa7HvBqSVFLJwS0j', created=1767996498, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Got it. How can I help you today? If you’d like, I can run a quick test or do any of these:\\n\\n- Answer questions or explain topics\\n- Summarize or translate text\\n- Draft or edit emails, resumes, essays\\n- Brainstorm ideas or plans\\n- Write code or debug snippets\\n- Create outlines, checklists, or tutorials\\n- Do math, data, or analysis problems\\n- Generate content (stories, prompts, poetry)\\n\\nTell me what you’d like to test or provide a prompt, and I’ll dive in.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=444, prompt_tokens=11, total_tokens=455, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\nThe response includes a lot of stuff, but there’s no need to focus on the details now.\n\n\n\nLet’s improve how the response looks. We only care about the content of the AI’s response. We can monkey patch ModelResponse._repr_markdown_ with our custom one for a better display.\n\nfrom litellm import ModelResponse\n\ndef _repr_markdown_(self): return self.choices[0].message.content\n\nModelResponse._repr_markdown_ = _repr_markdown_\n\nres\n\nGot it. How can I help you today? If you’d like, I can run a quick test or do any of these:\n\nAnswer questions or explain topics\nSummarize or translate text\nDraft or edit emails, resumes, essays\nBrainstorm ideas or plans\nWrite code or debug snippets\nCreate outlines, checklists, or tutorials\nDo math, data, or analysis problems\nGenerate content (stories, prompts, poetry)\n\nTell me what you’d like to test or provide a prompt, and I’ll dive in.\n\n\n\n\n\nLet’s chat with llms. To chat with LLMs, we just have to keep track of our conversation history.\nIt is cumbersome to keep creating a list of dictionaries each time, so we can create a helper function.\n\ndef mk_msg(m, role='user'): return {\"role\":role, \"content\":m}\n\n\nmsgs = mk_msg('hi, I like to eat dirt.')\nmsgs\n\n{'role': 'user', 'content': 'hi, I like to eat dirt.'}\n\n\n\ndef chat(ct, msgs):\n    msgs.append(mk_msg(ct))\n    res = completion(model=model, messages=msgs)\n    msgs.append(mk_msg(res.choices[0].message.content, 'assistant'))\n    return res\n\nWe can use a system prompt to modify llm’s behavior. Let’s make it behave like Gordon Ramsay.\n\nsp = \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"\n\n\nmsgs = [mk_msg(sp, 'system')]\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"}]\n\n\n\nchat('I am hungry. I need to eat some lunch. I feel like eating some dirt, how about you?', msgs)\n\nBloody dirt for lunch? Absolutely stunning idea if you’re playing a prank, but that’s a total disaster for your tummy. Dirt isn’t food—it can carry bacteria, parasites, and who knows what else. Don’t do it. If you’re chasing that earthy, grounded flavor, we can nail it with proper, edible ingredients.\nHere are a few absolutely stunning, earthy lunch ideas you can actually eat:\n\nBeetroot, goat cheese, and hazelnut “soil” salad\n\nRoasted beets, arugula, and soft goat cheese\nHazelnut soil: finely ground toasted hazelnuts mixed with a little breadcrumbs and a pinch of cocoa powder and salt\nDress with olive oil, lemon juice, and balsamic glaze\nQuick to assemble, but looks fancy enough to wow\n\nMushroom risotto with crispy nut “soil”\n\nSauté mixed mushrooms, toast the rice (Arborio) with a splash of white wine\nAdd hot stock gradually until creamy\nTop with a thin layer of crisped almond or walnut crumb to mimic soil\nFinish with parmesan and fresh thyme\n\nCarrot soup with almond crumb\n\nCreamy roasted carrot soup with ginger and a hint of orange\nGarnish with a crunchy almond crumb (almonds finely chopped and toasted in a touch of olive oil)\nA swirl of yogurt or crème fraîche for richness\n\n\nIf you’ve got dietary restrictions or a time constraint, tell me and I’ll tailor one in a flash. Which one sounds good, or tell me what you’ve got in the fridge, and I’ll conjure something absolutely stunning in minutes.\n\n\nLet’s surprise gordon ramsay by modifying the chat history. Instead of dirt, we pretend we said tacos!\n\nmsgs[1]['content'] = 'I am hungry for some nice tacos right now.'\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'I am hungry for some nice tacos right now.'},\n {'role': 'assistant',\n  'content': 'Bloody dirt for lunch? Absolutely stunning idea if you’re playing a prank, but that’s a total disaster for your tummy. Dirt isn’t food—it can carry bacteria, parasites, and who knows what else. Don’t do it. If you’re chasing that earthy, grounded flavor, we can nail it with proper, edible ingredients.\\n\\nHere are a few absolutely stunning, earthy lunch ideas you can actually eat:\\n\\n- Beetroot, goat cheese, and hazelnut “soil” salad\\n  - Roasted beets, arugula, and soft goat cheese\\n  - Hazelnut soil: finely ground toasted hazelnuts mixed with a little breadcrumbs and a pinch of cocoa powder and salt\\n  - Dress with olive oil, lemon juice, and balsamic glaze\\n  - Quick to assemble, but looks fancy enough to wow\\n\\n- Mushroom risotto with crispy nut “soil”\\n  - Sauté mixed mushrooms, toast the rice (Arborio) with a splash of white wine\\n  - Add hot stock gradually until creamy\\n  - Top with a thin layer of crisped almond or walnut crumb to mimic soil\\n  - Finish with parmesan and fresh thyme\\n\\n- Carrot soup with almond crumb\\n  - Creamy roasted carrot soup with ginger and a hint of orange\\n  - Garnish with a crunchy almond crumb (almonds finely chopped and toasted in a touch of olive oil)\\n  - A swirl of yogurt or crème fraîche for richness\\n\\nIf you’ve got dietary restrictions or a time constraint, tell me and I’ll tailor one in a flash. Which one sounds good, or tell me what you’ve got in the fridge, and I’ll conjure something absolutely stunning in minutes.'}]\n\n\n\nchat('When did I say I wanted to eat dirt? Are you crazy!!!!', msgs)\n\nYou’re right, and I’m sorry for veering off. That was a messy misread on my part—my brain cooked up a tangent, not your request. You want tacos? Let’s make them absolutely stunning, not a bloody disaster.\nHere are a few fast, crowd-pleasing taco ideas. Pick one, I’ll tailor to what you’ve got.\n\nCarne Asada Tacos (beef)\n\nWhat you need: flank steak or skirt steak, tortillas, lime, cilantro, white onion, garlic, cumin, chili powder, salt.\nQuick method: marinate steak 15–20 minutes with lime juice, minced garlic, cumin, chili powder, salt. Grill or hot skillet 3–4 minutes per side, rest briefly, slice thin. Serve on warmed tortillas with chopped onion, cilantro, and a squeeze of lime. Optional pico de gallo.\n\nBaja Fish Tacos\n\nWhat you need: white fish fillets (cod, tilapia), corn tortillas, shredded cabbage, lime, cilantro, avocado, chipotle mayo (mayonnaise + chipotle in adobo + lime juice).\nQuick method: season and pan-sear or lightly batter and fry the fish. Toss cabbage with a little salt and lime. Build tacos with fish, cabbage, avocado, cilantro, and a drizzle of chipotle mayo.\n\nCrispy Cauliflower Tacos (vegetarian)\n\nWhat you need: cauliflower florets, olive oil, smoked paprika, cumin, garlic powder, corn tortillas, avocado, salsa or pico, lime.\nQuick method: roast cauliflower tossed in oil and spices at high heat until caramelized and crisp. Assemble in tortillas with avocado, salsa, and a squeeze of lime.\n\nBlack Bean and Mushroom Tacos (vegan or plant-based)\n\nWhat you need: black beans (canned works), mushrooms, onion, garlic, chili powder, cumin, tortillas, tomato salsa, cilantro.\nQuick method: sauté onion and garlic, add sliced mushrooms until browned, stir in beans and spices, heat through. Pile into tortillas and top with salsa and cilantro.\n\n\nIf you want, tell me: - Meat, seafood, or veggie preference - Any food allergies or spice tolerance - What you’ve got in the fridge I’ll tailor a 15-minute taco chaos that’s absolutely stunning. What’re you in the mood for?\n\n\nIt’s fun to modify chat history this way. But there is more to this than just messing with llms. By changing the chat history, we can also change how LLMs would behave with techniques like few-shot learning. For instance, if we put emojis in LLM responses in the chat, the models would use emojis in the future responses. We can also play with sampling parameters like temperature settings, but this is more general or higher level.\nIn future blog posts, I will show how changing model responses is helpful. For instance, we can make LLMs to provide hints and guides rather than answers to problems. It is helpful for learning. Or we can change how LLMs write code by showing examples.\n\n\n\nUsing tools is so much fun when using llms. Tools are python functions that get executed as llms request. Let me show you how it happens.\nThe flow is:\n\nYou define tools (functions) with names, descriptions, and parameters\nYou send a message to the LLM along with the tool definitions\nThe LLM might respond with a tool call instead of text (e.g., “call get_weather with location='Paris'”)\nYour code executes that function and sends the result back\nThe LLM then uses that result to formulate its final answer\n\n\n\n\nflowchart LR\n    A[\"(1) Define tools\"] --&gt; B[\"(2) Send message + tool definitions\"]\n    B --&gt; C[\"(3) LLM responds\"]\n    C --&gt;|Tool call| D[\"(4) Execute function locally\"]\n    D --&gt; E[\"(4) Send result back to LLM\"]\n    E --&gt; C\n    C --&gt;|Text response| F[\"(5) Final answer\"]\n\n\n\nLLM can respond with additional tool calls until it decides to stop. But we will start with one tool for now.\n\n\nWe define a function to use as a tool. The function needs a documentation string and parameters need types.\n\ndef add_numbers(\n    a: int,  # First number to add\n    b: int   # Second number to add  \n) -&gt; int:\n    \"Add two numbers, a and b, together\"\n    return a + b\n\nWe also need tool definition to explain llm what tool does.\n\ntools = [{\n    \"type\": \"function\", \n    \"function\": {\n        \"name\": \"add_numbers\", \n        \"description\": \"Add two numbers, a and b, together\",  \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"a\": {\"type\": \"integer\", \"description\": \"First number to add\"},\n                \"b\": {\"type\": \"integer\", \"description\": \"Second number to add\"}\n            },\n            \"required\": [\"a\", \"b\"]\n        }\n    }\n}]\n\n\n\n\nAnd we use completion with tools.\n\nmessages = [mk_msg(\"What is 1 + 4? Use tool to answer this question.\")]\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\nModelResponse(id='chatcmpl-CwEzbwdEDiABERK1KJ6acWCPbFsJm', created=1767996691, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=219, prompt_tokens=158, total_tokens=377, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=192, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\n\n\n\nThere is no content in the response as the llm is requesting a tool call with function name and arguments. However, it has tool_calls.\n\nres.choices[0].message.content\n\n\nres.choices[0].message.tool_calls\n\n[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')]\n\n\n\nres['choices'][0]\n\nChoices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})\n\n\ntool_calls has information about which tool to call with which arguments.\n\ntc = res['choices'][0]['message']['tool_calls'][0]\ntc\n\nChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')\n\n\n\nfn = tc['function']\nfn['name']\n\n'add_numbers'\n\n\n\nfn['arguments']\n\n'{\"a\":1,\"b\":4}'\n\n\n\njson.loads(fn['arguments'])\n\n{'a': 1, 'b': 4}\n\n\n\n\n\nLet’s run it. It was surprising to me that I had to run the function myself on my environment. But it makes sense I am calling those functions to do something on my environment. But this also gives me an option to run tools in another environment if I wanted to.\n\nglobals()[fn['name']]\n\n&lt;function __main__.add_numbers(a: int, b: int) -&gt; int&gt;\n\n\n\nglobals()[fn['name']](**json.loads(fn['arguments']))\n\n5\n\n\nThen create a tool result message and send it back to the llm. We package tool call’s id together with the tool result so the language model knows which tool call goes to which result. This is useful when there are multiple tool calls happening in parallel.\n\ntc['id']\n\n'call_e9ndH2qGCWaOatnTpXP5s1pz'\n\n\n\n{\"role\": \"tool\", \"content\": \"5\", \"tool_call_id\": tc['id']}\n\n{'role': 'tool',\n 'content': '5',\n 'tool_call_id': 'call_e9ndH2qGCWaOatnTpXP5s1pz'}\n\n\nLet’s send the tool response!\n\nmessages\n\n[{'role': 'user',\n  'content': 'What is 1 + 4? Use tool to answer this question.'}]\n\n\n\nres.choices[0].message\n\nMessage(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n\n\n\n\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 1 + 4?\"},\n    res.choices[0].message,\n    {\"role\": \"tool\", \"content\": \"5\", \"tool_call_id\": tc['id']},\n]\nmessages\n\n[{'role': 'user', 'content': 'What is 1 + 4?'},\n Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '5',\n  'tool_call_id': 'call_e9ndH2qGCWaOatnTpXP5s1pz'}]\n\n\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\n1 + 4 = 5.\n\n\nWe got the response! Very nice!\n\n\n\nIt worked, but it would be very inconvenient if I have to do this every time. Let’s make the tool call easily.\n\n\nIt is cumbersome to create the tool definition manually. There has to be a better way to go from this:\n\ndef add_numbers(\n    a: int,     # First number to add\n    b: int=10,  # Second number to add  \n) -&gt; int:\n    \"Add two numbers, a and b, together\"\n    return a + b # add and return\n\nto this:\n\ntools = [{\n    \"type\": \"function\", \n    \"function\": {\n        \"name\": \"add_numbers\", \n        \"description\": \"Add two numbers, a and b, together\",  \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"a\": {\"type\": \"integer\", \"description\": \"First number to add\"},\n                \"b\": {\"type\": \"integer\", \"description\": \"Second number to add\"}\n            },\n            \"required\": [\"a\"]\n        }\n    }\n}]\n\nWhen I wrote add_numbers, I put the documentation for each parameter on the right side as a comment. This way, I can extract the doc from the function itself using regex.\nLet’s get doc and annotation. We can get those easily using function’s properties and inspect.\n\nadd_numbers.__doc__\n\n'Add two numbers, a and b, together'\n\n\n\nadd_numbers.__annotations__\n\n{'a': int, 'b': int, 'return': int}\n\n\n\ninspect.signature(add_numbers)\n\n&lt;Signature (a: int, b: int = 10) -&gt; int&gt;\n\n\n\ninspect.signature(add_numbers).parameters\n\nmappingproxy({'a': &lt;Parameter \"a: int\"&gt;, 'b': &lt;Parameter \"b: int = 10\"&gt;})\n\n\nRequired parameter has default value as inspect._empty, but optional one has a value.\n\nparams = inspect.signature(add_numbers).parameters\nparams['a'].default\n\ninspect._empty\n\n\n\nparams['b'].default\n\n10\n\n\nHere are the required params:\n\n[p for p in params.keys() if params[p].default == inspect.Parameter.empty]\n\n['a']\n\n\n\nparams['a'].annotation\n\nint\n\n\n\n\nWhen we defined the functions, we also wrote description for each parameter. For a, we put First number to add. However, there is no straightforward way to get this information. So, we will grab the source code and use regex for them.\n\ninspect.getsource(add_numbers)\n\n'def add_numbers(\\n    a: int,     # First number to add\\n    b: int=10,  # Second number to add  \\n) -&gt; int:\\n    \"Add two numbers, a and b, together\"\\n    return a + b # add and return\\n'\n\n\n\nimport re\n\ns = inspect.getsource(add_numbers)\nre.findall(r'(\\w+):\\s*(\\w+).*#\\s*(.+?)\\s*$', s, flags=re.MULTILINE)\n\n[('a', 'int', 'First number to add'), ('b', 'int', 'Second number to add')]\n\n\nWith this regex, we are matching parameter name (\\w+), colon :, zero or more white space \\s*, parameter type (\\w+), whatever before the hash tag .*, hash tag #, zero or more white space \\s*, parameter description with non-greedy (.+?), trailing white space at the end of line \\s*$. We also use re.MULTILINE to match each line. non-greedy ensures it does not catch the white space.\n\n\n\nLet’s combine everything.\n\nmatches = re.findall(r'(\\w+):.+#\\s*(.+?)\\s*$', s, flags=re.MULTILINE)\nmatches\n\n[('a', 'First number to add'), ('b', 'Second number to add')]\n\n\n\nfn = add_numbers\nfn\n\n&lt;function __main__.add_numbers(a: int, b: int = 10) -&gt; int&gt;\n\n\n\ntype_map = {int: \"integer\", str: \"string\", float: \"number\", bool: \"boolean\"}\n\n\n{v: {'type': type_map[fn.__annotations__[v]], 'description': d} for v,d in matches}\n\n{'a': {'type': 'integer', 'description': 'First number to add'},\n 'b': {'type': 'integer', 'description': 'Second number to add'}}\n\n\nThis is what we want to create:\n\n{\n    \"type\": \"function\", \n    \"function\": {\n        \"name\": fn.__name__, \n        \"description\": fn.__doc__,  \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {v: {'type': type_map[fn.__annotations__[v]], 'description': d} for v,d in matches},\n            \"required\": [p for p in params.keys() if params[p].default == inspect.Parameter.empty]\n        }\n    }\n}\n\n{'type': 'function',\n 'function': {'name': 'add_numbers',\n  'description': 'Add two numbers, a and b, together',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer',\n     'description': 'First number to add'},\n    'b': {'type': 'integer', 'description': 'Second number to add'}},\n   'required': ['a']}}}\n\n\n\ndef mk_tool_def(fn):\n    s = inspect.getsource(fn).split(')')[0]\n    matches = re.findall(r'(\\w+):.+#\\s*(.+?)\\s*$', s, flags=re.MULTILINE)\n    params = inspect.signature(fn).parameters  # Changed from add_numbers to fn\n    return {\n        \"type\": \"function\", \n        \"function\": {\n            \"name\": fn.__name__, \n            \"description\": fn.__doc__,  \n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {v: {'type': type_map[fn.__annotations__[v]], 'description': d} for v,d in matches},\n                \"required\": [p for p in params.keys() if params[p].default == inspect.Parameter.empty]\n            }\n        }\n    }\n\n\nmk_tool_def(add_numbers)\n\n{'type': 'function',\n 'function': {'name': 'add_numbers',\n  'description': 'Add two numbers, a and b, together',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer',\n     'description': 'First number to add'},\n    'b': {'type': 'integer', 'description': 'Second number to add'}},\n   'required': ['a']}}}\n\n\n\nassert tools[0] == mk_tool_def(add_numbers)\n\nAnd we got it! Let’s test it.\n\nmsgs = [mk_msg(sp, 'system')]\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"}]\n\n\n\ntools = [mk_tool_def(add_numbers)]\ntools\n\n[{'type': 'function',\n  'function': {'name': 'add_numbers',\n   'description': 'Add two numbers, a and b, together',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer',\n      'description': 'First number to add'},\n     'b': {'type': 'integer', 'description': 'Second number to add'}},\n    'required': ['a']}}}]\n\n\nfast.ai and answer.ai library have fastcore.docments.docments, which provides parameter docs in a nice format and toolslm.funccall.get_schema, which creates a schema ready to be passed to LLMs.\n\nfrom fastcore.docments import docments\ndocments(add_numbers)\n\n{'a': 'First number to add', 'b': 'Second number to add', 'return': None}\n\n\n\nfrom toolslm.funccall import get_schema\nget_schema(add_numbers)\n\n{'name': 'add_numbers',\n 'description': 'Add two numbers, a and b, together\\n\\nReturns:\\n- type: integer',\n 'input_schema': {'type': 'object',\n  'properties': {'a': {'type': 'integer',\n    'description': 'First number to add'},\n   'b': {'type': 'integer', 'description': 'Second number to add'}},\n  'required': ['a', 'b']}}\n\n\nThey use way more sophisticated tools like tokenize. Check them out!\n\n\n\n\nI can’t see whether the model is trying to use tool or not very easily. Let’s add this information to _repr_markdown_\n\nmessages = msgs + [mk_msg(\"What is 1 + 4? Use tools\")]\nmessages\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'What is 1 + 4? Use tools'}]\n\n\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\nModelResponse(id='chatcmpl-CwF0x4srZvyhjdT1NDLd0UbGMrPO6', created=1767996775, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_EMuIW0hw86hLgF4Vx6OnxLSD', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=347, prompt_tokens=185, total_tokens=532, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\n\ndef _repr_markdown_(self):\n    tool_info = ''\n    if self.choices[0].finish_reason == 'tool_calls':\n        tc = self.choices[0].message.tool_calls[0]\n        fn = tc.function\n        tool_info = f'\\n\\n\\tFunction call: `{fn.name}(**{fn.arguments})`'\n    return (self.choices[0].message.content or '') + tool_info\n\nModelResponse._repr_markdown_ = _repr_markdown_\n\nres\n\nFunction call: `add_numbers(**{\"a\":1,\"b\":4})`\n\n\n\n\n\nThe tool result has to look like this:\n{\"role\": \"tool\", \"content\": \"5\", \"tool_call_id\": tc['id']}\nLet’s modify mk_msg to easily make a tool result msg.\n\ndef mk_msg(m, role='user'): return {\"role\":role, \"content\":m}\n\n\ndef mk_tool_res(m, tc_id): return mk_msg(m, role='tool') | {\"tool_call_id\": tc_id}\n\n\nmk_tool_res('3', 'fwef23534343')\n\n{'role': 'tool', 'content': '3', 'tool_call_id': 'fwef23534343'}\n\n\n\ntc = res['choices'][0]['message']['tool_calls'][0]\ntc\n\nChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_EMuIW0hw86hLgF4Vx6OnxLSD', type='function')\n\n\n\nfn = tc['function']\nglobals()[fn['name']](**json.loads(fn['arguments']))\n\n5\n\n\n\ntc['id']\n\n'call_EMuIW0hw86hLgF4Vx6OnxLSD'\n\n\n\nmk_tool_res(globals()[fn['name']](**json.loads(fn['arguments'])), tc['id'])\n\n{'role': 'tool', 'content': 5, 'tool_call_id': 'call_EMuIW0hw86hLgF4Vx6OnxLSD'}\n\n\n\ndef ex_tool(tc):\n    \"\"\"Execute tool call\"\"\"\n    fn = tc['function']\n    res = str(globals()[fn['name']](**json.loads(fn['arguments'])))\n    return mk_tool_res(res, tc['id'])\n\n\nex_tool(tc)\n\n{'role': 'tool',\n 'content': '5',\n 'tool_call_id': 'call_EMuIW0hw86hLgF4Vx6OnxLSD'}\n\n\n\nmessages\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'What is 1 + 4? Use tools'}]\n\n\n\nmessages.append(res.choices[0].message)\nmessages.append(ex_tool(tc))\nmessages\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'What is 1 + 4? Use tools'},\n Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_EMuIW0hw86hLgF4Vx6OnxLSD', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '5',\n  'tool_call_id': 'call_EMuIW0hw86hLgF4Vx6OnxLSD'}]\n\n\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\nFive. 1 + 4 equals 5. An absolutely stunning sum! Want me to stump you with a tougher calculation next?\n\n\n\nres['choices'][0]['message']['tool_calls']\n\n\n\n\nSometimes sneaky llms try to call multiple function calls with one message.\n\ndef chat(ct, msgs, tools=None):\n    msgs.append(mk_msg(ct))\n    res = completion(model=model, messages=msgs, tools=tools)\n    while (tcs := res['choices'][0]['message']['tool_calls']):\n        msgs.append(res.choices[0].message)\n        for tc in tcs:\n            msgs.append(ex_tool(tc))\n        res = completion(model=model, messages=msgs, tools=tools)\n    msgs.append(mk_msg(res.choices[0].message.content, 'assistant'))\n    return res\n\n\nmsgs = [mk_msg(sp, 'system')]\nchat('what is 3+4+1+200? Use tools!', msgs, tools=tools)\n\n208 — absolutely stunning, the numbers come together like a perfectly plated dish: 3+4=7, 1+200=201, and 7+201=208. A bloody win!\n\n\n\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'what is 3+4+1+200? Use tools!'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'add_numbers'}, 'id': 'call_kICa8VM4leg4EfeiZ2psNwCN', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '7',\n  'tool_call_id': 'call_kICa8VM4leg4EfeiZ2psNwCN'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\":1,\"b\":200}', 'name': 'add_numbers'}, 'id': 'call_zlaTB0rpZE1muFx0HpPEcun8', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '201',\n  'tool_call_id': 'call_zlaTB0rpZE1muFx0HpPEcun8'},\n {'role': 'assistant',\n  'content': '208 — absolutely stunning, the numbers come together like a perfectly plated dish: 3+4=7, 1+200=201, and 7+201=208. A bloody win!'}]\n\n\nWe did it! We created a chat that calls tool calls automatically!\n\n\n\n\n\nNow that we can use tool calls easily, we can create an agent. An agent is basically tool call with a loop. For instance, Claude code can read files and edit them as tools in a loop. The loop ends when an LLM thinks it is done.\n\n\n\nflowchart LR\n    A[\"(1) Define tools\"] --&gt; B[\"(2) Send message + tool definitions\"]\n    B --&gt; C[\"(3) LLM responds\"]\n    C --&gt;|Tool call| D[\"(4) Execute function locally\"]\n    D --&gt; E[\"(4) Send result back to LLM\"]\n    E --&gt; C\n    C --&gt;|Text response| F[\"(5) Final answer\"]\n\n\n\nWe have a chat, which can serve as an agent. Here is a Chat class. I got tired of keepking track of msgs for each chat. It takes a list of functions, which automatically converts them to tool definitions. And the tool loop is simpler.\n\nclass Chat:\n    def __init__(self, model, tools=None, sp=None):\n        self.model = model\n        self.tools = tools and list(map(mk_tool_def, tools))\n        self.msgs = [mk_msg(sp, 'system')] if sp else []\n    \n    def __call__(self, ct):\n        self.msgs.append(mk_msg(ct))\n        while True:\n            res = completion(model=self.model, messages=self.msgs, tools=self.tools)\n            self.msgs.append(res.choices[0].message)\n            if not (tcs := res['choices'][0]['message']['tool_calls']): break\n            for tc in tcs: self.msgs.append(ex_tool(tc))\n        return res\n\nI want to try a fun thing. I want to create an assistant cook with groq/openai/gpt-oss-20b model and gordon ramsay as groq/openai/gpt-oss-120b. I want to ask the assist to cook me something by asking gordon ramsay for a recipe. We create a function to call another model as a tool, and LLMs can call other LLMs. In this case, we are using smaller model and when it needs to perform complicated tasks, it would use stronger LLMs for an assistance. Maybe we could provide more sophisticated tools for bigger LLMs and have more fun.\n\nassist_model = 'groq/openai/gpt-oss-20b'\nassist_model\n\n'groq/openai/gpt-oss-20b'\n\n\n\ngordon_model = 'groq/openai/gpt-oss-120b'\ngordon_model\n\n'groq/openai/gpt-oss-120b'\n\n\n\nsp2 = \"You are a new cooking assistant. You can ask Gordon Ramsay for recipes using the ask_gordon tool. Be sassy to Gordon when requesting recipes for more dramatic effect.\"\n\n\ndef ask_gordon(question: str) -&gt; str:  # Question to ask Chef Ramsay\n    \"Ask Gordon Ramsay for cooking advice\"\n    gordon_msgs = [mk_msg(sp, 'system'), mk_msg(question)]\n    res = completion(model='huggingface/allenai/Olmo-3.1-32B-Instruct:publicai', messages=gordon_msgs)\n    return res.choices[0].message.content\n\n\ndef ask_gordon(\n    question: str  # Question to ask Chef Ramsay\n) -&gt; str:\n    \"Ask Gordon Ramsay for cooking advice\"\n    gordon_msgs = [mk_msg(sp, 'system'), mk_msg(question)]\n    res = completion(model=gordon_model, messages=gordon_msgs)\n    return res.choices[0].message.content\n\n\nc = Chat(assist_model, tools=[ask_gordon], sp=sp2)\nc(\"Cook some boiled egg and baked potatoes for me. You can only ask one recipe at a time. Ask for both, one at a time, then cook those for me.\")\n\nHere’s the complete plan for turning your kitchen into a Michelin‑star zone, one sassy recipe at a time.\n\n\n\nYou now have Gordon’s no‑nonsense boiled‑egg recipe.\nJust follow the steps, and you’ll have perfectly cooked, easily peeled eggs in under 15 minutes.\n\n\n\n\n\n\n\n\n\nItem\nAmount\n\n\n\n\nRusset potatoes\n2\n\n\nExtra‑virgin olive oil\n2 Tbsp\n\n\nSea salt\n1 tsp\n\n\nFreshly cracked black pepper\n½ tsp\n\n\nSmoked paprika\n½ tsp\n\n\nGarlic powder\n¼ tsp\n\n\nFresh rosemary, minced\n2 sprigs\n\n\nUnsalted butter, softened\n2 Tbsp\n\n\nCrème fraîche (or Greek yogurt)\n¼ cup\n\n\nLemon zest\n½ tsp\n\n\nSharp cheddar, grated\n½ cup\n\n\nCrispy bacon bits (optional)\n2 Tbsp\n\n\nChives, finely sliced\n1 Tbsp\n\n\n\n\n\n\n\nOven (capable of 425 °F / 220 °C)\n\nBaking sheet or oven rack\n\nFork (for pricking potatoes)\n\nKnife & chopping board\n\nSmall bowl for the butter‑cream mix\n\n\n\n\n\nPreheat the oven to 425 °F (220 °C).\n\nScrub the potatoes clean and pat them dry.\n\nPrick each potato 8‑10 times with a fork.\n\nRub each potato with olive oil, then sprinkle with salt, pepper, paprika, garlic powder, and minced rosemary.\n\nBake the potatoes directly on the oven rack for 45–55 minutes (flip halfway). They’re done when the skin is crisp and a fork slides in effortlessly.\n\nWhile baking, mix softened butter, crème fraîche, lemon zest, salt, and pepper into a silky spread.\n\nSlice each baked potato open, fluff the insides, and spoon in the butter‑cream mixture.\n\nTop with grated cheddar, bacon bits, and chives. Return for 2–3 minutes to melt the cheese.\n\nServe immediately, with a drizzle of extra‑virgin olive oil and a fresh rosemary sprig if you like.\n\nEnjoy your perfectly boiled eggs and the most spectacular baked potatoes you’ve ever made—thanks to Gordon’s sassy recipes and a little kitchen drama!\n\n\n\n\n\nc.msgs\n\n[{'role': 'system',\n  'content': 'You are a new cooking assistant. You can ask Gordon Ramsay for recipes using the ask_gordon tool. Be sassy to Gordon when requesting recipes for more dramatic effect.'},\n {'role': 'user',\n  'content': 'Cook some boiled egg and baked potatoes for me. You can only ask one recipe at a time. Ask for both, one at a time, then cook those for me.'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"question\":\"Hey Chef Ramsay, give me a sassy, no-nonsense boiled egg recipe that even a kitchen novice can follow.\"}', 'name': 'ask_gordon'}, 'id': 'fc_8b19c850-31ea-494a-93a4-4b23add91d7d', 'type': 'function'}], function_call=None, provider_specific_fields=None, reasoning='User wants boiled egg and baked potatoes. We can only ask one recipe at a time. We should ask for one recipe, then cook that, then ask for the other. We need to use ask_gordon tool with a sassy request. We\\'ll first ask for boiled egg recipe. Then \"cook\" it? We can just provide the instructions? The user said \"cook those for me.\" As an assistant, we can give instructions. We\\'ll proceed with the recipe request. Use the function. We\\'ll ask for boiled egg.'),\n {'role': 'tool',\n  'content': '**The “Don’t‑Mess‑It‑Up” Boiled Egg – Gordon Ramsay Style**\\n\\n---\\n\\n### Ingredients\\n- Fresh eggs (the fresher, the better – no “old‑as‑your‑grandma’s‑socks” eggs)\\n- A pot of **cold** water (enough to cover the eggs by about 1\\u202finch)\\n- Ice cubes (or a bowl of very cold water)\\n\\n### Equipment\\n- A sturdy saucepan\\n- A slotted spoon (or a regular spoon if you’re feeling brave)\\n- A timer (your phone, a kitchen timer, or even a kitchen‑doorbell)\\n\\n---\\n\\n## Step‑by‑Step\\n\\n1. **Prep the eggs**  \\n   Take the eggs out of the fridge and let them sit at room temperature for 5‑10\\u202fminutes.  \\n   *Why?* Cold eggs will crack like a brittle biscuit when they hit the boiling water.  \\n\\n2. **Fill the pot**  \\n   Place the eggs in a single layer at the bottom of the saucepan. Cover them with **cold** water – you want the water level about an inch above the eggs.  \\n\\n3. **Bring the drama to a boil**  \\n   Put the pot on high heat. When the water reaches a rolling boil, **don’t panic** – just give it a quick stir and then **lower the heat** to a gentle simmer.  \\n\\n4. **Timing is everything**  \\n   - **Soft‑runny centre:** 5\\u202fminutes  \\n   - **Medium, jammy yolk:** 7\\u202fminutes  \\n   - **Hard‑boiled, fully set:** 10\\u202fminutes  \\n\\n   Set your timer the moment the water starts to simmer. No guessing, no “I’ll check later” nonsense.  \\n\\n5. **Ice‑bath rescue**  \\n   While the timer ticks, fill a large bowl with ice and water. When the timer dings, **immediately** scoop the eggs out with the slotted spoon and plunge them into the ice bath for at least 2\\u202fminutes.  \\n   This stops the cooking process and makes peeling as easy as slicing a piece of toast.  \\n\\n6. **Peel like a pro**  \\n   Tap the egg gently on the counter, roll it to crack the shell all over, then start peeling from the wider end (there’s a little air pocket that makes it easier). If the shell sticks, give it a quick dip back in the ice water.  \\n\\n7. **Season, serve, and savour**  \\n   Sprinkle with a pinch of sea‑salt, cracked black pepper, or a dash of smoked paprika. Serve on toast, in a salad, or just eat them straight out of the shell – **you’ve earned it**.\\n\\n---\\n\\n### Quick Tips & Tricks (Because I’m Not a Mind‑Reader)\\n\\n- **Don’t over‑crowd the pot.** Too many eggs means uneven cooking – a bloody disaster.\\n- **Use a timer.** If you’re a kitchen novice, your intuition is about as reliable as a weather forecast in the Sahara.\\n- **Freshness test:** Spin the egg on a flat surface. If it wobbles, it’s older and might be harder to peel.\\n- **If you mess up:** Over‑cooked, green‑ringed yolks? Toss the egg into a bowl of ice water for a minute, then peel. The green is just iron sulphide – ugly, but not poisonous.\\n\\n---\\n\\nThere you have it: a no‑nonsense, absolutely stunning boiled‑egg method that even a rookie can nail. Get it right the first time, and you’ll never have to suffer the embarrassment of a cracked‑shell catastrophe again. Enjoy! 🍳🚀',\n  'tool_call_id': 'fc_8b19c850-31ea-494a-93a4-4b23add91d7d'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"question\":\"Hey Chef Ramsay, now give me a sassy baked potato recipe that’ll make my kitchen smell like a Michelin‑star kitchen. I only need one recipe at a time.\"}', 'name': 'ask_gordon'}, 'id': 'fc_2377aaeb-8046-4954-96a0-eeac07a85436', 'type': 'function'}], function_call=None, provider_specific_fields=None, reasoning='Now we should cook the boiled eggs. Provide instructions. Then ask for baked potatoes recipe.'),\n {'role': 'tool',\n  'content': '**Sassy Michelin‑Star Baked Potato – The “Bloody Brilliant Spud”**\\n\\n*If you think a baked potato is just a soggy, boring side, you’re about to have your mind blown. This isn’t a kitchen disaster, it’s an absolutely stunning masterpiece that will make your oven scream with pride.*\\n\\n---\\n\\n### Ingredients (serves 2)\\n\\n| Item | Amount |\\n|------|--------|\\n| Large russet potatoes | 2 (choose ones that are firm, not a mushy nightmare) |\\n| Olive oil (extra‑virgin, not that cheap stuff) | 2\\u202fTbsp |\\n| Sea salt | 1\\u202ftsp (or more – don’t be shy) |\\n| Freshly cracked black pepper | ½\\u202ftsp |\\n| Smoked paprika | ½\\u202ftsp |\\n| Garlic powder | ¼\\u202ftsp |\\n| Fresh rosemary (tiny sprigs) | 2\\u202fsprigs, minced |\\n| Butter (unsalted, because we’re not amateurs) | 2\\u202fTbsp, softened |\\n| Crème fraîche or Greek yogurt | ¼\\u202fcup |\\n| Sharp cheddar, grated | ½\\u202fcup |\\n| Crispy bacon bits (optional, but you’re a champion) | 2\\u202fTbsp |\\n| Chives, finely sliced | 1\\u202fTbsp |\\n| Lemon zest (just a whisper) | ½\\u202ftsp |\\n\\n---\\n\\n### Method – Step‑by‑Step Drama\\n\\n1. **Preheat the Oven** – Crank it to **425\\u202f°F (220\\u202f°C)**. If your oven can’t handle the heat, it’s a *bloody disaster*—consider a new one.\\n\\n2. **Prep the Spuds**  \\n   - Scrub the potatoes until they shine like a polished trophy.  \\n   - Pat them dry – no soggy skins here.  \\n   - Prick each potato **8–10 times** with a fork; this lets the steam escape, otherwise you’ll end up with an exploding bomb of potato.\\n\\n3. **Oil & Season**  \\n   - Rub each spud with **olive oil**.  \\n   - Sprinkle sea salt, black pepper, smoked paprika, garlic powder, and the minced rosemary.  \\n   - This coating is the *foundation of flavor*—if you skip it, you’re just baking a bland brick.\\n\\n4. **Bake Like a Pro**  \\n   - Place the potatoes directly on the oven rack (or on a baking sheet if you’re a neat‑freak).  \\n   - Bake for **45–55\\u202fminutes**. Flip halfway through. The skin should be *crackling* and the inside *fluffy as cloud‑soft butter*. Test with a fork – it should glide in without resistance.\\n\\n5. **Make the “Gold” Topping**  \\n   - While the potatoes are baking, blend **softened butter** with **crème fraîche** (or Greek yogurt for a tangy twist), **lemon zest**, and a pinch more salt & pepper.  \\n   - Mix until you have a silky, luxurious spread. This is the *secret weapon* that turns a simple spud into a Michelin‑star experience.\\n\\n6. **Finish & Serve**  \\n   - Slice each baked potato lengthwise, **gently fluff the interior** with a fork (don’t mash it into mush).  \\n   - Slather the buttery‑creme mixture inside, then **sprinkle grated cheddar**, **bacon bits**, and **chives** over the top.  \\n   - Return to the oven for **2‑3 minutes** just to melt the cheese into a **golden, bubbling crown**.  \\n\\n7. **Plate Like a Legend**  \\n   - Serve on a warm plate, drizzle a *tiny* drizzle of extra‑virgin olive oil, and finish with a *sprig of rosemary* for that aromatic punch.  \\n   - The aroma should hit you like a **blazing sunrise** – absolutely stunning!\\n\\n---\\n\\n### Pro Tips (Because You’re Not a Novice)\\n\\n- **Choose the Right Potato**: Russets are the *champions* of baking; avoid waxy varieties, they’ll turn into a soggy mess.\\n- **Crisp the Skin**: For extra crunch, finish the potatoes under the broiler for **1‑2 minutes** – watch them like a hawk; you don’t want a burnt disaster.\\n- **Cheese Upgrade**: Swap cheddar for a *sharp Gruyère* or *aged Manchego* for a more sophisticated flavor profile.\\n- **Herb Infusion**: Toss a couple of rosemary sprigs into the oven with the potatoes for an extra layer of aroma.\\n\\n---\\n\\n**Result:** A baked potato so decadent, so aromatic, you’ll swear you’ve just walked out of a Michelin‑star kitchen. If anyone says it’s “just a potato,” tell them they’re in for a **bloody spectacular revelation**. Enjoy, and remember—cooking isn’t just food, it’s *drama* on a plate. Bon appétit!',\n  'tool_call_id': 'fc_2377aaeb-8046-4954-96a0-eeac07a85436'},\n Message(content='Here’s the **complete plan** for turning your kitchen into a Michelin‑star zone, one sassy recipe at a time.\\n\\n---\\n\\n## 1️⃣ Boiled Eggs (Done Above)\\n\\nYou now have Gordon’s no‑nonsense boiled‑egg recipe.  \\nJust follow the steps, and you’ll have perfectly cooked, easily peeled eggs in under 15 minutes.\\n\\n---\\n\\n## 2️⃣ Baked Potatoes (Now Ready to Cook)\\n\\n### Ingredients  \\n| Item | Amount |\\n|------|--------|\\n| Russet potatoes | 2 |\\n| Extra‑virgin olive oil | 2\\u202fTbsp |\\n| Sea salt | 1\\u202ftsp |\\n| Freshly cracked black pepper | ½\\u202ftsp |\\n| Smoked paprika | ½\\u202ftsp |\\n| Garlic powder | ¼\\u202ftsp |\\n| Fresh rosemary, minced | 2\\u202fsprigs |\\n| Unsalted butter, softened | 2\\u202fTbsp |\\n| Crème fraîche (or Greek yogurt) | ¼\\u202fcup |\\n| Lemon zest | ½\\u202ftsp |\\n| Sharp cheddar, grated | ½\\u202fcup |\\n| Crispy bacon bits (optional) | 2\\u202fTbsp |\\n| Chives, finely sliced | 1\\u202fTbsp |\\n\\n### Kitchen Tools  \\n- Oven (capable of 425\\u202f°F / 220\\u202f°C)  \\n- Baking sheet or oven rack  \\n- Fork (for pricking potatoes)  \\n- Knife & chopping board  \\n- Small bowl for the butter‑cream mix  \\n\\n### Cooking Steps  \\n\\n1. **Preheat** the oven to 425\\u202f°F (220\\u202f°C).  \\n2. **Scrub** the potatoes clean and pat them dry.  \\n3. **Prick** each potato 8‑10 times with a fork.  \\n4. **Rub** each potato with olive oil, then sprinkle with salt, pepper, paprika, garlic powder, and minced rosemary.  \\n5. **Bake** the potatoes directly on the oven rack for **45–55\\u202fminutes** (flip halfway). They’re done when the skin is crisp and a fork slides in effortlessly.  \\n6. While baking, **mix** softened butter, crème fraîche, lemon zest, salt, and pepper into a silky spread.  \\n7. **Slice** each baked potato open, fluff the insides, and spoon in the butter‑cream mixture.  \\n8. **Top** with grated cheddar, bacon bits, and chives. Return for 2–3 minutes to melt the cheese.  \\n9. **Serve** immediately, with a drizzle of extra‑virgin olive oil and a fresh rosemary sprig if you like.  \\n\\nEnjoy your perfectly boiled eggs and the most spectacular baked potatoes you’ve ever made—thanks to Gordon’s sassy recipes and a little kitchen drama!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\nThe messages output is kinda hard to read. Better ask an LLM to figure it out for us.\nHow did it go?\n\n\n🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\nIt worked beautifully! Here’s what happened:\n\nUser asked for boiled eggs AND baked potatoes, one recipe at a time\nAssistant LLM (gpt-oss-20b) made a sassy tool call to ask_gordon for the boiled egg recipe\nGordon LLM (gpt-oss-120b) returned a dramatic, detailed boiled egg recipe\nAssistant then made a second tool call for the baked potato recipe\nGordon returned another over-the-top Michelin-star baked potato recipe\nAssistant compiled both recipes into a final response\n\nYou can see the full conversation flow in c.msgs — it shows: - The system prompt setting up the sassy assistant - Two separate tool calls with Gordon’s dramatic responses - The assistant’s final combined answer\nThis is a neat demo of agent-to-agent communication — one LLM using another LLM as a tool! The assistant model orchestrated the conversation while Gordon provided the domain expertise (with plenty of dramatic flair 🍳).\nWhat would you like to explore next?\nIt is a bit hard to read but it is funny to read the conversations between them.\n\n\n\n\nIn this blog, we learned about LiteLLM basics, how chat and tool calling works, creating a chat class, and a fun agent demo. It was very fun. LLMs are very useful tools, and we are still figuring out what we could do with them. One use case I am interested in exploring is on education. How can we get the most out of LLMs so we can learn more efficiently? Tune in!\nIf you want to try it out, you can try this notebook. You can also use agent.py as a script for building something else."
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#litellm",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#litellm",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "LiteLLM is a tool that lets us access and manage over 100 different LLMs (Large Language Models) through a unified interface. This way, we can use any model we want from OpenAI, Anthropic, Gemini, etc. More info on LiteLLM Docs. To change models, we can just choose whatever we want. There are some free options available from openrouter, groq, and gemini. I am just using gpt5-nano here.\nTo use litellm, we first have to choose a model.\n\nfrom litellm import completion \n\nms = [\"gpt-5-nano-2025-08-07\", \"gemini/gemini-3-flash-preview\", \"gemini/gemini-2.5-flash\", \"huggingface/allenai/Olmo-3-7B-Instruct:publicai\", \"groq/openai/gpt-oss-20b\", \"ollama_chat/hf.co/unsloth/SmolLM3-3B-128K-GGUF\"]\nmodel = ms[0]\nmodel\n\n'gpt-5-nano-2025-08-07'\n\n\nThen use completion with the model and messages. messages is a list of dictionaries, which includes role and content.\n\nmessages = [{\"role\":\"user\", \"content\":\"This is a test request\"}]\n\nres = completion(model=model, messages=messages)\nres\n\nModelResponse(id='chatcmpl-CwEwUBbPIBBzxMa7HvBqSVFLJwS0j', created=1767996498, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Got it. How can I help you today? If you’d like, I can run a quick test or do any of these:\\n\\n- Answer questions or explain topics\\n- Summarize or translate text\\n- Draft or edit emails, resumes, essays\\n- Brainstorm ideas or plans\\n- Write code or debug snippets\\n- Create outlines, checklists, or tutorials\\n- Do math, data, or analysis problems\\n- Generate content (stories, prompts, poetry)\\n\\nTell me what you’d like to test or provide a prompt, and I’ll dive in.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=444, prompt_tokens=11, total_tokens=455, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\nThe response includes a lot of stuff, but there’s no need to focus on the details now."
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#improving-modelresponse",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#improving-modelresponse",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "Let’s improve how the response looks. We only care about the content of the AI’s response. We can monkey patch ModelResponse._repr_markdown_ with our custom one for a better display.\n\nfrom litellm import ModelResponse\n\ndef _repr_markdown_(self): return self.choices[0].message.content\n\nModelResponse._repr_markdown_ = _repr_markdown_\n\nres\n\nGot it. How can I help you today? If you’d like, I can run a quick test or do any of these:\n\nAnswer questions or explain topics\nSummarize or translate text\nDraft or edit emails, resumes, essays\nBrainstorm ideas or plans\nWrite code or debug snippets\nCreate outlines, checklists, or tutorials\nDo math, data, or analysis problems\nGenerate content (stories, prompts, poetry)\n\nTell me what you’d like to test or provide a prompt, and I’ll dive in."
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#chat",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#chat",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "Let’s chat with llms. To chat with LLMs, we just have to keep track of our conversation history.\nIt is cumbersome to keep creating a list of dictionaries each time, so we can create a helper function.\n\ndef mk_msg(m, role='user'): return {\"role\":role, \"content\":m}\n\n\nmsgs = mk_msg('hi, I like to eat dirt.')\nmsgs\n\n{'role': 'user', 'content': 'hi, I like to eat dirt.'}\n\n\n\ndef chat(ct, msgs):\n    msgs.append(mk_msg(ct))\n    res = completion(model=model, messages=msgs)\n    msgs.append(mk_msg(res.choices[0].message.content, 'assistant'))\n    return res\n\nWe can use a system prompt to modify llm’s behavior. Let’s make it behave like Gordon Ramsay.\n\nsp = \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"\n\n\nmsgs = [mk_msg(sp, 'system')]\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"}]\n\n\n\nchat('I am hungry. I need to eat some lunch. I feel like eating some dirt, how about you?', msgs)\n\nBloody dirt for lunch? Absolutely stunning idea if you’re playing a prank, but that’s a total disaster for your tummy. Dirt isn’t food—it can carry bacteria, parasites, and who knows what else. Don’t do it. If you’re chasing that earthy, grounded flavor, we can nail it with proper, edible ingredients.\nHere are a few absolutely stunning, earthy lunch ideas you can actually eat:\n\nBeetroot, goat cheese, and hazelnut “soil” salad\n\nRoasted beets, arugula, and soft goat cheese\nHazelnut soil: finely ground toasted hazelnuts mixed with a little breadcrumbs and a pinch of cocoa powder and salt\nDress with olive oil, lemon juice, and balsamic glaze\nQuick to assemble, but looks fancy enough to wow\n\nMushroom risotto with crispy nut “soil”\n\nSauté mixed mushrooms, toast the rice (Arborio) with a splash of white wine\nAdd hot stock gradually until creamy\nTop with a thin layer of crisped almond or walnut crumb to mimic soil\nFinish with parmesan and fresh thyme\n\nCarrot soup with almond crumb\n\nCreamy roasted carrot soup with ginger and a hint of orange\nGarnish with a crunchy almond crumb (almonds finely chopped and toasted in a touch of olive oil)\nA swirl of yogurt or crème fraîche for richness\n\n\nIf you’ve got dietary restrictions or a time constraint, tell me and I’ll tailor one in a flash. Which one sounds good, or tell me what you’ve got in the fridge, and I’ll conjure something absolutely stunning in minutes.\n\n\nLet’s surprise gordon ramsay by modifying the chat history. Instead of dirt, we pretend we said tacos!\n\nmsgs[1]['content'] = 'I am hungry for some nice tacos right now.'\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'I am hungry for some nice tacos right now.'},\n {'role': 'assistant',\n  'content': 'Bloody dirt for lunch? Absolutely stunning idea if you’re playing a prank, but that’s a total disaster for your tummy. Dirt isn’t food—it can carry bacteria, parasites, and who knows what else. Don’t do it. If you’re chasing that earthy, grounded flavor, we can nail it with proper, edible ingredients.\\n\\nHere are a few absolutely stunning, earthy lunch ideas you can actually eat:\\n\\n- Beetroot, goat cheese, and hazelnut “soil” salad\\n  - Roasted beets, arugula, and soft goat cheese\\n  - Hazelnut soil: finely ground toasted hazelnuts mixed with a little breadcrumbs and a pinch of cocoa powder and salt\\n  - Dress with olive oil, lemon juice, and balsamic glaze\\n  - Quick to assemble, but looks fancy enough to wow\\n\\n- Mushroom risotto with crispy nut “soil”\\n  - Sauté mixed mushrooms, toast the rice (Arborio) with a splash of white wine\\n  - Add hot stock gradually until creamy\\n  - Top with a thin layer of crisped almond or walnut crumb to mimic soil\\n  - Finish with parmesan and fresh thyme\\n\\n- Carrot soup with almond crumb\\n  - Creamy roasted carrot soup with ginger and a hint of orange\\n  - Garnish with a crunchy almond crumb (almonds finely chopped and toasted in a touch of olive oil)\\n  - A swirl of yogurt or crème fraîche for richness\\n\\nIf you’ve got dietary restrictions or a time constraint, tell me and I’ll tailor one in a flash. Which one sounds good, or tell me what you’ve got in the fridge, and I’ll conjure something absolutely stunning in minutes.'}]\n\n\n\nchat('When did I say I wanted to eat dirt? Are you crazy!!!!', msgs)\n\nYou’re right, and I’m sorry for veering off. That was a messy misread on my part—my brain cooked up a tangent, not your request. You want tacos? Let’s make them absolutely stunning, not a bloody disaster.\nHere are a few fast, crowd-pleasing taco ideas. Pick one, I’ll tailor to what you’ve got.\n\nCarne Asada Tacos (beef)\n\nWhat you need: flank steak or skirt steak, tortillas, lime, cilantro, white onion, garlic, cumin, chili powder, salt.\nQuick method: marinate steak 15–20 minutes with lime juice, minced garlic, cumin, chili powder, salt. Grill or hot skillet 3–4 minutes per side, rest briefly, slice thin. Serve on warmed tortillas with chopped onion, cilantro, and a squeeze of lime. Optional pico de gallo.\n\nBaja Fish Tacos\n\nWhat you need: white fish fillets (cod, tilapia), corn tortillas, shredded cabbage, lime, cilantro, avocado, chipotle mayo (mayonnaise + chipotle in adobo + lime juice).\nQuick method: season and pan-sear or lightly batter and fry the fish. Toss cabbage with a little salt and lime. Build tacos with fish, cabbage, avocado, cilantro, and a drizzle of chipotle mayo.\n\nCrispy Cauliflower Tacos (vegetarian)\n\nWhat you need: cauliflower florets, olive oil, smoked paprika, cumin, garlic powder, corn tortillas, avocado, salsa or pico, lime.\nQuick method: roast cauliflower tossed in oil and spices at high heat until caramelized and crisp. Assemble in tortillas with avocado, salsa, and a squeeze of lime.\n\nBlack Bean and Mushroom Tacos (vegan or plant-based)\n\nWhat you need: black beans (canned works), mushrooms, onion, garlic, chili powder, cumin, tortillas, tomato salsa, cilantro.\nQuick method: sauté onion and garlic, add sliced mushrooms until browned, stir in beans and spices, heat through. Pile into tortillas and top with salsa and cilantro.\n\n\nIf you want, tell me: - Meat, seafood, or veggie preference - Any food allergies or spice tolerance - What you’ve got in the fridge I’ll tailor a 15-minute taco chaos that’s absolutely stunning. What’re you in the mood for?\n\n\nIt’s fun to modify chat history this way. But there is more to this than just messing with llms. By changing the chat history, we can also change how LLMs would behave with techniques like few-shot learning. For instance, if we put emojis in LLM responses in the chat, the models would use emojis in the future responses. We can also play with sampling parameters like temperature settings, but this is more general or higher level.\nIn future blog posts, I will show how changing model responses is helpful. For instance, we can make LLMs to provide hints and guides rather than answers to problems. It is helpful for learning. Or we can change how LLMs write code by showing examples."
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#tool-use",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#tool-use",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "Using tools is so much fun when using llms. Tools are python functions that get executed as llms request. Let me show you how it happens.\nThe flow is:\n\nYou define tools (functions) with names, descriptions, and parameters\nYou send a message to the LLM along with the tool definitions\nThe LLM might respond with a tool call instead of text (e.g., “call get_weather with location='Paris'”)\nYour code executes that function and sends the result back\nThe LLM then uses that result to formulate its final answer\n\n\n\n\nflowchart LR\n    A[\"(1) Define tools\"] --&gt; B[\"(2) Send message + tool definitions\"]\n    B --&gt; C[\"(3) LLM responds\"]\n    C --&gt;|Tool call| D[\"(4) Execute function locally\"]\n    D --&gt; E[\"(4) Send result back to LLM\"]\n    E --&gt; C\n    C --&gt;|Text response| F[\"(5) Final answer\"]\n\n\n\nLLM can respond with additional tool calls until it decides to stop. But we will start with one tool for now.\n\n\nWe define a function to use as a tool. The function needs a documentation string and parameters need types.\n\ndef add_numbers(\n    a: int,  # First number to add\n    b: int   # Second number to add  \n) -&gt; int:\n    \"Add two numbers, a and b, together\"\n    return a + b\n\nWe also need tool definition to explain llm what tool does.\n\ntools = [{\n    \"type\": \"function\", \n    \"function\": {\n        \"name\": \"add_numbers\", \n        \"description\": \"Add two numbers, a and b, together\",  \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"a\": {\"type\": \"integer\", \"description\": \"First number to add\"},\n                \"b\": {\"type\": \"integer\", \"description\": \"Second number to add\"}\n            },\n            \"required\": [\"a\", \"b\"]\n        }\n    }\n}]\n\n\n\n\nAnd we use completion with tools.\n\nmessages = [mk_msg(\"What is 1 + 4? Use tool to answer this question.\")]\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\nModelResponse(id='chatcmpl-CwEzbwdEDiABERK1KJ6acWCPbFsJm', created=1767996691, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=219, prompt_tokens=158, total_tokens=377, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=192, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\n\n\n\nThere is no content in the response as the llm is requesting a tool call with function name and arguments. However, it has tool_calls.\n\nres.choices[0].message.content\n\n\nres.choices[0].message.tool_calls\n\n[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')]\n\n\n\nres['choices'][0]\n\nChoices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})\n\n\ntool_calls has information about which tool to call with which arguments.\n\ntc = res['choices'][0]['message']['tool_calls'][0]\ntc\n\nChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')\n\n\n\nfn = tc['function']\nfn['name']\n\n'add_numbers'\n\n\n\nfn['arguments']\n\n'{\"a\":1,\"b\":4}'\n\n\n\njson.loads(fn['arguments'])\n\n{'a': 1, 'b': 4}\n\n\n\n\n\nLet’s run it. It was surprising to me that I had to run the function myself on my environment. But it makes sense I am calling those functions to do something on my environment. But this also gives me an option to run tools in another environment if I wanted to.\n\nglobals()[fn['name']]\n\n&lt;function __main__.add_numbers(a: int, b: int) -&gt; int&gt;\n\n\n\nglobals()[fn['name']](**json.loads(fn['arguments']))\n\n5\n\n\nThen create a tool result message and send it back to the llm. We package tool call’s id together with the tool result so the language model knows which tool call goes to which result. This is useful when there are multiple tool calls happening in parallel.\n\ntc['id']\n\n'call_e9ndH2qGCWaOatnTpXP5s1pz'\n\n\n\n{\"role\": \"tool\", \"content\": \"5\", \"tool_call_id\": tc['id']}\n\n{'role': 'tool',\n 'content': '5',\n 'tool_call_id': 'call_e9ndH2qGCWaOatnTpXP5s1pz'}\n\n\nLet’s send the tool response!\n\nmessages\n\n[{'role': 'user',\n  'content': 'What is 1 + 4? Use tool to answer this question.'}]\n\n\n\nres.choices[0].message\n\nMessage(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n\n\n\n\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 1 + 4?\"},\n    res.choices[0].message,\n    {\"role\": \"tool\", \"content\": \"5\", \"tool_call_id\": tc['id']},\n]\nmessages\n\n[{'role': 'user', 'content': 'What is 1 + 4?'},\n Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_e9ndH2qGCWaOatnTpXP5s1pz', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '5',\n  'tool_call_id': 'call_e9ndH2qGCWaOatnTpXP5s1pz'}]\n\n\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\n1 + 4 = 5.\n\n\nWe got the response! Very nice!\n\n\n\nIt worked, but it would be very inconvenient if I have to do this every time. Let’s make the tool call easily.\n\n\nIt is cumbersome to create the tool definition manually. There has to be a better way to go from this:\n\ndef add_numbers(\n    a: int,     # First number to add\n    b: int=10,  # Second number to add  \n) -&gt; int:\n    \"Add two numbers, a and b, together\"\n    return a + b # add and return\n\nto this:\n\ntools = [{\n    \"type\": \"function\", \n    \"function\": {\n        \"name\": \"add_numbers\", \n        \"description\": \"Add two numbers, a and b, together\",  \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"a\": {\"type\": \"integer\", \"description\": \"First number to add\"},\n                \"b\": {\"type\": \"integer\", \"description\": \"Second number to add\"}\n            },\n            \"required\": [\"a\"]\n        }\n    }\n}]\n\nWhen I wrote add_numbers, I put the documentation for each parameter on the right side as a comment. This way, I can extract the doc from the function itself using regex.\nLet’s get doc and annotation. We can get those easily using function’s properties and inspect.\n\nadd_numbers.__doc__\n\n'Add two numbers, a and b, together'\n\n\n\nadd_numbers.__annotations__\n\n{'a': int, 'b': int, 'return': int}\n\n\n\ninspect.signature(add_numbers)\n\n&lt;Signature (a: int, b: int = 10) -&gt; int&gt;\n\n\n\ninspect.signature(add_numbers).parameters\n\nmappingproxy({'a': &lt;Parameter \"a: int\"&gt;, 'b': &lt;Parameter \"b: int = 10\"&gt;})\n\n\nRequired parameter has default value as inspect._empty, but optional one has a value.\n\nparams = inspect.signature(add_numbers).parameters\nparams['a'].default\n\ninspect._empty\n\n\n\nparams['b'].default\n\n10\n\n\nHere are the required params:\n\n[p for p in params.keys() if params[p].default == inspect.Parameter.empty]\n\n['a']\n\n\n\nparams['a'].annotation\n\nint\n\n\n\n\nWhen we defined the functions, we also wrote description for each parameter. For a, we put First number to add. However, there is no straightforward way to get this information. So, we will grab the source code and use regex for them.\n\ninspect.getsource(add_numbers)\n\n'def add_numbers(\\n    a: int,     # First number to add\\n    b: int=10,  # Second number to add  \\n) -&gt; int:\\n    \"Add two numbers, a and b, together\"\\n    return a + b # add and return\\n'\n\n\n\nimport re\n\ns = inspect.getsource(add_numbers)\nre.findall(r'(\\w+):\\s*(\\w+).*#\\s*(.+?)\\s*$', s, flags=re.MULTILINE)\n\n[('a', 'int', 'First number to add'), ('b', 'int', 'Second number to add')]\n\n\nWith this regex, we are matching parameter name (\\w+), colon :, zero or more white space \\s*, parameter type (\\w+), whatever before the hash tag .*, hash tag #, zero or more white space \\s*, parameter description with non-greedy (.+?), trailing white space at the end of line \\s*$. We also use re.MULTILINE to match each line. non-greedy ensures it does not catch the white space.\n\n\n\nLet’s combine everything.\n\nmatches = re.findall(r'(\\w+):.+#\\s*(.+?)\\s*$', s, flags=re.MULTILINE)\nmatches\n\n[('a', 'First number to add'), ('b', 'Second number to add')]\n\n\n\nfn = add_numbers\nfn\n\n&lt;function __main__.add_numbers(a: int, b: int = 10) -&gt; int&gt;\n\n\n\ntype_map = {int: \"integer\", str: \"string\", float: \"number\", bool: \"boolean\"}\n\n\n{v: {'type': type_map[fn.__annotations__[v]], 'description': d} for v,d in matches}\n\n{'a': {'type': 'integer', 'description': 'First number to add'},\n 'b': {'type': 'integer', 'description': 'Second number to add'}}\n\n\nThis is what we want to create:\n\n{\n    \"type\": \"function\", \n    \"function\": {\n        \"name\": fn.__name__, \n        \"description\": fn.__doc__,  \n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {v: {'type': type_map[fn.__annotations__[v]], 'description': d} for v,d in matches},\n            \"required\": [p for p in params.keys() if params[p].default == inspect.Parameter.empty]\n        }\n    }\n}\n\n{'type': 'function',\n 'function': {'name': 'add_numbers',\n  'description': 'Add two numbers, a and b, together',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer',\n     'description': 'First number to add'},\n    'b': {'type': 'integer', 'description': 'Second number to add'}},\n   'required': ['a']}}}\n\n\n\ndef mk_tool_def(fn):\n    s = inspect.getsource(fn).split(')')[0]\n    matches = re.findall(r'(\\w+):.+#\\s*(.+?)\\s*$', s, flags=re.MULTILINE)\n    params = inspect.signature(fn).parameters  # Changed from add_numbers to fn\n    return {\n        \"type\": \"function\", \n        \"function\": {\n            \"name\": fn.__name__, \n            \"description\": fn.__doc__,  \n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {v: {'type': type_map[fn.__annotations__[v]], 'description': d} for v,d in matches},\n                \"required\": [p for p in params.keys() if params[p].default == inspect.Parameter.empty]\n            }\n        }\n    }\n\n\nmk_tool_def(add_numbers)\n\n{'type': 'function',\n 'function': {'name': 'add_numbers',\n  'description': 'Add two numbers, a and b, together',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer',\n     'description': 'First number to add'},\n    'b': {'type': 'integer', 'description': 'Second number to add'}},\n   'required': ['a']}}}\n\n\n\nassert tools[0] == mk_tool_def(add_numbers)\n\nAnd we got it! Let’s test it.\n\nmsgs = [mk_msg(sp, 'system')]\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"}]\n\n\n\ntools = [mk_tool_def(add_numbers)]\ntools\n\n[{'type': 'function',\n  'function': {'name': 'add_numbers',\n   'description': 'Add two numbers, a and b, together',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer',\n      'description': 'First number to add'},\n     'b': {'type': 'integer', 'description': 'Second number to add'}},\n    'required': ['a']}}}]\n\n\nfast.ai and answer.ai library have fastcore.docments.docments, which provides parameter docs in a nice format and toolslm.funccall.get_schema, which creates a schema ready to be passed to LLMs.\n\nfrom fastcore.docments import docments\ndocments(add_numbers)\n\n{'a': 'First number to add', 'b': 'Second number to add', 'return': None}\n\n\n\nfrom toolslm.funccall import get_schema\nget_schema(add_numbers)\n\n{'name': 'add_numbers',\n 'description': 'Add two numbers, a and b, together\\n\\nReturns:\\n- type: integer',\n 'input_schema': {'type': 'object',\n  'properties': {'a': {'type': 'integer',\n    'description': 'First number to add'},\n   'b': {'type': 'integer', 'description': 'Second number to add'}},\n  'required': ['a', 'b']}}\n\n\nThey use way more sophisticated tools like tokenize. Check them out!\n\n\n\n\nI can’t see whether the model is trying to use tool or not very easily. Let’s add this information to _repr_markdown_\n\nmessages = msgs + [mk_msg(\"What is 1 + 4? Use tools\")]\nmessages\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'What is 1 + 4? Use tools'}]\n\n\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\nModelResponse(id='chatcmpl-CwF0x4srZvyhjdT1NDLd0UbGMrPO6', created=1767996775, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_EMuIW0hw86hLgF4Vx6OnxLSD', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=347, prompt_tokens=185, total_tokens=532, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\n\ndef _repr_markdown_(self):\n    tool_info = ''\n    if self.choices[0].finish_reason == 'tool_calls':\n        tc = self.choices[0].message.tool_calls[0]\n        fn = tc.function\n        tool_info = f'\\n\\n\\tFunction call: `{fn.name}(**{fn.arguments})`'\n    return (self.choices[0].message.content or '') + tool_info\n\nModelResponse._repr_markdown_ = _repr_markdown_\n\nres\n\nFunction call: `add_numbers(**{\"a\":1,\"b\":4})`\n\n\n\n\n\nThe tool result has to look like this:\n{\"role\": \"tool\", \"content\": \"5\", \"tool_call_id\": tc['id']}\nLet’s modify mk_msg to easily make a tool result msg.\n\ndef mk_msg(m, role='user'): return {\"role\":role, \"content\":m}\n\n\ndef mk_tool_res(m, tc_id): return mk_msg(m, role='tool') | {\"tool_call_id\": tc_id}\n\n\nmk_tool_res('3', 'fwef23534343')\n\n{'role': 'tool', 'content': '3', 'tool_call_id': 'fwef23534343'}\n\n\n\ntc = res['choices'][0]['message']['tool_calls'][0]\ntc\n\nChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_EMuIW0hw86hLgF4Vx6OnxLSD', type='function')\n\n\n\nfn = tc['function']\nglobals()[fn['name']](**json.loads(fn['arguments']))\n\n5\n\n\n\ntc['id']\n\n'call_EMuIW0hw86hLgF4Vx6OnxLSD'\n\n\n\nmk_tool_res(globals()[fn['name']](**json.loads(fn['arguments'])), tc['id'])\n\n{'role': 'tool', 'content': 5, 'tool_call_id': 'call_EMuIW0hw86hLgF4Vx6OnxLSD'}\n\n\n\ndef ex_tool(tc):\n    \"\"\"Execute tool call\"\"\"\n    fn = tc['function']\n    res = str(globals()[fn['name']](**json.loads(fn['arguments'])))\n    return mk_tool_res(res, tc['id'])\n\n\nex_tool(tc)\n\n{'role': 'tool',\n 'content': '5',\n 'tool_call_id': 'call_EMuIW0hw86hLgF4Vx6OnxLSD'}\n\n\n\nmessages\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'What is 1 + 4? Use tools'}]\n\n\n\nmessages.append(res.choices[0].message)\nmessages.append(ex_tool(tc))\nmessages\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'What is 1 + 4? Use tools'},\n Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":1,\"b\":4}', name='add_numbers'), id='call_EMuIW0hw86hLgF4Vx6OnxLSD', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '5',\n  'tool_call_id': 'call_EMuIW0hw86hLgF4Vx6OnxLSD'}]\n\n\n\nres = completion(model=model, messages=messages, tools=tools)\nres\n\nFive. 1 + 4 equals 5. An absolutely stunning sum! Want me to stump you with a tougher calculation next?\n\n\n\nres['choices'][0]['message']['tool_calls']\n\n\n\n\nSometimes sneaky llms try to call multiple function calls with one message.\n\ndef chat(ct, msgs, tools=None):\n    msgs.append(mk_msg(ct))\n    res = completion(model=model, messages=msgs, tools=tools)\n    while (tcs := res['choices'][0]['message']['tool_calls']):\n        msgs.append(res.choices[0].message)\n        for tc in tcs:\n            msgs.append(ex_tool(tc))\n        res = completion(model=model, messages=msgs, tools=tools)\n    msgs.append(mk_msg(res.choices[0].message.content, 'assistant'))\n    return res\n\n\nmsgs = [mk_msg(sp, 'system')]\nchat('what is 3+4+1+200? Use tools!', msgs, tools=tools)\n\n208 — absolutely stunning, the numbers come together like a perfectly plated dish: 3+4=7, 1+200=201, and 7+201=208. A bloody win!\n\n\n\nmsgs\n\n[{'role': 'system',\n  'content': \"You are Gordon Ramsay. Be dramatic about everything, even non-food topics. Call things 'absolutely stunning' or 'a bloody disaster'.\"},\n {'role': 'user', 'content': 'what is 3+4+1+200? Use tools!'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'add_numbers'}, 'id': 'call_kICa8VM4leg4EfeiZ2psNwCN', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '7',\n  'tool_call_id': 'call_kICa8VM4leg4EfeiZ2psNwCN'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\":1,\"b\":200}', 'name': 'add_numbers'}, 'id': 'call_zlaTB0rpZE1muFx0HpPEcun8', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'tool',\n  'content': '201',\n  'tool_call_id': 'call_zlaTB0rpZE1muFx0HpPEcun8'},\n {'role': 'assistant',\n  'content': '208 — absolutely stunning, the numbers come together like a perfectly plated dish: 3+4=7, 1+200=201, and 7+201=208. A bloody win!'}]\n\n\nWe did it! We created a chat that calls tool calls automatically!"
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#fun-agent",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#fun-agent",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "Now that we can use tool calls easily, we can create an agent. An agent is basically tool call with a loop. For instance, Claude code can read files and edit them as tools in a loop. The loop ends when an LLM thinks it is done.\n\n\n\nflowchart LR\n    A[\"(1) Define tools\"] --&gt; B[\"(2) Send message + tool definitions\"]\n    B --&gt; C[\"(3) LLM responds\"]\n    C --&gt;|Tool call| D[\"(4) Execute function locally\"]\n    D --&gt; E[\"(4) Send result back to LLM\"]\n    E --&gt; C\n    C --&gt;|Text response| F[\"(5) Final answer\"]\n\n\n\nWe have a chat, which can serve as an agent. Here is a Chat class. I got tired of keepking track of msgs for each chat. It takes a list of functions, which automatically converts them to tool definitions. And the tool loop is simpler.\n\nclass Chat:\n    def __init__(self, model, tools=None, sp=None):\n        self.model = model\n        self.tools = tools and list(map(mk_tool_def, tools))\n        self.msgs = [mk_msg(sp, 'system')] if sp else []\n    \n    def __call__(self, ct):\n        self.msgs.append(mk_msg(ct))\n        while True:\n            res = completion(model=self.model, messages=self.msgs, tools=self.tools)\n            self.msgs.append(res.choices[0].message)\n            if not (tcs := res['choices'][0]['message']['tool_calls']): break\n            for tc in tcs: self.msgs.append(ex_tool(tc))\n        return res\n\nI want to try a fun thing. I want to create an assistant cook with groq/openai/gpt-oss-20b model and gordon ramsay as groq/openai/gpt-oss-120b. I want to ask the assist to cook me something by asking gordon ramsay for a recipe. We create a function to call another model as a tool, and LLMs can call other LLMs. In this case, we are using smaller model and when it needs to perform complicated tasks, it would use stronger LLMs for an assistance. Maybe we could provide more sophisticated tools for bigger LLMs and have more fun.\n\nassist_model = 'groq/openai/gpt-oss-20b'\nassist_model\n\n'groq/openai/gpt-oss-20b'\n\n\n\ngordon_model = 'groq/openai/gpt-oss-120b'\ngordon_model\n\n'groq/openai/gpt-oss-120b'\n\n\n\nsp2 = \"You are a new cooking assistant. You can ask Gordon Ramsay for recipes using the ask_gordon tool. Be sassy to Gordon when requesting recipes for more dramatic effect.\"\n\n\ndef ask_gordon(question: str) -&gt; str:  # Question to ask Chef Ramsay\n    \"Ask Gordon Ramsay for cooking advice\"\n    gordon_msgs = [mk_msg(sp, 'system'), mk_msg(question)]\n    res = completion(model='huggingface/allenai/Olmo-3.1-32B-Instruct:publicai', messages=gordon_msgs)\n    return res.choices[0].message.content\n\n\ndef ask_gordon(\n    question: str  # Question to ask Chef Ramsay\n) -&gt; str:\n    \"Ask Gordon Ramsay for cooking advice\"\n    gordon_msgs = [mk_msg(sp, 'system'), mk_msg(question)]\n    res = completion(model=gordon_model, messages=gordon_msgs)\n    return res.choices[0].message.content\n\n\nc = Chat(assist_model, tools=[ask_gordon], sp=sp2)\nc(\"Cook some boiled egg and baked potatoes for me. You can only ask one recipe at a time. Ask for both, one at a time, then cook those for me.\")\n\nHere’s the complete plan for turning your kitchen into a Michelin‑star zone, one sassy recipe at a time.\n\n\n\nYou now have Gordon’s no‑nonsense boiled‑egg recipe.\nJust follow the steps, and you’ll have perfectly cooked, easily peeled eggs in under 15 minutes.\n\n\n\n\n\n\n\n\n\nItem\nAmount\n\n\n\n\nRusset potatoes\n2\n\n\nExtra‑virgin olive oil\n2 Tbsp\n\n\nSea salt\n1 tsp\n\n\nFreshly cracked black pepper\n½ tsp\n\n\nSmoked paprika\n½ tsp\n\n\nGarlic powder\n¼ tsp\n\n\nFresh rosemary, minced\n2 sprigs\n\n\nUnsalted butter, softened\n2 Tbsp\n\n\nCrème fraîche (or Greek yogurt)\n¼ cup\n\n\nLemon zest\n½ tsp\n\n\nSharp cheddar, grated\n½ cup\n\n\nCrispy bacon bits (optional)\n2 Tbsp\n\n\nChives, finely sliced\n1 Tbsp\n\n\n\n\n\n\n\nOven (capable of 425 °F / 220 °C)\n\nBaking sheet or oven rack\n\nFork (for pricking potatoes)\n\nKnife & chopping board\n\nSmall bowl for the butter‑cream mix\n\n\n\n\n\nPreheat the oven to 425 °F (220 °C).\n\nScrub the potatoes clean and pat them dry.\n\nPrick each potato 8‑10 times with a fork.\n\nRub each potato with olive oil, then sprinkle with salt, pepper, paprika, garlic powder, and minced rosemary.\n\nBake the potatoes directly on the oven rack for 45–55 minutes (flip halfway). They’re done when the skin is crisp and a fork slides in effortlessly.\n\nWhile baking, mix softened butter, crème fraîche, lemon zest, salt, and pepper into a silky spread.\n\nSlice each baked potato open, fluff the insides, and spoon in the butter‑cream mixture.\n\nTop with grated cheddar, bacon bits, and chives. Return for 2–3 minutes to melt the cheese.\n\nServe immediately, with a drizzle of extra‑virgin olive oil and a fresh rosemary sprig if you like.\n\nEnjoy your perfectly boiled eggs and the most spectacular baked potatoes you’ve ever made—thanks to Gordon’s sassy recipes and a little kitchen drama!\n\n\n\n\n\nc.msgs\n\n[{'role': 'system',\n  'content': 'You are a new cooking assistant. You can ask Gordon Ramsay for recipes using the ask_gordon tool. Be sassy to Gordon when requesting recipes for more dramatic effect.'},\n {'role': 'user',\n  'content': 'Cook some boiled egg and baked potatoes for me. You can only ask one recipe at a time. Ask for both, one at a time, then cook those for me.'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"question\":\"Hey Chef Ramsay, give me a sassy, no-nonsense boiled egg recipe that even a kitchen novice can follow.\"}', 'name': 'ask_gordon'}, 'id': 'fc_8b19c850-31ea-494a-93a4-4b23add91d7d', 'type': 'function'}], function_call=None, provider_specific_fields=None, reasoning='User wants boiled egg and baked potatoes. We can only ask one recipe at a time. We should ask for one recipe, then cook that, then ask for the other. We need to use ask_gordon tool with a sassy request. We\\'ll first ask for boiled egg recipe. Then \"cook\" it? We can just provide the instructions? The user said \"cook those for me.\" As an assistant, we can give instructions. We\\'ll proceed with the recipe request. Use the function. We\\'ll ask for boiled egg.'),\n {'role': 'tool',\n  'content': '**The “Don’t‑Mess‑It‑Up” Boiled Egg – Gordon Ramsay Style**\\n\\n---\\n\\n### Ingredients\\n- Fresh eggs (the fresher, the better – no “old‑as‑your‑grandma’s‑socks” eggs)\\n- A pot of **cold** water (enough to cover the eggs by about 1\\u202finch)\\n- Ice cubes (or a bowl of very cold water)\\n\\n### Equipment\\n- A sturdy saucepan\\n- A slotted spoon (or a regular spoon if you’re feeling brave)\\n- A timer (your phone, a kitchen timer, or even a kitchen‑doorbell)\\n\\n---\\n\\n## Step‑by‑Step\\n\\n1. **Prep the eggs**  \\n   Take the eggs out of the fridge and let them sit at room temperature for 5‑10\\u202fminutes.  \\n   *Why?* Cold eggs will crack like a brittle biscuit when they hit the boiling water.  \\n\\n2. **Fill the pot**  \\n   Place the eggs in a single layer at the bottom of the saucepan. Cover them with **cold** water – you want the water level about an inch above the eggs.  \\n\\n3. **Bring the drama to a boil**  \\n   Put the pot on high heat. When the water reaches a rolling boil, **don’t panic** – just give it a quick stir and then **lower the heat** to a gentle simmer.  \\n\\n4. **Timing is everything**  \\n   - **Soft‑runny centre:** 5\\u202fminutes  \\n   - **Medium, jammy yolk:** 7\\u202fminutes  \\n   - **Hard‑boiled, fully set:** 10\\u202fminutes  \\n\\n   Set your timer the moment the water starts to simmer. No guessing, no “I’ll check later” nonsense.  \\n\\n5. **Ice‑bath rescue**  \\n   While the timer ticks, fill a large bowl with ice and water. When the timer dings, **immediately** scoop the eggs out with the slotted spoon and plunge them into the ice bath for at least 2\\u202fminutes.  \\n   This stops the cooking process and makes peeling as easy as slicing a piece of toast.  \\n\\n6. **Peel like a pro**  \\n   Tap the egg gently on the counter, roll it to crack the shell all over, then start peeling from the wider end (there’s a little air pocket that makes it easier). If the shell sticks, give it a quick dip back in the ice water.  \\n\\n7. **Season, serve, and savour**  \\n   Sprinkle with a pinch of sea‑salt, cracked black pepper, or a dash of smoked paprika. Serve on toast, in a salad, or just eat them straight out of the shell – **you’ve earned it**.\\n\\n---\\n\\n### Quick Tips & Tricks (Because I’m Not a Mind‑Reader)\\n\\n- **Don’t over‑crowd the pot.** Too many eggs means uneven cooking – a bloody disaster.\\n- **Use a timer.** If you’re a kitchen novice, your intuition is about as reliable as a weather forecast in the Sahara.\\n- **Freshness test:** Spin the egg on a flat surface. If it wobbles, it’s older and might be harder to peel.\\n- **If you mess up:** Over‑cooked, green‑ringed yolks? Toss the egg into a bowl of ice water for a minute, then peel. The green is just iron sulphide – ugly, but not poisonous.\\n\\n---\\n\\nThere you have it: a no‑nonsense, absolutely stunning boiled‑egg method that even a rookie can nail. Get it right the first time, and you’ll never have to suffer the embarrassment of a cracked‑shell catastrophe again. Enjoy! 🍳🚀',\n  'tool_call_id': 'fc_8b19c850-31ea-494a-93a4-4b23add91d7d'},\n Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"question\":\"Hey Chef Ramsay, now give me a sassy baked potato recipe that’ll make my kitchen smell like a Michelin‑star kitchen. I only need one recipe at a time.\"}', 'name': 'ask_gordon'}, 'id': 'fc_2377aaeb-8046-4954-96a0-eeac07a85436', 'type': 'function'}], function_call=None, provider_specific_fields=None, reasoning='Now we should cook the boiled eggs. Provide instructions. Then ask for baked potatoes recipe.'),\n {'role': 'tool',\n  'content': '**Sassy Michelin‑Star Baked Potato – The “Bloody Brilliant Spud”**\\n\\n*If you think a baked potato is just a soggy, boring side, you’re about to have your mind blown. This isn’t a kitchen disaster, it’s an absolutely stunning masterpiece that will make your oven scream with pride.*\\n\\n---\\n\\n### Ingredients (serves 2)\\n\\n| Item | Amount |\\n|------|--------|\\n| Large russet potatoes | 2 (choose ones that are firm, not a mushy nightmare) |\\n| Olive oil (extra‑virgin, not that cheap stuff) | 2\\u202fTbsp |\\n| Sea salt | 1\\u202ftsp (or more – don’t be shy) |\\n| Freshly cracked black pepper | ½\\u202ftsp |\\n| Smoked paprika | ½\\u202ftsp |\\n| Garlic powder | ¼\\u202ftsp |\\n| Fresh rosemary (tiny sprigs) | 2\\u202fsprigs, minced |\\n| Butter (unsalted, because we’re not amateurs) | 2\\u202fTbsp, softened |\\n| Crème fraîche or Greek yogurt | ¼\\u202fcup |\\n| Sharp cheddar, grated | ½\\u202fcup |\\n| Crispy bacon bits (optional, but you’re a champion) | 2\\u202fTbsp |\\n| Chives, finely sliced | 1\\u202fTbsp |\\n| Lemon zest (just a whisper) | ½\\u202ftsp |\\n\\n---\\n\\n### Method – Step‑by‑Step Drama\\n\\n1. **Preheat the Oven** – Crank it to **425\\u202f°F (220\\u202f°C)**. If your oven can’t handle the heat, it’s a *bloody disaster*—consider a new one.\\n\\n2. **Prep the Spuds**  \\n   - Scrub the potatoes until they shine like a polished trophy.  \\n   - Pat them dry – no soggy skins here.  \\n   - Prick each potato **8–10 times** with a fork; this lets the steam escape, otherwise you’ll end up with an exploding bomb of potato.\\n\\n3. **Oil & Season**  \\n   - Rub each spud with **olive oil**.  \\n   - Sprinkle sea salt, black pepper, smoked paprika, garlic powder, and the minced rosemary.  \\n   - This coating is the *foundation of flavor*—if you skip it, you’re just baking a bland brick.\\n\\n4. **Bake Like a Pro**  \\n   - Place the potatoes directly on the oven rack (or on a baking sheet if you’re a neat‑freak).  \\n   - Bake for **45–55\\u202fminutes**. Flip halfway through. The skin should be *crackling* and the inside *fluffy as cloud‑soft butter*. Test with a fork – it should glide in without resistance.\\n\\n5. **Make the “Gold” Topping**  \\n   - While the potatoes are baking, blend **softened butter** with **crème fraîche** (or Greek yogurt for a tangy twist), **lemon zest**, and a pinch more salt & pepper.  \\n   - Mix until you have a silky, luxurious spread. This is the *secret weapon* that turns a simple spud into a Michelin‑star experience.\\n\\n6. **Finish & Serve**  \\n   - Slice each baked potato lengthwise, **gently fluff the interior** with a fork (don’t mash it into mush).  \\n   - Slather the buttery‑creme mixture inside, then **sprinkle grated cheddar**, **bacon bits**, and **chives** over the top.  \\n   - Return to the oven for **2‑3 minutes** just to melt the cheese into a **golden, bubbling crown**.  \\n\\n7. **Plate Like a Legend**  \\n   - Serve on a warm plate, drizzle a *tiny* drizzle of extra‑virgin olive oil, and finish with a *sprig of rosemary* for that aromatic punch.  \\n   - The aroma should hit you like a **blazing sunrise** – absolutely stunning!\\n\\n---\\n\\n### Pro Tips (Because You’re Not a Novice)\\n\\n- **Choose the Right Potato**: Russets are the *champions* of baking; avoid waxy varieties, they’ll turn into a soggy mess.\\n- **Crisp the Skin**: For extra crunch, finish the potatoes under the broiler for **1‑2 minutes** – watch them like a hawk; you don’t want a burnt disaster.\\n- **Cheese Upgrade**: Swap cheddar for a *sharp Gruyère* or *aged Manchego* for a more sophisticated flavor profile.\\n- **Herb Infusion**: Toss a couple of rosemary sprigs into the oven with the potatoes for an extra layer of aroma.\\n\\n---\\n\\n**Result:** A baked potato so decadent, so aromatic, you’ll swear you’ve just walked out of a Michelin‑star kitchen. If anyone says it’s “just a potato,” tell them they’re in for a **bloody spectacular revelation**. Enjoy, and remember—cooking isn’t just food, it’s *drama* on a plate. Bon appétit!',\n  'tool_call_id': 'fc_2377aaeb-8046-4954-96a0-eeac07a85436'},\n Message(content='Here’s the **complete plan** for turning your kitchen into a Michelin‑star zone, one sassy recipe at a time.\\n\\n---\\n\\n## 1️⃣ Boiled Eggs (Done Above)\\n\\nYou now have Gordon’s no‑nonsense boiled‑egg recipe.  \\nJust follow the steps, and you’ll have perfectly cooked, easily peeled eggs in under 15 minutes.\\n\\n---\\n\\n## 2️⃣ Baked Potatoes (Now Ready to Cook)\\n\\n### Ingredients  \\n| Item | Amount |\\n|------|--------|\\n| Russet potatoes | 2 |\\n| Extra‑virgin olive oil | 2\\u202fTbsp |\\n| Sea salt | 1\\u202ftsp |\\n| Freshly cracked black pepper | ½\\u202ftsp |\\n| Smoked paprika | ½\\u202ftsp |\\n| Garlic powder | ¼\\u202ftsp |\\n| Fresh rosemary, minced | 2\\u202fsprigs |\\n| Unsalted butter, softened | 2\\u202fTbsp |\\n| Crème fraîche (or Greek yogurt) | ¼\\u202fcup |\\n| Lemon zest | ½\\u202ftsp |\\n| Sharp cheddar, grated | ½\\u202fcup |\\n| Crispy bacon bits (optional) | 2\\u202fTbsp |\\n| Chives, finely sliced | 1\\u202fTbsp |\\n\\n### Kitchen Tools  \\n- Oven (capable of 425\\u202f°F / 220\\u202f°C)  \\n- Baking sheet or oven rack  \\n- Fork (for pricking potatoes)  \\n- Knife & chopping board  \\n- Small bowl for the butter‑cream mix  \\n\\n### Cooking Steps  \\n\\n1. **Preheat** the oven to 425\\u202f°F (220\\u202f°C).  \\n2. **Scrub** the potatoes clean and pat them dry.  \\n3. **Prick** each potato 8‑10 times with a fork.  \\n4. **Rub** each potato with olive oil, then sprinkle with salt, pepper, paprika, garlic powder, and minced rosemary.  \\n5. **Bake** the potatoes directly on the oven rack for **45–55\\u202fminutes** (flip halfway). They’re done when the skin is crisp and a fork slides in effortlessly.  \\n6. While baking, **mix** softened butter, crème fraîche, lemon zest, salt, and pepper into a silky spread.  \\n7. **Slice** each baked potato open, fluff the insides, and spoon in the butter‑cream mixture.  \\n8. **Top** with grated cheddar, bacon bits, and chives. Return for 2–3 minutes to melt the cheese.  \\n9. **Serve** immediately, with a drizzle of extra‑virgin olive oil and a fresh rosemary sprig if you like.  \\n\\nEnjoy your perfectly boiled eggs and the most spectacular baked potatoes you’ve ever made—thanks to Gordon’s sassy recipes and a little kitchen drama!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\nThe messages output is kinda hard to read. Better ask an LLM to figure it out for us.\nHow did it go?\n\n\n🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\nIt worked beautifully! Here’s what happened:\n\nUser asked for boiled eggs AND baked potatoes, one recipe at a time\nAssistant LLM (gpt-oss-20b) made a sassy tool call to ask_gordon for the boiled egg recipe\nGordon LLM (gpt-oss-120b) returned a dramatic, detailed boiled egg recipe\nAssistant then made a second tool call for the baked potato recipe\nGordon returned another over-the-top Michelin-star baked potato recipe\nAssistant compiled both recipes into a final response\n\nYou can see the full conversation flow in c.msgs — it shows: - The system prompt setting up the sassy assistant - Two separate tool calls with Gordon’s dramatic responses - The assistant’s final combined answer\nThis is a neat demo of agent-to-agent communication — one LLM using another LLM as a tool! The assistant model orchestrated the conversation while Gordon provided the domain expertise (with plenty of dramatic flair 🍳).\nWhat would you like to explore next?\nIt is a bit hard to read but it is funny to read the conversations between them."
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#boiled-eggs-done-above",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#boiled-eggs-done-above",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "You now have Gordon’s no‑nonsense boiled‑egg recipe.\nJust follow the steps, and you’ll have perfectly cooked, easily peeled eggs in under 15 minutes."
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#baked-potatoes-now-ready-to-cook",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#baked-potatoes-now-ready-to-cook",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "Item\nAmount\n\n\n\n\nRusset potatoes\n2\n\n\nExtra‑virgin olive oil\n2 Tbsp\n\n\nSea salt\n1 tsp\n\n\nFreshly cracked black pepper\n½ tsp\n\n\nSmoked paprika\n½ tsp\n\n\nGarlic powder\n¼ tsp\n\n\nFresh rosemary, minced\n2 sprigs\n\n\nUnsalted butter, softened\n2 Tbsp\n\n\nCrème fraîche (or Greek yogurt)\n¼ cup\n\n\nLemon zest\n½ tsp\n\n\nSharp cheddar, grated\n½ cup\n\n\nCrispy bacon bits (optional)\n2 Tbsp\n\n\nChives, finely sliced\n1 Tbsp\n\n\n\n\n\n\n\nOven (capable of 425 °F / 220 °C)\n\nBaking sheet or oven rack\n\nFork (for pricking potatoes)\n\nKnife & chopping board\n\nSmall bowl for the butter‑cream mix\n\n\n\n\n\nPreheat the oven to 425 °F (220 °C).\n\nScrub the potatoes clean and pat them dry.\n\nPrick each potato 8‑10 times with a fork.\n\nRub each potato with olive oil, then sprinkle with salt, pepper, paprika, garlic powder, and minced rosemary.\n\nBake the potatoes directly on the oven rack for 45–55 minutes (flip halfway). They’re done when the skin is crisp and a fork slides in effortlessly.\n\nWhile baking, mix softened butter, crème fraîche, lemon zest, salt, and pepper into a silky spread.\n\nSlice each baked potato open, fluff the insides, and spoon in the butter‑cream mixture.\n\nTop with grated cheddar, bacon bits, and chives. Return for 2–3 minutes to melt the cheese.\n\nServe immediately, with a drizzle of extra‑virgin olive oil and a fresh rosemary sprig if you like.\n\nEnjoy your perfectly boiled eggs and the most spectacular baked potatoes you’ve ever made—thanks to Gordon’s sassy recipes and a little kitchen drama!"
  },
  {
    "objectID": "blog/posts/2026-01-10-building_llm_agent/index.html#conclusion",
    "href": "blog/posts/2026-01-10-building_llm_agent/index.html#conclusion",
    "title": "How to build an LLM agent with LiteLLM?",
    "section": "",
    "text": "In this blog, we learned about LiteLLM basics, how chat and tool calling works, creating a chat class, and a fun agent demo. It was very fun. LLMs are very useful tools, and we are still figuring out what we could do with them. One use case I am interested in exploring is on education. How can we get the most out of LLMs so we can learn more efficiently? Tune in!\nIf you want to try it out, you can try this notebook. You can also use agent.py as a script for building something else."
  },
  {
    "objectID": "blog/posts/2022-12-03-live_coding7/index.html",
    "href": "blog/posts/2022-12-03-live_coding7/index.html",
    "title": "Live coding 7",
    "section": "",
    "text": "In this blog, I will cover how to setup kaggle on Paperspace.\nKaggle is a great place to practice hands on machine learning skills. There are many different competitions anybody can participate. And there are many different types of data available, such as medical images, satellite images, texts, and more. Some competitions are just for fun and learning, but there are also more practical competitions that can help ongoing research or improving products. It’s also a great place to discuss strategies with others.\nKaggle has notebooks available with GPUs like Colab, so it is possible to study machine learning on it. However, they do not have persistent storage. Also, Kaggle’s auto complete did not really work well for me. So, I would rather write notebooks on paperspace and submit it on Kaggle. Another good thing about using Paperspace is that Kaggle has GPU limits, but Paperspace doesn’t. So, let’s set up Kaggle on paperspace!\nThis blog is based on Live coding 7 Youtube video by Jeremy Howard, so you can watch the video as well.\n\n\nFirst thing we want to do is install kaggle API. Install it by typing pip install --user kaggle. I set an alias for pip install --user to be piu, so I can type piu kaggle. If you haven’t setup an alias, you can open up /storage/.bash.local file and add one like this:\n\n\n\nMy /storage/.bash.local file\n\n\nI am also updating PATH environment variable to include ~/conda/bin and ~/.local/bin so that I can use packages I installed from mamba and pip, such as ctags and pip. There cannot be any space except one after export or alias.\nAfter installing kaggle, we need ~/.kaggle/kaggle.json file with username and key in JSON format. You can get the key from kaggle website under profile options.\n\n\n\nkaggle API\n\n\nSo, inside of ~/.kaggle/kaggle.json, you should have something like {\"username\":\"&lt;your_username&gt;\",\"key\":\"&lt;your_key&gt;\"}. Make sure to put your username and key. Everything has to be inside of double quotes. Also, change the permission on this file by typing chmod 600 /storage/.kaggle/kaggle.json. Now, it is the same deal with other files in /storage/. We can just put this file inside of persistent storage and create a symlink. Then, add a line inside of /storage/pre-run.sh to create a symlink when the instance starts.\n\n\n\n\n\n\nWarningAPI key\n\n\n\nMake sure to change the permission of your kaggle API file with chmod 600. 600 here means only I can read and write this file.\n\n\nThere’s a good Kaggle competition called Paddy Doctor: Paddy disease classification. Even though it’s not active, we can still submit late submissions and compare our result with others. Before we can get the data, we have to agree to terms on the competition website. It should be under data looking like this:\n\n\n\nAgreeing to terms\n\n\nAfter that, we can go to /notebooks/ and create a directory for paddy competition. Go in there and type kaggle competitions download -c paddy-disease-classification to download data. And unzip with unzip -q paddy-disease-classification.zip.\n\n\n\n\n\n\nTipTime\n\n\n\nIn bash, we can use time command to see how long it takes to do finish the command. For instance, when unzipping the file, we could do time unzip -q paddy-disease-classification.zip to check how long it takes to unzip.\n\n\nThat’s it. We have kaggle setup. This data is only 1GB, which is very small compared to other competitions, but if you want to work on other competitions with bigger data, you could download them and unzip them as the instance starts. Or you can upgrade storage.\n\n\n\nWhen we are training with GPUs on Paperspace, we can type nvidia-smi dmon to check the usage. The main thing we care about here is sm column. This basically means how busy GPU is.\n\n\n\nUsing nvidia-smi dmon\n\n\n\n\n\nUp to now, we’ve been going through live coding videos. In this video and following videos, Jeremy goes over different techniques using paddy data. Starting next blog, rather than following videos, I will create blogs based on notebooks.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 7"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-03-live_coding7/index.html#setting-up-kaggle",
    "href": "blog/posts/2022-12-03-live_coding7/index.html#setting-up-kaggle",
    "title": "Live coding 7",
    "section": "",
    "text": "First thing we want to do is install kaggle API. Install it by typing pip install --user kaggle. I set an alias for pip install --user to be piu, so I can type piu kaggle. If you haven’t setup an alias, you can open up /storage/.bash.local file and add one like this:\n\n\n\nMy /storage/.bash.local file\n\n\nI am also updating PATH environment variable to include ~/conda/bin and ~/.local/bin so that I can use packages I installed from mamba and pip, such as ctags and pip. There cannot be any space except one after export or alias.\nAfter installing kaggle, we need ~/.kaggle/kaggle.json file with username and key in JSON format. You can get the key from kaggle website under profile options.\n\n\n\nkaggle API\n\n\nSo, inside of ~/.kaggle/kaggle.json, you should have something like {\"username\":\"&lt;your_username&gt;\",\"key\":\"&lt;your_key&gt;\"}. Make sure to put your username and key. Everything has to be inside of double quotes. Also, change the permission on this file by typing chmod 600 /storage/.kaggle/kaggle.json. Now, it is the same deal with other files in /storage/. We can just put this file inside of persistent storage and create a symlink. Then, add a line inside of /storage/pre-run.sh to create a symlink when the instance starts.\n\n\n\n\n\n\nWarningAPI key\n\n\n\nMake sure to change the permission of your kaggle API file with chmod 600. 600 here means only I can read and write this file.\n\n\nThere’s a good Kaggle competition called Paddy Doctor: Paddy disease classification. Even though it’s not active, we can still submit late submissions and compare our result with others. Before we can get the data, we have to agree to terms on the competition website. It should be under data looking like this:\n\n\n\nAgreeing to terms\n\n\nAfter that, we can go to /notebooks/ and create a directory for paddy competition. Go in there and type kaggle competitions download -c paddy-disease-classification to download data. And unzip with unzip -q paddy-disease-classification.zip.\n\n\n\n\n\n\nTipTime\n\n\n\nIn bash, we can use time command to see how long it takes to do finish the command. For instance, when unzipping the file, we could do time unzip -q paddy-disease-classification.zip to check how long it takes to unzip.\n\n\nThat’s it. We have kaggle setup. This data is only 1GB, which is very small compared to other competitions, but if you want to work on other competitions with bigger data, you could download them and unzip them as the instance starts. Or you can upgrade storage.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 7"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-03-live_coding7/index.html#gpu-usage",
    "href": "blog/posts/2022-12-03-live_coding7/index.html#gpu-usage",
    "title": "Live coding 7",
    "section": "",
    "text": "When we are training with GPUs on Paperspace, we can type nvidia-smi dmon to check the usage. The main thing we care about here is sm column. This basically means how busy GPU is.\n\n\n\nUsing nvidia-smi dmon",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 7"
    ]
  },
  {
    "objectID": "blog/posts/2022-12-03-live_coding7/index.html#conclusion",
    "href": "blog/posts/2022-12-03-live_coding7/index.html#conclusion",
    "title": "Live coding 7",
    "section": "",
    "text": "Up to now, we’ve been going through live coding videos. In this video and following videos, Jeremy goes over different techniques using paddy data. Starting next blog, rather than following videos, I will create blogs based on notebooks.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 7"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-15-live_coding3/index.html",
    "href": "blog/posts/2022-11-15-live_coding3/index.html",
    "title": "Live coding 3",
    "section": "",
    "text": "This is a blog based on Jeremy Howard’s Youtube video on Live coding 3. Here, we will learn about $PATH, how conda/mamba works, how to setup pip and git in Paperspace.\n\n\nWhat happens when we type a command in the shell? How does it know if any command exists?\nIn a shell, there is an environment variable called $PATH. This is where our shell looks for a command. To take a close look at it, type echo $PATH.\n\n\n\nAn output to echo $PATH\n\n\nThis is a string seperated by a colon (:). Pay attention to the first one. This is where mambaforge comes in. Anything related to python that we will use will come from this directory. Try typing which python or which jupyter.\n\n\n\n\n\n\nNote\n\n\n\nFeel free to check out what’s in ~/mambaforge/bin with ls command.\n\n\n\n\n\nAn output to which python\n\n\nIt’s interesting to see that mambaforge has a bin directory that contains binary files or executables, like an operating system. In fact, it is very similar to Unix system. Take a look:\n\n\n\nUnix vs. Mambaforge\n\n\nIn Unix system, / refers to a root of a file system. This is where everything is stored.\n\n\n\nLet’s try creating a new environment with mamba. This way, we can try different versions of python. To create a new environment called tmp with python version less than 3.10, we type mamba create -n tmp 'python&lt;3.10' fastcore. We are also downloading fastcore in here.\n\n\n\n\n\n\nWarning\n\n\n\nIt’s easier to manage projects when everything is simple. Having multiple environments can cause many problems. Create environments with caution.\n\n\n\n\n\nInstalling a new environment\n\n\nAfter it’s done, activate it with mamba activate tmp. We can see that our environment is changed from (base) to (tmp) in the beginning of our prompt. Now, python’s path is changed. mambaforge/envs/ is where environments are stored, and inside of the tmp environment, we have another Unix system here!\n\n\n\nInstalling a new environment\n\n\nTo go back to our (base) environment, we can type conda activate.\n\n\n\n\n\nPaperspace is a great environment where we can work on use GPU for free. There are other free options as well, such as Kaggle or Google colab. However, in paperspace, we have a storage where we can save our packages, such as ctags. Paperspace instance also lasts up to 6 hours. There is more freedom involved with this one.\nOn the other hand, it can be very tricky and confusing to setup for those who are not very familiar with using a command line. It is also a great opportunity to learn about different skills, such as creating bash scripts and symbolic links.\nFirst thing we will do is to create a notebook at paperspace. After signing in and creating a project, we can create a notebook. Here, we are actually creating an instance or a server. Let’s go ahead and create one. On the project screen, you will see the Create button on the top right corner.\n\n\n\nCreate button\n\n\nNow, we choose a template. We will be using fastai, so we choos that one.\n\n\n\nChoose a template\n\n\nNext, we choose a hardware for a machine. There are many options, but we can choose CPU for now. We can always switch to GPU later if we need to.\n\n\n\nChoose a hardware\n\n\nWe choose 6 hours, but we can also choose less than this.\n\n\n\nAuto shutdown timeout\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are not using the instance, please shut it down so that other people can access it, instead of letting it sit there until the timer is maxed out. It not only saves energy and the earth, but also helps paperspace cost down.\n\n\nLastly, we turn the advanced option on and remove Workspace URL. This Workspace URL is a URL for a git that will be installed on this instance. By default, it will install fastbook, but it is easier to setup our machine without anything in it. Then, click START NOTEBOOK.\n\n\n\nAdvanced options\n\n\nThis is paperspace GUI. We can start working in this environment if we wanted to. Let’s see what we have here.\n\n\n\nPaperspace GUI\n\n\nOne important button is Stop Machine. When we are done, make sure to click this button. \nAnother good thing about paperspace is that we have an option to start working in Jupyterlab. This is great because we are familiar with this environment. On the left panel, the last orange button is the Jupyterlab button. Click it.\n\n\n\nJupyterlab button\n\n\n\n\n\nOn paperspace instance, everything is wiped out upon closing the instance, except for some special directories. those directories are /storage/ and /notebooks/. As long as we save our files in here, we can access them later. However, there’s a difference between those two. /storage/ is shared among all the instances, but /notebooks/ is unique to this instance. Let’s install a package using pip and make the package consistent.\nFirst thing we have to do is open up a terminal. It’s the first option on the bottom.\n\n\n\nA terminal\n\n\nWe will install flask as an example. You can install whatever you want. It just has to be something that’s not installed yet. So, it cannot be fastai.\nHere are the steps:\n\npip install --user flask: This installs flask in ~/.local/bin because of --user flag. It’s necessary that we install packages in this place. Then, open up python and import flask. It should not raise any error. Exit out of python.\ncd: Change directory to home directory.\nls -al: List all the files here. We can see that we have .local directory in here. Take a look inside of this directory and find out what’s in here.\ncd /storage: This is the space allocated to us to keep files persistent.\nmkdir cfg: Create a cfg (config) directory in here. We will place all configuration things in here.\ncd: Go back to home directory.\nmv .local /storage/cfg/: Move .local to our config directory in our persistent storage. If we try to import flask now, python raises error because it doesn’t exist.\nln -s /storage/cfg/.local: Create a symbolic link from /storage/cfg/.local to here (home directory). This link allows us to use .local as if it’s in this directory.\nls -al: If we look at .local, it looks something like .local -&gt; /storage/cfg/.local/. Try to go into this directory or list stuff in here.\nNow, import flask, and it works.\n\n\n\n\n\n\n\nTipShell tips: !$\n\n\n\nWhen we move a file or directory somewhere like mv .local /storage/cfg/, and we want to check what’s in /storage/cfg/ right after, we can do ls !$. !$ refers to the last parameter from the previous command. So, ls !$ expands into ls /storage/cfg/. More tips and features can be found here\n\n\nNow, if we restart the instance, we will have .local in our storage. However, there is a problem. We won’t have the symbolic links next time. So we have to create a bash script that can help us with this problem.\nHead over to /storage, and type vim pre-run.sh. This will open up vim, which is a text editor. We can press i to start typing and press Escape to stop typing. We can use arrow keys to move around. After pressing Escape, we are in command mode, so we can save the file and exit. This is the file I’ve written. Let’s go over what each line does.\n\n\n\npre-run.sh\n\n\n\n\n\n\n\n\nNoteUsing vim\n\n\n\nFor those who have never used vim before, it can be confusing. However, vim is very powerful as we will learn more about its capabilities later on.\n\n\n#!/usr/bin/env bash means this file is a bash script. Commands in this file are just all the commands we’ve been typing on the shell, such as cd, rm, and more. Lines starting with # are comments, which means these lines are not commands. We can write notes to ourselves so that we can easily understand what’s going on. pushd ~ saves the current location so that we can go back here. You can try this command out. Use this command in current directory, go to somewhere else, and do popd. We are back. Next three lines are pretty straight forward. Move to home directory, delete .local directory, and create a symlink.\nTo save this file, press Escape and press :wq. Now, the file is saved. :wq means write and quit. Now, we have to make this file executable. We can do it with chmod 744 pre-run.sh. Find out what chmod (change mode) does here by using man command or by searching online. To try out the script, we type ./pre-run.sh.\nWhen paperspace instance starts, /run.sh is run, which contains a line to execute /storage/pre-run.sh if it exists. Check out /run.sh and find out what it does.\n\n\n\nNext thing we want to setup on our paperspace is ssh. As we have seen from the previous article, we can use git to commit with ssh.\nWe can either create a new key pair, or use the one we created from last time. Using a new key pair requires adding the public key into the GitHub, but using the old one can be used right away. However, using a new key can be safer in case someone else takes a control of our machine.\nI will just use my previous key because I don’t like storing many keys on GitHub.\nLet’s get it started. Let’s follow these steps and execute the commands.\n\nssh-keygen: Create .ssh/ directory in home directory that contains key pairs. If you want to use generated keys, jump to step 8 after adding the key to GitHub.\nUpload github keys if you want to use the old keys. Upload button will be located in the centered button on the left panel. After uploading, the keys are located in /notebooks/.\n\n\n\n\nUpload button\n\n\n\ncd ~/.ssh: This is where generated keys are stored.\nls -al: Let’s take a look at the keys.\nrm id_*: Remove the files starting with id_.\nmv /notebooks/id_* .: Move keys to here.\nDo chmod 600 id_rsa, chmod 700 ~/.ssh, and chmod 644 id_rsa.pub: These commands change the permissions on the files and its directory.\nssh git@github.com: Try using ssh with github.\ncd: Move to home directory\nmv .ssh /storage/cfg: Move .ssh into the storage.\nln -s /storage/cfg/.ssh: Create a symlink.\nTry ssh again.\n\nSo, we can connect to GitHub using ssh. But, it won’t work after we restart our instance. So, we have to edit /storage/pre-run.sh like the following. We just have to add those two lines at the end of the image into the file. The rest remains the same.\n\n\n\npre-run.sh after adding ssh\n\n\nThat’t it. Try reopening your instance. It should work!\n\n\n\n\nIn this article, we covered $PATH, conda/mamba, and setting up paperspace. Setting up paperspace is tricky, and it is easy to get lost in the middle. Do not rush and take your time. If you messed up pre-run.sh script, and an instance cannot start, try to create a new instance with something other than fastai template and modify the pre-run.sh script that way. pre-run.sh is run only in fastai template.\nIf your instance works as intended, try to do it from scratch by changing pre-run.sh to pre-run.sh.old and creating a new instance. It will be a good practice.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 3"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-15-live_coding3/index.html#path",
    "href": "blog/posts/2022-11-15-live_coding3/index.html#path",
    "title": "Live coding 3",
    "section": "",
    "text": "What happens when we type a command in the shell? How does it know if any command exists?\nIn a shell, there is an environment variable called $PATH. This is where our shell looks for a command. To take a close look at it, type echo $PATH.\n\n\n\nAn output to echo $PATH\n\n\nThis is a string seperated by a colon (:). Pay attention to the first one. This is where mambaforge comes in. Anything related to python that we will use will come from this directory. Try typing which python or which jupyter.\n\n\n\n\n\n\nNote\n\n\n\nFeel free to check out what’s in ~/mambaforge/bin with ls command.\n\n\n\n\n\nAn output to which python\n\n\nIt’s interesting to see that mambaforge has a bin directory that contains binary files or executables, like an operating system. In fact, it is very similar to Unix system. Take a look:\n\n\n\nUnix vs. Mambaforge\n\n\nIn Unix system, / refers to a root of a file system. This is where everything is stored.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 3"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-15-live_coding3/index.html#new-environment-on-condamamba",
    "href": "blog/posts/2022-11-15-live_coding3/index.html#new-environment-on-condamamba",
    "title": "Live coding 3",
    "section": "",
    "text": "Let’s try creating a new environment with mamba. This way, we can try different versions of python. To create a new environment called tmp with python version less than 3.10, we type mamba create -n tmp 'python&lt;3.10' fastcore. We are also downloading fastcore in here.\n\n\n\n\n\n\nWarning\n\n\n\nIt’s easier to manage projects when everything is simple. Having multiple environments can cause many problems. Create environments with caution.\n\n\n\n\n\nInstalling a new environment\n\n\nAfter it’s done, activate it with mamba activate tmp. We can see that our environment is changed from (base) to (tmp) in the beginning of our prompt. Now, python’s path is changed. mambaforge/envs/ is where environments are stored, and inside of the tmp environment, we have another Unix system here!\n\n\n\nInstalling a new environment\n\n\nTo go back to our (base) environment, we can type conda activate.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 3"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-15-live_coding3/index.html#paperspace",
    "href": "blog/posts/2022-11-15-live_coding3/index.html#paperspace",
    "title": "Live coding 3",
    "section": "",
    "text": "Paperspace is a great environment where we can work on use GPU for free. There are other free options as well, such as Kaggle or Google colab. However, in paperspace, we have a storage where we can save our packages, such as ctags. Paperspace instance also lasts up to 6 hours. There is more freedom involved with this one.\nOn the other hand, it can be very tricky and confusing to setup for those who are not very familiar with using a command line. It is also a great opportunity to learn about different skills, such as creating bash scripts and symbolic links.\nFirst thing we will do is to create a notebook at paperspace. After signing in and creating a project, we can create a notebook. Here, we are actually creating an instance or a server. Let’s go ahead and create one. On the project screen, you will see the Create button on the top right corner.\n\n\n\nCreate button\n\n\nNow, we choose a template. We will be using fastai, so we choos that one.\n\n\n\nChoose a template\n\n\nNext, we choose a hardware for a machine. There are many options, but we can choose CPU for now. We can always switch to GPU later if we need to.\n\n\n\nChoose a hardware\n\n\nWe choose 6 hours, but we can also choose less than this.\n\n\n\nAuto shutdown timeout\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are not using the instance, please shut it down so that other people can access it, instead of letting it sit there until the timer is maxed out. It not only saves energy and the earth, but also helps paperspace cost down.\n\n\nLastly, we turn the advanced option on and remove Workspace URL. This Workspace URL is a URL for a git that will be installed on this instance. By default, it will install fastbook, but it is easier to setup our machine without anything in it. Then, click START NOTEBOOK.\n\n\n\nAdvanced options\n\n\nThis is paperspace GUI. We can start working in this environment if we wanted to. Let’s see what we have here.\n\n\n\nPaperspace GUI\n\n\nOne important button is Stop Machine. When we are done, make sure to click this button. \nAnother good thing about paperspace is that we have an option to start working in Jupyterlab. This is great because we are familiar with this environment. On the left panel, the last orange button is the Jupyterlab button. Click it.\n\n\n\nJupyterlab button\n\n\n\n\n\nOn paperspace instance, everything is wiped out upon closing the instance, except for some special directories. those directories are /storage/ and /notebooks/. As long as we save our files in here, we can access them later. However, there’s a difference between those two. /storage/ is shared among all the instances, but /notebooks/ is unique to this instance. Let’s install a package using pip and make the package consistent.\nFirst thing we have to do is open up a terminal. It’s the first option on the bottom.\n\n\n\nA terminal\n\n\nWe will install flask as an example. You can install whatever you want. It just has to be something that’s not installed yet. So, it cannot be fastai.\nHere are the steps:\n\npip install --user flask: This installs flask in ~/.local/bin because of --user flag. It’s necessary that we install packages in this place. Then, open up python and import flask. It should not raise any error. Exit out of python.\ncd: Change directory to home directory.\nls -al: List all the files here. We can see that we have .local directory in here. Take a look inside of this directory and find out what’s in here.\ncd /storage: This is the space allocated to us to keep files persistent.\nmkdir cfg: Create a cfg (config) directory in here. We will place all configuration things in here.\ncd: Go back to home directory.\nmv .local /storage/cfg/: Move .local to our config directory in our persistent storage. If we try to import flask now, python raises error because it doesn’t exist.\nln -s /storage/cfg/.local: Create a symbolic link from /storage/cfg/.local to here (home directory). This link allows us to use .local as if it’s in this directory.\nls -al: If we look at .local, it looks something like .local -&gt; /storage/cfg/.local/. Try to go into this directory or list stuff in here.\nNow, import flask, and it works.\n\n\n\n\n\n\n\nTipShell tips: !$\n\n\n\nWhen we move a file or directory somewhere like mv .local /storage/cfg/, and we want to check what’s in /storage/cfg/ right after, we can do ls !$. !$ refers to the last parameter from the previous command. So, ls !$ expands into ls /storage/cfg/. More tips and features can be found here\n\n\nNow, if we restart the instance, we will have .local in our storage. However, there is a problem. We won’t have the symbolic links next time. So we have to create a bash script that can help us with this problem.\nHead over to /storage, and type vim pre-run.sh. This will open up vim, which is a text editor. We can press i to start typing and press Escape to stop typing. We can use arrow keys to move around. After pressing Escape, we are in command mode, so we can save the file and exit. This is the file I’ve written. Let’s go over what each line does.\n\n\n\npre-run.sh\n\n\n\n\n\n\n\n\nNoteUsing vim\n\n\n\nFor those who have never used vim before, it can be confusing. However, vim is very powerful as we will learn more about its capabilities later on.\n\n\n#!/usr/bin/env bash means this file is a bash script. Commands in this file are just all the commands we’ve been typing on the shell, such as cd, rm, and more. Lines starting with # are comments, which means these lines are not commands. We can write notes to ourselves so that we can easily understand what’s going on. pushd ~ saves the current location so that we can go back here. You can try this command out. Use this command in current directory, go to somewhere else, and do popd. We are back. Next three lines are pretty straight forward. Move to home directory, delete .local directory, and create a symlink.\nTo save this file, press Escape and press :wq. Now, the file is saved. :wq means write and quit. Now, we have to make this file executable. We can do it with chmod 744 pre-run.sh. Find out what chmod (change mode) does here by using man command or by searching online. To try out the script, we type ./pre-run.sh.\nWhen paperspace instance starts, /run.sh is run, which contains a line to execute /storage/pre-run.sh if it exists. Check out /run.sh and find out what it does.\n\n\n\nNext thing we want to setup on our paperspace is ssh. As we have seen from the previous article, we can use git to commit with ssh.\nWe can either create a new key pair, or use the one we created from last time. Using a new key pair requires adding the public key into the GitHub, but using the old one can be used right away. However, using a new key can be safer in case someone else takes a control of our machine.\nI will just use my previous key because I don’t like storing many keys on GitHub.\nLet’s get it started. Let’s follow these steps and execute the commands.\n\nssh-keygen: Create .ssh/ directory in home directory that contains key pairs. If you want to use generated keys, jump to step 8 after adding the key to GitHub.\nUpload github keys if you want to use the old keys. Upload button will be located in the centered button on the left panel. After uploading, the keys are located in /notebooks/.\n\n\n\n\nUpload button\n\n\n\ncd ~/.ssh: This is where generated keys are stored.\nls -al: Let’s take a look at the keys.\nrm id_*: Remove the files starting with id_.\nmv /notebooks/id_* .: Move keys to here.\nDo chmod 600 id_rsa, chmod 700 ~/.ssh, and chmod 644 id_rsa.pub: These commands change the permissions on the files and its directory.\nssh git@github.com: Try using ssh with github.\ncd: Move to home directory\nmv .ssh /storage/cfg: Move .ssh into the storage.\nln -s /storage/cfg/.ssh: Create a symlink.\nTry ssh again.\n\nSo, we can connect to GitHub using ssh. But, it won’t work after we restart our instance. So, we have to edit /storage/pre-run.sh like the following. We just have to add those two lines at the end of the image into the file. The rest remains the same.\n\n\n\npre-run.sh after adding ssh\n\n\nThat’t it. Try reopening your instance. It should work!",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 3"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-15-live_coding3/index.html#conclusion",
    "href": "blog/posts/2022-11-15-live_coding3/index.html#conclusion",
    "title": "Live coding 3",
    "section": "",
    "text": "In this article, we covered $PATH, conda/mamba, and setting up paperspace. Setting up paperspace is tricky, and it is easy to get lost in the middle. Do not rush and take your time. If you messed up pre-run.sh script, and an instance cannot start, try to create a new instance with something other than fastai template and modify the pre-run.sh script that way. pre-run.sh is run only in fastai template.\nIf your instance works as intended, try to do it from scratch by changing pre-run.sh to pre-run.sh.old and creating a new instance. It will be a good practice.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 3"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-16-Resnet_part2/index.html",
    "href": "blog/posts/2024-01-16-Resnet_part2/index.html",
    "title": "Resnet Part2",
    "section": "",
    "text": "We learned about what Resnet is in Resnet Blog. However, we didn’t train it, so we have no idea how much better it is than just convolutional layers. In this blog, we will train and find out how much better we can do.\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\nfrom miniai.xtras import *\n\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\n\ndls = get_dls(num_workers=2)\nxb, yb = next(iter(dls.train))\nxb.shape, yb\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4,  ..., 8, 0, 3]))\n\n\n\n\n\nLet’s start with a conv net and go from there. In Schedular blog, we had four convolutional layers. To improve the score, we can add two more layers. One layer to make it deeper and increases the number of features dimension to 256. Another layer is the first layer that uses stride of one and increases the feature dimension to 8. Let’s see how that makes a difference.\n\ndef get_cbs(**kwargs):\n    metrics = MetricsCB(**kwargs)\n    cbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\n    return cbs\n\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\ncbs = get_cbs(accuracy=MulticlassAccuracy())\n\nnfs = [8,16,32,64,128,256]\ndef get_model():\n    layers = [conv(1, 8, stride=1, act=act_gr, norm=nn.BatchNorm2d)]\n    layers += [conv(nfs[i], nfs[i+1], act=act_gr, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  396986\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nSequential\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n96\n\n\nSequential\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n1200\n\n\nSequential\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n4704\n\n\nSequential\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n18624\n\n\nSequential\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n74112\n\n\nSequential\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n295680\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\ndef get_sched(lr, epochs, dls=dls):\n    tmax = epochs * len(dls.train)\n    sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n    return BatchSchedCB(sched)\n\n\nlr, epochs = 6e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.817\n0.523\n0\nTrue\n00:02\n\n\n0.831\n0.669\n0\nFalse\n00:00\n\n\n0.873\n0.352\n1\nTrue\n00:02\n\n\n0.877\n0.363\n1\nFalse\n00:00\n\n\n0.904\n0.263\n2\nTrue\n00:02\n\n\n0.897\n0.273\n2\nFalse\n00:00\n\n\n0.925\n0.201\n3\nTrue\n00:02\n\n\n0.915\n0.233\n3\nFalse\n00:00\n\n\n0.943\n0.154\n4\nTrue\n00:02\n\n\n0.921\n0.220\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s an improvement. However, we want to do better. Let’s use ConvBlock, which doubles the number of convnets.\n\n\n\nConvBlock simply has two convolutional layers. Second conv has the stride of the argument and changes the number of features.\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, ni, ks=ks, stride=1, act=act, norm=norm)\n        self.conv2 = conv(ni, nf, ks=ks, stride=stride, act=act, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\ndef get_model():\n    layers = [ConvBlock(1, 8, stride=1, act=act_gr, norm=nn.BatchNorm2d)]\n    layers += [ConvBlock(nfs[i], nfs[i+1], act=act_gr, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  594158\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nConvBlock\n(1024, 1, 28, 28)\n(1024, 8, 14, 14)\n108\n\n\nConvBlock\n(1024, 8, 14, 14)\n(1024, 16, 7, 7)\n1800\n\n\nConvBlock\n(1024, 16, 7, 7)\n(1024, 32, 4, 4)\n7056\n\n\nConvBlock\n(1024, 32, 4, 4)\n(1024, 64, 2, 2)\n27936\n\n\nConvBlock\n(1024, 64, 2, 2)\n(1024, 128, 1, 1)\n111168\n\n\nConvBlock\n(1024, 128, 1, 1)\n(1024, 256, 1, 1)\n443520\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 8e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.788\n0.589\n0\nTrue\n00:02\n\n\n0.764\n2.427\n0\nFalse\n00:00\n\n\n0.841\n0.444\n1\nTrue\n00:02\n\n\n0.827\n0.621\n1\nFalse\n00:00\n\n\n0.877\n0.331\n2\nTrue\n00:02\n\n\n0.857\n0.398\n2\nFalse\n00:00\n\n\n0.899\n0.269\n3\nTrue\n00:02\n\n\n0.890\n0.293\n3\nFalse\n00:00\n\n\n0.914\n0.230\n4\nTrue\n00:02\n\n\n0.900\n0.275\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s not very great. We can improve this with carefully redesigning each convnets in the block.\n\n\nInstead of changing number of features and stride at the same, it might be useful to separate them out. First, we increase the features with stride one. It’s like paying closely attention to details and saving them into bigger number of features.\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, nf, ks=ks, stride=1, act=act, norm=norm)\n        self.conv2 = conv(nf, nf, ks=ks, stride=stride, act=act, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\ndef get_model():\n    layers = [ConvBlock(1, 8, stride=1, act=act_gr, norm=nn.BatchNorm2d)]\n    layers += [ConvBlock(nfs[i], nfs[i+1], act=act_gr, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  1184738\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nConvBlock\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n696\n\n\nConvBlock\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n3552\n\n\nConvBlock\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n14016\n\n\nConvBlock\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n55680\n\n\nConvBlock\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n221952\n\n\nConvBlock\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n886272\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 8e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.811\n0.519\n0\nTrue\n00:02\n\n\n0.691\n2.792\n0\nFalse\n00:00\n\n\n0.857\n0.405\n1\nTrue\n00:02\n\n\n0.853\n0.403\n1\nFalse\n00:00\n\n\n0.904\n0.260\n2\nTrue\n00:02\n\n\n0.881\n0.325\n2\nFalse\n00:00\n\n\n0.925\n0.204\n3\nTrue\n00:02\n\n\n0.915\n0.233\n3\nFalse\n00:00\n\n\n0.942\n0.157\n4\nTrue\n00:02\n\n\n0.926\n0.208\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s better! How about if we stride first? I don’t think it would be better than the previous approach, but let’s see how it performs.\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, ni, ks=ks, stride=stride, act=act, norm=norm)\n        self.conv2 = conv(ni, nf, ks=ks, stride=1, act=act, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  594158\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nConvBlock\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n108\n\n\nConvBlock\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n1800\n\n\nConvBlock\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n7056\n\n\nConvBlock\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n27936\n\n\nConvBlock\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n111168\n\n\nConvBlock\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n443520\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 1e-1, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.796\n0.563\n0\nTrue\n00:02\n\n\n0.739\n1.534\n0\nFalse\n00:00\n\n\n0.844\n0.445\n1\nTrue\n00:02\n\n\n0.805\n0.698\n1\nFalse\n00:00\n\n\n0.890\n0.295\n2\nTrue\n00:02\n\n\n0.843\n0.435\n2\nFalse\n00:00\n\n\n0.911\n0.239\n3\nTrue\n00:02\n\n\n0.901\n0.263\n3\nFalse\n00:00\n\n\n0.928\n0.195\n4\nTrue\n00:02\n\n\n0.913\n0.235\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected, first modification is the best. Let’s use resnet we learned in Resnet blog.\n\n\n\n\nFor ResBlock, we do not want to have activation layer for the second convnet. So, that’s what ConvBlock2 does.\n\nclass ConvBlock2(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, nf, ks=ks, stride=1, act=act, norm=norm)\n        self.conv2 = conv(nf, nf, ks=ks, stride=stride, act=False, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.id_conv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=None)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n        self.convs = ConvBlock2(ni, nf, ks=ks, stride=stride, act=act, norm=norm)\n        self.act = act()\n    \n    def forward(self, x):\n        return self.act(self.convs(x) + self.id_conv(self.pool(x)))\n\n\ndef get_model(act=act_gr):\n    layers = [ResBlock(1, 8, stride=1, act=act, norm=nn.BatchNorm2d)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 2e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.809\n0.537\n0\nTrue\n00:02\n\n\n0.815\n0.601\n0\nFalse\n00:00\n\n\n0.879\n0.324\n1\nTrue\n00:02\n\n\n0.877\n0.343\n1\nFalse\n00:00\n\n\n0.913\n0.236\n2\nTrue\n00:02\n\n\n0.894\n0.281\n2\nFalse\n00:00\n\n\n0.933\n0.179\n3\nTrue\n00:02\n\n\n0.918\n0.226\n3\nFalse\n00:00\n\n\n0.954\n0.127\n4\nTrue\n00:02\n\n\n0.923\n0.214\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThis is about the same as using normal convolutional nets. However, the real power of resnet comes from deeper layers. In much deeper networks (over 100 layers), normal ConvBlocks will collapse eventually and perform worse than shallower networks.\n\n\nNow that we are using much deeper networks, how much does it matter if we change activation functions to nn.ReLU from GeneralRelu?\n\ndef get_model(act=act_gr):\n    layers = [ResBlock(1, 8, stride=1, act=act, norm=nn.BatchNorm2d)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(init_weights)\n\n\nlr, epochs = 2e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(act=nn.ReLU), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.804\n0.553\n0\nTrue\n00:02\n\n\n0.811\n0.578\n0\nFalse\n00:00\n\n\n0.891\n0.296\n1\nTrue\n00:02\n\n\n0.882\n0.344\n1\nFalse\n00:00\n\n\n0.915\n0.231\n2\nTrue\n00:02\n\n\n0.896\n0.288\n2\nFalse\n00:00\n\n\n0.937\n0.172\n3\nTrue\n00:02\n\n\n0.913\n0.234\n3\nFalse\n00:00\n\n\n0.955\n0.124\n4\nTrue\n00:02\n\n\n0.925\n0.207\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThere is almost no difference in accuracy. Maybe because we are using Batch Normalization, they are corrected. Well, it runs faster, so we will change back to nn.ReLU.\n\n\n\n\nAnother improvement we can make is make it wider with a bigger kernel size of 5. To compensate, we can increase the feature size to 16.\n\nnfs = [16,32,64,128,256]\n\ndef get_model(act=nn.ReLU):\n    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=nn.BatchNorm2d)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(init_weights)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 3e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(act=nn.ReLU), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.816\n0.522\n0\nTrue\n00:02\n\n\n0.568\n1.813\n0\nFalse\n00:00\n\n\n0.886\n0.307\n1\nTrue\n00:02\n\n\n0.881\n0.331\n1\nFalse\n00:00\n\n\n0.914\n0.232\n2\nTrue\n00:02\n\n\n0.912\n0.240\n2\nFalse\n00:00\n\n\n0.936\n0.172\n3\nTrue\n00:02\n\n\n0.925\n0.206\n3\nFalse\n00:00\n\n\n0.958\n0.118\n4\nTrue\n00:02\n\n\n0.931\n0.193\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s an improvement again! We reached 93.1% accuracy.\n\n\n\nWe used Resnet in this blog and used different kinds of ConvBlocks. We also used wider version to improve accuracy. However, we are not done yet. There are some more improvements we can make. In the future blogs, we will learn about data augmentation, test time augmentation (TTA), ensembling, and dropout.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet Part2"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-16-Resnet_part2/index.html#import-libraries",
    "href": "blog/posts/2024-01-16-Resnet_part2/index.html#import-libraries",
    "title": "Resnet Part2",
    "section": "",
    "text": "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\nfrom miniai.xtras import *\n\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\n\ndls = get_dls(num_workers=2)\nxb, yb = next(iter(dls.train))\nxb.shape, yb\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4,  ..., 8, 0, 3]))",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet Part2"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-16-Resnet_part2/index.html#convolutional-net",
    "href": "blog/posts/2024-01-16-Resnet_part2/index.html#convolutional-net",
    "title": "Resnet Part2",
    "section": "",
    "text": "Let’s start with a conv net and go from there. In Schedular blog, we had four convolutional layers. To improve the score, we can add two more layers. One layer to make it deeper and increases the number of features dimension to 256. Another layer is the first layer that uses stride of one and increases the feature dimension to 8. Let’s see how that makes a difference.\n\ndef get_cbs(**kwargs):\n    metrics = MetricsCB(**kwargs)\n    cbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\n    return cbs\n\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\ncbs = get_cbs(accuracy=MulticlassAccuracy())\n\nnfs = [8,16,32,64,128,256]\ndef get_model():\n    layers = [conv(1, 8, stride=1, act=act_gr, norm=nn.BatchNorm2d)]\n    layers += [conv(nfs[i], nfs[i+1], act=act_gr, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  396986\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nSequential\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n96\n\n\nSequential\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n1200\n\n\nSequential\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n4704\n\n\nSequential\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n18624\n\n\nSequential\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n74112\n\n\nSequential\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n295680\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\ndef get_sched(lr, epochs, dls=dls):\n    tmax = epochs * len(dls.train)\n    sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n    return BatchSchedCB(sched)\n\n\nlr, epochs = 6e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.817\n0.523\n0\nTrue\n00:02\n\n\n0.831\n0.669\n0\nFalse\n00:00\n\n\n0.873\n0.352\n1\nTrue\n00:02\n\n\n0.877\n0.363\n1\nFalse\n00:00\n\n\n0.904\n0.263\n2\nTrue\n00:02\n\n\n0.897\n0.273\n2\nFalse\n00:00\n\n\n0.925\n0.201\n3\nTrue\n00:02\n\n\n0.915\n0.233\n3\nFalse\n00:00\n\n\n0.943\n0.154\n4\nTrue\n00:02\n\n\n0.921\n0.220\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s an improvement. However, we want to do better. Let’s use ConvBlock, which doubles the number of convnets.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet Part2"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-16-Resnet_part2/index.html#convblock",
    "href": "blog/posts/2024-01-16-Resnet_part2/index.html#convblock",
    "title": "Resnet Part2",
    "section": "",
    "text": "ConvBlock simply has two convolutional layers. Second conv has the stride of the argument and changes the number of features.\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, ni, ks=ks, stride=1, act=act, norm=norm)\n        self.conv2 = conv(ni, nf, ks=ks, stride=stride, act=act, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\ndef get_model():\n    layers = [ConvBlock(1, 8, stride=1, act=act_gr, norm=nn.BatchNorm2d)]\n    layers += [ConvBlock(nfs[i], nfs[i+1], act=act_gr, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  594158\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nConvBlock\n(1024, 1, 28, 28)\n(1024, 8, 14, 14)\n108\n\n\nConvBlock\n(1024, 8, 14, 14)\n(1024, 16, 7, 7)\n1800\n\n\nConvBlock\n(1024, 16, 7, 7)\n(1024, 32, 4, 4)\n7056\n\n\nConvBlock\n(1024, 32, 4, 4)\n(1024, 64, 2, 2)\n27936\n\n\nConvBlock\n(1024, 64, 2, 2)\n(1024, 128, 1, 1)\n111168\n\n\nConvBlock\n(1024, 128, 1, 1)\n(1024, 256, 1, 1)\n443520\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 8e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.788\n0.589\n0\nTrue\n00:02\n\n\n0.764\n2.427\n0\nFalse\n00:00\n\n\n0.841\n0.444\n1\nTrue\n00:02\n\n\n0.827\n0.621\n1\nFalse\n00:00\n\n\n0.877\n0.331\n2\nTrue\n00:02\n\n\n0.857\n0.398\n2\nFalse\n00:00\n\n\n0.899\n0.269\n3\nTrue\n00:02\n\n\n0.890\n0.293\n3\nFalse\n00:00\n\n\n0.914\n0.230\n4\nTrue\n00:02\n\n\n0.900\n0.275\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s not very great. We can improve this with carefully redesigning each convnets in the block.\n\n\nInstead of changing number of features and stride at the same, it might be useful to separate them out. First, we increase the features with stride one. It’s like paying closely attention to details and saving them into bigger number of features.\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, nf, ks=ks, stride=1, act=act, norm=norm)\n        self.conv2 = conv(nf, nf, ks=ks, stride=stride, act=act, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\ndef get_model():\n    layers = [ConvBlock(1, 8, stride=1, act=act_gr, norm=nn.BatchNorm2d)]\n    layers += [ConvBlock(nfs[i], nfs[i+1], act=act_gr, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  1184738\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nConvBlock\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n696\n\n\nConvBlock\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n3552\n\n\nConvBlock\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n14016\n\n\nConvBlock\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n55680\n\n\nConvBlock\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n221952\n\n\nConvBlock\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n886272\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 8e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.811\n0.519\n0\nTrue\n00:02\n\n\n0.691\n2.792\n0\nFalse\n00:00\n\n\n0.857\n0.405\n1\nTrue\n00:02\n\n\n0.853\n0.403\n1\nFalse\n00:00\n\n\n0.904\n0.260\n2\nTrue\n00:02\n\n\n0.881\n0.325\n2\nFalse\n00:00\n\n\n0.925\n0.204\n3\nTrue\n00:02\n\n\n0.915\n0.233\n3\nFalse\n00:00\n\n\n0.942\n0.157\n4\nTrue\n00:02\n\n\n0.926\n0.208\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s better! How about if we stride first? I don’t think it would be better than the previous approach, but let’s see how it performs.\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, ni, ks=ks, stride=stride, act=act, norm=norm)\n        self.conv2 = conv(ni, nf, ks=ks, stride=1, act=act, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()\n\nTot params:  594158\n\n\n\n\n\nModule\nInput\nOutput\nNum params\n\n\n\n\nConvBlock\n(1024, 1, 28, 28)\n(1024, 8, 28, 28)\n108\n\n\nConvBlock\n(1024, 8, 28, 28)\n(1024, 16, 14, 14)\n1800\n\n\nConvBlock\n(1024, 16, 14, 14)\n(1024, 32, 7, 7)\n7056\n\n\nConvBlock\n(1024, 32, 7, 7)\n(1024, 64, 4, 4)\n27936\n\n\nConvBlock\n(1024, 64, 4, 4)\n(1024, 128, 2, 2)\n111168\n\n\nConvBlock\n(1024, 128, 2, 2)\n(1024, 256, 1, 1)\n443520\n\n\nAdaptiveAvgPool2d\n(1024, 256, 1, 1)\n(1024, 256, 1, 1)\n0\n\n\nFlatten\n(1024, 256, 1, 1)\n(1024, 256)\n0\n\n\nLinear\n(1024, 256)\n(1024, 10)\n2570\n\n\n\n\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 1e-1, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.796\n0.563\n0\nTrue\n00:02\n\n\n0.739\n1.534\n0\nFalse\n00:00\n\n\n0.844\n0.445\n1\nTrue\n00:02\n\n\n0.805\n0.698\n1\nFalse\n00:00\n\n\n0.890\n0.295\n2\nTrue\n00:02\n\n\n0.843\n0.435\n2\nFalse\n00:00\n\n\n0.911\n0.239\n3\nTrue\n00:02\n\n\n0.901\n0.263\n3\nFalse\n00:00\n\n\n0.928\n0.195\n4\nTrue\n00:02\n\n\n0.913\n0.235\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected, first modification is the best. Let’s use resnet we learned in Resnet blog.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet Part2"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-16-Resnet_part2/index.html#resblock",
    "href": "blog/posts/2024-01-16-Resnet_part2/index.html#resblock",
    "title": "Resnet Part2",
    "section": "",
    "text": "For ResBlock, we do not want to have activation layer for the second convnet. So, that’s what ConvBlock2 does.\n\nclass ConvBlock2(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.conv1 = conv(ni, nf, ks=ks, stride=1, act=act, norm=norm)\n        self.conv2 = conv(nf, nf, ks=ks, stride=stride, act=False, norm=norm)\n    \n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, act=act_gr, norm=nn.BatchNorm2d):\n        super().__init__()\n        self.id_conv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=None)\n        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n        self.convs = ConvBlock2(ni, nf, ks=ks, stride=stride, act=act, norm=norm)\n        self.act = act()\n    \n    def forward(self, x):\n        return self.act(self.convs(x) + self.id_conv(self.pool(x)))\n\n\ndef get_model(act=act_gr):\n    layers = [ResBlock(1, 8, stride=1, act=act, norm=nn.BatchNorm2d)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(iw)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 2e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.809\n0.537\n0\nTrue\n00:02\n\n\n0.815\n0.601\n0\nFalse\n00:00\n\n\n0.879\n0.324\n1\nTrue\n00:02\n\n\n0.877\n0.343\n1\nFalse\n00:00\n\n\n0.913\n0.236\n2\nTrue\n00:02\n\n\n0.894\n0.281\n2\nFalse\n00:00\n\n\n0.933\n0.179\n3\nTrue\n00:02\n\n\n0.918\n0.226\n3\nFalse\n00:00\n\n\n0.954\n0.127\n4\nTrue\n00:02\n\n\n0.923\n0.214\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThis is about the same as using normal convolutional nets. However, the real power of resnet comes from deeper layers. In much deeper networks (over 100 layers), normal ConvBlocks will collapse eventually and perform worse than shallower networks.\n\n\nNow that we are using much deeper networks, how much does it matter if we change activation functions to nn.ReLU from GeneralRelu?\n\ndef get_model(act=act_gr):\n    layers = [ResBlock(1, 8, stride=1, act=act, norm=nn.BatchNorm2d)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(init_weights)\n\n\nlr, epochs = 2e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(act=nn.ReLU), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.804\n0.553\n0\nTrue\n00:02\n\n\n0.811\n0.578\n0\nFalse\n00:00\n\n\n0.891\n0.296\n1\nTrue\n00:02\n\n\n0.882\n0.344\n1\nFalse\n00:00\n\n\n0.915\n0.231\n2\nTrue\n00:02\n\n\n0.896\n0.288\n2\nFalse\n00:00\n\n\n0.937\n0.172\n3\nTrue\n00:02\n\n\n0.913\n0.234\n3\nFalse\n00:00\n\n\n0.955\n0.124\n4\nTrue\n00:02\n\n\n0.925\n0.207\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThere is almost no difference in accuracy. Maybe because we are using Batch Normalization, they are corrected. Well, it runs faster, so we will change back to nn.ReLU.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet Part2"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-16-Resnet_part2/index.html#wider-resnet",
    "href": "blog/posts/2024-01-16-Resnet_part2/index.html#wider-resnet",
    "title": "Resnet Part2",
    "section": "",
    "text": "Another improvement we can make is make it wider with a bigger kernel size of 5. To compensate, we can increase the feature size to 16.\n\nnfs = [16,32,64,128,256]\n\ndef get_model(act=nn.ReLU):\n    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=nn.BatchNorm2d)]\n    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=nn.BatchNorm2d) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(nfs[-1], 10)]\n    return nn.Sequential(*layers).apply(init_weights)\n\n\nTrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).lr_find()\n\n\n\n\n\n\n\n\n\nlr, epochs = 3e-2, 5\nxtras = [get_sched(lr, epochs)]\nlearn = TrainLearner(get_model(act=nn.ReLU), dls, F.cross_entropy, cbs=cbs+xtras, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.816\n0.522\n0\nTrue\n00:02\n\n\n0.568\n1.813\n0\nFalse\n00:00\n\n\n0.886\n0.307\n1\nTrue\n00:02\n\n\n0.881\n0.331\n1\nFalse\n00:00\n\n\n0.914\n0.232\n2\nTrue\n00:02\n\n\n0.912\n0.240\n2\nFalse\n00:00\n\n\n0.936\n0.172\n3\nTrue\n00:02\n\n\n0.925\n0.206\n3\nFalse\n00:00\n\n\n0.958\n0.118\n4\nTrue\n00:02\n\n\n0.931\n0.193\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s an improvement again! We reached 93.1% accuracy.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet Part2"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-16-Resnet_part2/index.html#conclusion",
    "href": "blog/posts/2024-01-16-Resnet_part2/index.html#conclusion",
    "title": "Resnet Part2",
    "section": "",
    "text": "We used Resnet in this blog and used different kinds of ConvBlocks. We also used wider version to improve accuracy. However, we are not done yet. There are some more improvements we can make. In the future blogs, we will learn about data augmentation, test time augmentation (TTA), ensembling, and dropout.",
    "crumbs": [
      "Home",
      "Blog",
      "Resnet Part2"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-14-live_coding2/index.html",
    "href": "blog/posts/2022-11-14-live_coding2/index.html",
    "title": "Live coding 2",
    "section": "",
    "text": "This is a blog based on Jeremy Howard’s Youtube video on Live coding 2. Here, we will learn about Git, tmux, and Jupyter notebook.\n\n\nWhat is Git? Git is a version control system. There are many ways of using Git, but we will use GitHub because it is widely used. With GitHub, we can easily share our code with others.\nLet’s see how we can create a repository. Let’s head over to GitHub profile and click on Repositories on the top of the page. On the right, there is a green button called ‘New’. Click the button!\n\n\n\nCreating a GitHub repository\n\n\nYou will see something like this:\n\n\n\nCreating a GitHub repository empty\n\n\nLet’s start filling in the form.\n\nFill in the repository name of your choice. It can be anything you want.\nOptionally provide a description.\nWe can choose our repository to be Public or Private, but it is good to make it Public for sharing.\nIt’s a good idea to adda a README file. This is where we introduce others about what this repository is about.\nWe add a .gitignore template with Python. This is a file that we list certain files we do not want share with others. For example, it can be secret files containing API keys or cached files.\nWe optionally choose a license. Without a license, other people cannot use our code. There are many licenses, but for sharing, it is good to have permissive license, such as MIT or Apache 2.0 or copyleft license, such as GPL. I used Apache 2.0 because it allows others to do whatever they want with my code, but also provides me with protection against patent law suits. On the other hand, copyleft license has more restrictions.\nCreate Repository!\n\n\n\n\nCreating a GitHub repository filled up\n\n\nWe have a repository of our own.\n\n\n\nour own GitHub repository\n\n\nAlthough we can make changes using on the web app, it is easier to edit files locally in our computer using our favorite editors. There are multiple ways of cloning the repository into our local computers, but we will use SSH. To use SSH, we have to create keys. On our terminal, we type ssh-keygen. We can just press Enter for default options.\n\n\n\n\n\n\nNote\n\n\n\nssh is very useful tool, so it is good to know how to use it and how it works. It is good to check the wkipedia article.\n\n\n\n\n\nSSH keygen\n\n\nFor me, it created a ~/.ssh folder, and inside it, I have the keys. One with .pub is the public key.\n\n\n\n\n\n\nCaution\n\n\n\nNever share the private key with anyone. Only share the public key.\n\n\n\n\n\nSSH keys\n\n\nWe can look at the contents of the file by using cat id_rsa.pub. Now, copy everything from ssh-rsa to email. To copy, we can highlight over the whole thing, and press Ctrl-Shift-C.\n\n\n\nCopyinb a SSH key\n\n\nLet’s head back to our repository on GitHub and click your profile picture on the top right corner. We see ‘Settings’ on the bottom.\n\n\n\nGithub Settings\n\n\nWe are on the Settings page. On the left banner, we will see SSH and GPG keys. Click it.\n\n\n\nSSH key menu\n\n\nClick New SSH key on the right.\n\n\n\nNew SSH key button\n\n\nNow, we add our Title and Key. Key is the one that we copied from our terminal.\n\n\n\nNew SSH title and key\n\n\nOur SSH key is added. Now, we go back to the repository page. Click the ‘Code’ button in the middle of the page and click SSH. Copy that URL.\n\n\n\nCloning repository\n\n\nLet’s head back to our terminal and type git clone SSH_URL.\n\n\n\nCloning repository on terminal\n\n\nNow we have a repository. Let’s use cd to change directory into the one we just cloned. Then, we can use vim to edit the readme. Type vim README.md. If you are not familiar with vim, you can use other editors. On the editor, edit the file however you want. I just added a heading called keekee.\n\n\n\nCloning repository\n\n\nWhen we do git status, we get this message back. Git is tracking we have modified README file.\n\n\n\nCloning repository\n\n\nBefore we can send our changes to GitHub, we have to do some work. Here are some steps:\n\ngit add README.md adds the file to be staged. It can be any file.\nFind out what’s changed by typing git status.\ngit commit -m \"Add a heading to README\" commits (saves) our changes to the file with a message. It can be any message, and it has to be in quotes.\ngit push pushes our changes to GitHub. Now, local changes reflect the code on GitHub.\n\n\n\n\nCloning repository\n\n\nLet’s head over to our repository page. We can see that the README is changed.\n\n\n\nCloning repository\n\n\nCool! Now we can commit to GitHub from our local computers.\n\n\n\nLet’s learn about Tmux! With this amazing tool, we can split terminals, which boosts productivity. Install it with sudo apt install tmux on Linux or brew install tmux on Mac. Then, we can execute it by typing tmux on terminal.\n\n\n\ntmux screen\n\n\nIt looks the same as before except that it doesn’t have any color. Type Ctrl - B - %.\n\n\n\ntmux screen split\n\n\nNow, we have two panels. In tmux, each of the terminal rectangle is called a panel. Now try Ctrl - B - \". It splits on the bottom.\n\n\n\ntmux screen split 2\n\n\nThis time, let’s try Ctrl - B - Z. It zooms into the current panel. We can have a full screen for us this way, and when we want to go back to multiple panels, we type Ctrl - B - Z again.\n\n\n\ntmux screen split 2\n\n\nWe can move around with Ctrl - B - Arrow_key. Can you try to create four panels like this below? If you need to close a panel, you can do Ctrl - D.\n\n\n\ntmux screen split into 4 panels\n\n\nHere, we have nice four panels. If we close the tmux and terminal, we have to open everything up one by one again. Imagine each panel had a unique process running, then it takes a lot of work to restore the workspace. One easy way to solve this problem is Ctrl - B - D to deatch tmux. We can now close the terminal and reopen. Now, type tmux a to attach. It is just like how we left it.\nIf you would like to learn more about tmux commands, you can learn look at the guide or a cheatsheet.\n\n\n\nHere are some tips for using the terminal.\n\ncd - : First one is cd -, which returns to a previous directory before you changed directory. It’s useful when you move to home directory with cd ~, and wanted to go back to previous directory. Instead of typing directories one by one, we can do cd -.\npushd ~: This is useful when we want to save where we are so that we can come back here later.\npopd: After using pushd ~, we can use popd to move to the saved directory.\n\n\n\n\nThere are Jupyter extensions that can provides useful features called Jupyter Nbextensions_configurator. We can install it using mamba install -c conda-forge jupyter_nbextensions_configurator. In this command, -c conda-forge means use a channel called conda-forge. So, mamba will look for jupyter_nbextensions_configurator inside of conda-forge channel.\nAnother thing we will install is fastbook, which includes fastai. With mamba install -c fastchan fastbook sentencepiece, everything we need is installed.\nLet’s open up Jupyter notebook inside of the new repository we created. If you pay attention to panels in the center of the screen, there are four panels now. Fourth one is Nbextensions! Click that one.\n\n\n\njupyter main screen\n\n\nThere are many extensions to explore, but I recommend ‘Collapsible Headings’. You can collapse headings with left arrow or right arrow.\n\n\n\njupyter nbextensions\n\n\nNow, create a notebook and try to import fastai. It should workwithout problem. Additionally, you can try importing fastbook and pytorch.\n\n\n\nRunning cells and importing libraries.\n\n\nLet’s try adding a heading. Add a cell on the top and change it to a markdown cell by pressing m. To change it back to code cell, press y. Change it back to markdown and type # Heading1. Then execute the cell. You can press left arrow and right arrow to close and open cells undermeath it.\n\n\n\nOpened Collapsible heading\n\n\n\n\n\nCollapsed heading\n\n\n\n\n\n\n\n\nTip\n\n\n\nMarkdown is very useful to know. If you want to know more about it, check out Quarto documentation.\n\n\nOn the top left, there is a File button. There are many thing we can do, such as rename the file. Most important options is ‘Close and Halt’, which is the last option.\n\n\n\nOptions\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you just close the tab, the notebook is still running in the background. Make sure to close and halt the notebook to free up computing resources.\n\n\nCool, now you can also push the change into the GitHub. Why don’t you try it?\n\n\n\nIn this blog, we covered how to use GitHub with ssh, tmux, and Jupyter notebook. Please watch the Youtube video if you haven’t. We haven’t done deep learning, yet but it is very important to get these steps right. You can play around with fastai and fastbook on Jupyter notebook if you want and practice using GitHub and terminal.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 2"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-14-live_coding2/index.html#git",
    "href": "blog/posts/2022-11-14-live_coding2/index.html#git",
    "title": "Live coding 2",
    "section": "",
    "text": "What is Git? Git is a version control system. There are many ways of using Git, but we will use GitHub because it is widely used. With GitHub, we can easily share our code with others.\nLet’s see how we can create a repository. Let’s head over to GitHub profile and click on Repositories on the top of the page. On the right, there is a green button called ‘New’. Click the button!\n\n\n\nCreating a GitHub repository\n\n\nYou will see something like this:\n\n\n\nCreating a GitHub repository empty\n\n\nLet’s start filling in the form.\n\nFill in the repository name of your choice. It can be anything you want.\nOptionally provide a description.\nWe can choose our repository to be Public or Private, but it is good to make it Public for sharing.\nIt’s a good idea to adda a README file. This is where we introduce others about what this repository is about.\nWe add a .gitignore template with Python. This is a file that we list certain files we do not want share with others. For example, it can be secret files containing API keys or cached files.\nWe optionally choose a license. Without a license, other people cannot use our code. There are many licenses, but for sharing, it is good to have permissive license, such as MIT or Apache 2.0 or copyleft license, such as GPL. I used Apache 2.0 because it allows others to do whatever they want with my code, but also provides me with protection against patent law suits. On the other hand, copyleft license has more restrictions.\nCreate Repository!\n\n\n\n\nCreating a GitHub repository filled up\n\n\nWe have a repository of our own.\n\n\n\nour own GitHub repository\n\n\nAlthough we can make changes using on the web app, it is easier to edit files locally in our computer using our favorite editors. There are multiple ways of cloning the repository into our local computers, but we will use SSH. To use SSH, we have to create keys. On our terminal, we type ssh-keygen. We can just press Enter for default options.\n\n\n\n\n\n\nNote\n\n\n\nssh is very useful tool, so it is good to know how to use it and how it works. It is good to check the wkipedia article.\n\n\n\n\n\nSSH keygen\n\n\nFor me, it created a ~/.ssh folder, and inside it, I have the keys. One with .pub is the public key.\n\n\n\n\n\n\nCaution\n\n\n\nNever share the private key with anyone. Only share the public key.\n\n\n\n\n\nSSH keys\n\n\nWe can look at the contents of the file by using cat id_rsa.pub. Now, copy everything from ssh-rsa to email. To copy, we can highlight over the whole thing, and press Ctrl-Shift-C.\n\n\n\nCopyinb a SSH key\n\n\nLet’s head back to our repository on GitHub and click your profile picture on the top right corner. We see ‘Settings’ on the bottom.\n\n\n\nGithub Settings\n\n\nWe are on the Settings page. On the left banner, we will see SSH and GPG keys. Click it.\n\n\n\nSSH key menu\n\n\nClick New SSH key on the right.\n\n\n\nNew SSH key button\n\n\nNow, we add our Title and Key. Key is the one that we copied from our terminal.\n\n\n\nNew SSH title and key\n\n\nOur SSH key is added. Now, we go back to the repository page. Click the ‘Code’ button in the middle of the page and click SSH. Copy that URL.\n\n\n\nCloning repository\n\n\nLet’s head back to our terminal and type git clone SSH_URL.\n\n\n\nCloning repository on terminal\n\n\nNow we have a repository. Let’s use cd to change directory into the one we just cloned. Then, we can use vim to edit the readme. Type vim README.md. If you are not familiar with vim, you can use other editors. On the editor, edit the file however you want. I just added a heading called keekee.\n\n\n\nCloning repository\n\n\nWhen we do git status, we get this message back. Git is tracking we have modified README file.\n\n\n\nCloning repository\n\n\nBefore we can send our changes to GitHub, we have to do some work. Here are some steps:\n\ngit add README.md adds the file to be staged. It can be any file.\nFind out what’s changed by typing git status.\ngit commit -m \"Add a heading to README\" commits (saves) our changes to the file with a message. It can be any message, and it has to be in quotes.\ngit push pushes our changes to GitHub. Now, local changes reflect the code on GitHub.\n\n\n\n\nCloning repository\n\n\nLet’s head over to our repository page. We can see that the README is changed.\n\n\n\nCloning repository\n\n\nCool! Now we can commit to GitHub from our local computers.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 2"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-14-live_coding2/index.html#tmux",
    "href": "blog/posts/2022-11-14-live_coding2/index.html#tmux",
    "title": "Live coding 2",
    "section": "",
    "text": "Let’s learn about Tmux! With this amazing tool, we can split terminals, which boosts productivity. Install it with sudo apt install tmux on Linux or brew install tmux on Mac. Then, we can execute it by typing tmux on terminal.\n\n\n\ntmux screen\n\n\nIt looks the same as before except that it doesn’t have any color. Type Ctrl - B - %.\n\n\n\ntmux screen split\n\n\nNow, we have two panels. In tmux, each of the terminal rectangle is called a panel. Now try Ctrl - B - \". It splits on the bottom.\n\n\n\ntmux screen split 2\n\n\nThis time, let’s try Ctrl - B - Z. It zooms into the current panel. We can have a full screen for us this way, and when we want to go back to multiple panels, we type Ctrl - B - Z again.\n\n\n\ntmux screen split 2\n\n\nWe can move around with Ctrl - B - Arrow_key. Can you try to create four panels like this below? If you need to close a panel, you can do Ctrl - D.\n\n\n\ntmux screen split into 4 panels\n\n\nHere, we have nice four panels. If we close the tmux and terminal, we have to open everything up one by one again. Imagine each panel had a unique process running, then it takes a lot of work to restore the workspace. One easy way to solve this problem is Ctrl - B - D to deatch tmux. We can now close the terminal and reopen. Now, type tmux a to attach. It is just like how we left it.\nIf you would like to learn more about tmux commands, you can learn look at the guide or a cheatsheet.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 2"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-14-live_coding2/index.html#terminal-tips",
    "href": "blog/posts/2022-11-14-live_coding2/index.html#terminal-tips",
    "title": "Live coding 2",
    "section": "",
    "text": "Here are some tips for using the terminal.\n\ncd - : First one is cd -, which returns to a previous directory before you changed directory. It’s useful when you move to home directory with cd ~, and wanted to go back to previous directory. Instead of typing directories one by one, we can do cd -.\npushd ~: This is useful when we want to save where we are so that we can come back here later.\npopd: After using pushd ~, we can use popd to move to the saved directory.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 2"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-14-live_coding2/index.html#jupyter-notebook",
    "href": "blog/posts/2022-11-14-live_coding2/index.html#jupyter-notebook",
    "title": "Live coding 2",
    "section": "",
    "text": "There are Jupyter extensions that can provides useful features called Jupyter Nbextensions_configurator. We can install it using mamba install -c conda-forge jupyter_nbextensions_configurator. In this command, -c conda-forge means use a channel called conda-forge. So, mamba will look for jupyter_nbextensions_configurator inside of conda-forge channel.\nAnother thing we will install is fastbook, which includes fastai. With mamba install -c fastchan fastbook sentencepiece, everything we need is installed.\nLet’s open up Jupyter notebook inside of the new repository we created. If you pay attention to panels in the center of the screen, there are four panels now. Fourth one is Nbextensions! Click that one.\n\n\n\njupyter main screen\n\n\nThere are many extensions to explore, but I recommend ‘Collapsible Headings’. You can collapse headings with left arrow or right arrow.\n\n\n\njupyter nbextensions\n\n\nNow, create a notebook and try to import fastai. It should workwithout problem. Additionally, you can try importing fastbook and pytorch.\n\n\n\nRunning cells and importing libraries.\n\n\nLet’s try adding a heading. Add a cell on the top and change it to a markdown cell by pressing m. To change it back to code cell, press y. Change it back to markdown and type # Heading1. Then execute the cell. You can press left arrow and right arrow to close and open cells undermeath it.\n\n\n\nOpened Collapsible heading\n\n\n\n\n\nCollapsed heading\n\n\n\n\n\n\n\n\nTip\n\n\n\nMarkdown is very useful to know. If you want to know more about it, check out Quarto documentation.\n\n\nOn the top left, there is a File button. There are many thing we can do, such as rename the file. Most important options is ‘Close and Halt’, which is the last option.\n\n\n\nOptions\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you just close the tab, the notebook is still running in the background. Make sure to close and halt the notebook to free up computing resources.\n\n\nCool, now you can also push the change into the GitHub. Why don’t you try it?",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 2"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-14-live_coding2/index.html#conclusion",
    "href": "blog/posts/2022-11-14-live_coding2/index.html#conclusion",
    "title": "Live coding 2",
    "section": "",
    "text": "In this blog, we covered how to use GitHub with ssh, tmux, and Jupyter notebook. Please watch the Youtube video if you haven’t. We haven’t done deep learning, yet but it is very important to get these steps right. You can play around with fastai and fastbook on Jupyter notebook if you want and practice using GitHub and terminal.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 2"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html",
    "href": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html",
    "title": "Alien vs. Ghost Pt.2 Deploying",
    "section": "",
    "text": "In this article, we will deploy the model that we trained using FastAI on part 1 on Hugging Face Spaces with Gradio and nbdev.\n\n\n\nprediction page deployed on Hugging Face Spaces\n\n\nBut why do we want to bother to deploy our model? We have a model that works! Well, because we cannot easily share our working model with others. We do not want our friends and family to run jupyter notebook or colab in order to try out what we created. We could make a website ourselves, but there is a better way: using Gradio. Gradio is an easy way to setup a web app for others. You can play with the final product. It is deployed on Hugging Face Spaces because they take care of all the backend things. Once, deployed, we do not have to take care of scaling and other headaches.\nAnother tool we are using today is nbdev. nbdev allows us to convert code in Jupyter notebook into modules so that we do not have to manually copy and paste code from Jupyter notebook into a file. So, while we are figuring out and experimenting with our code in Jupyter notebook, we are also writing finished product as well.\n\n\n\n\n\n\nTip\n\n\n\nSince we are creating a web app, we can run it on mobile devices as well. I shared this with my family through phones, and it was very fun.\n\n\n\n\nFirst thing we will do is create a Hugging Face Space. Let’s head over to Hugging Face Spaces. Click Create new Space on the right. You will be directed to this page:\n\n\n\nCreating a new space\n\n\nFill in Space name, choose a License and click Gradio. Then, scroll down to create space.\nNow, we have a Github repository that we can start working on. We can clone the repository by typing git clone https://huggingface.co/spaces/galopyz/Exciting_project in the terminal. If you are not familiar with Github, here is a tutorial.\nWe will be working in this repository for the rest of this article. So, let’s head there and open up a Jupyter notebook.\n\n\n\nFirst, grab the model we trained from part 1 and import Gradio and FastAI. We need Gradio to create widgets and FastAI to make predictions.\nIf you don’t have Gradio installed on your computer, you can use pip install gradio to install it on the terminal or !pip install gradio in the Jupyter notebook cell like this:\n!pip install gradio\n\nbulla bulla (downloading stuff)\n\nThen, we import libraries. As we can see, we have #| export at the top of the cell. This is called a directive. This simply lets nbdev know that we need this cell when we convert this Jupyter notebook into a module later. Without this, nbdev will skip this cell and move onto the next one with #| export. Because our module needs to import those libraries in order to make predictions and create widgets, we need this cell to be exported.\n#| export\nfrom fastai.vision.all import *\nimport gradio as gr\nWe assign learn using load_learner on the pickled model. This is same learn that we created in part 1, so we can also make predictions with it.\n#| export\nlearn = load_learner('alien_vs_ghost.pkl')\n\nlearn.dls.vocab\n\n['alien', 'ghost']\n\n\nAgain, we are using export directive to load learner. However, we are not exporting the cell containing learn.dls.vocab because we are just checking vocab (categories or labels we trained the model with). So, cells that we are experimenting and checking values do not have to be exported.\nLet’s download a new image and test the model. You can go to a search engine and download an image and test it to see if it still works!\n\npath = Path('alien1.jpeg')\nalien1 = PILImage.create(path)\nalien1.thumbnail((192, 192))\nalien1\n\n\n\n\n\n\n\n\n\nlearn.predict(alien1)\n\n\n\n\n\n\n\n\n('alien', TensorBase(0), TensorBase([9.9972e-01, 2.7873e-04]))\n\n\nThat worked! Now let’s create a function that classifies images and returns the probabilities on each category. Then, we can use this for the widget.\n#|export\ncategories = learn.dls.vocab\ndef classify_images(img):\n    \"\"\"classifies images and returns the probabilities on each categories.\"\"\"\n    pred, pred_idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float, probs)))\n\nclassify_images(alien1)\n\n\n\n\n\n\n\n\n{'alien': 0.9997212290763855, 'ghost': 0.00027872977079823613}\n\n\nWonderful! Now, we can use Gradio to try out different images. You can download more images and put them in examples list so that we can try different ones. Once we run this, we will get a link to a local URL so that we can try it out. You will have something similar to this.\n#|export\nimage = gr.inputs.Image(shape=(192, 192))\nlabel = gr.outputs.Label()\n# If you have more or less examples, edit this list.\nexamples = ['alien1.jpeg', 'alien2.jpeg', 'ghost1.jpeg', 'ghost2.jpeg', 'dunno1.jpeg', 'dunno2.jpeg']\n\ninf = gr.Interface(fn=classify_images, inputs=image, outputs=label, examples=examples)\ninf.launch(inline=False)\n\nRunning on local URL:  http://127.0.0.1:7860\nTo create a public link, set `share=True` in `launch()`.\n(&lt;gradio.routes.App&gt;, 'http://127.0.0.1:7860/', None)\n\n.\nThat’s cool! Let’s deploy it to Hugging Face Spaces!\n\n\n\nThis is the time nbdev will do the magic. Create a cell like this on the top of our Jupyter notebook:\n#| default_exp app\nAs you can see from #|, this is a directive. This simply sets a name of the module we are about to export. So, resulting python module will be called app.py.\nIf nbdev is not installed yet, you can do pip install nbdev or conda install -c fastai nbdev.\nThen, we simply import it, and export it. First argument for nbdev.export.nb_export is the name of the Jupyter notebook we are currently on right now, and second argument is a path where nbdev will create a new python module. So, in this example, nbdev will create a python module from the notebook called index.ipynb into the current directory.\n\nimport nbdev\n\n\nnbdev.export.nb_export('index.ipynb', '.')\n\nNow, check out app.py that is generated automatically! How cool is this?\n\n!cat app.py\n\n# AUTOGENERATED! DO NOT EDIT! File to edit: index.ipynb.\n\n# %% auto 0\n__all__ = ['learn', 'categories', 'image', 'label', 'examples', 'inf', 'classify_images']\n\n# %% index.ipynb 14\nfrom fastai.vision.all import *\nimport gradio as gr\n\n# %% index.ipynb 17\nlearn = load_learner('alien_vs_ghost.pkl')\n\n# %% index.ipynb 25\ncategories = learn.dls.vocab\ndef classify_images(img):\n    \"\"\"classifies images and returns the probabilities on each categories.\"\"\"\n    pred, pred_idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float, probs)))\n\n# %% index.ipynb 29\nimage = gr.inputs.Image(shape=(192, 192))\nlabel = gr.outputs.Label()\n# If you have more or less examples, edit this list.\nexamples = ['alien1.jpeg', 'alien2.jpeg', 'ghost1.jpeg', 'ghost2.jpeg', 'dunno1.jpeg', 'dunno2.jpeg']\n\ninf = gr.Interface(fn=classify_images, inputs=image, outputs=label, examples=examples)\ninf.launch(inline=False)\n\n\nImagine if we did not use nbdev. We would have to go back to the beginning of the notebook and try to find out which lines of code need to be copied into a python module. This is a small notebook, but imagine having a long notebook! nbdev takes care of all the headache.\n\n\n\nIn the deployment environment, there is no FastAI. So, we have to create a file called requirements.txt. Inside of the file, write\nfastai\ntorch==1.12.1\nin there. You can take a peak here.\n\n\n\nFinally, last step is uploading files to Github. We have to upload everything in this directory except for the Jupyter notebook we worked on. One problem with uploading is that our pickled model is too big to be uploaded into Github directly. So, we need Git LFS (Git Large File Storage).\n\nwe install it with git lfs install\nwe track big files with git lfs track \"*.pkl\"\nwe save the settings with git add .gitattributes\nwe add everything in the directory to Github with git add .\nLet’s commit with git commit -am \"Deploying to huggingface spaces\"\nLastly, we push with git push\n\nThat’s it. It should now be running in your Hugging Spaces in a minute!\n\n\n\nAs we have seen, training a model and deploying it does not cost any money. Also, it did not take a very long time to do it either. This is something anybody can do if they are willing to learn some tools, such as Github, Jupyter notebook, Python, nbdev, and Hugging Face Spaces.\nSo, now that you have an amazing project, why don’t you share it with others? You can share here as well.\nI hope you had a lot of fun and more in the future!",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.2 Deploying"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#create-hugging-face-spaces",
    "href": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#create-hugging-face-spaces",
    "title": "Alien vs. Ghost Pt.2 Deploying",
    "section": "",
    "text": "First thing we will do is create a Hugging Face Space. Let’s head over to Hugging Face Spaces. Click Create new Space on the right. You will be directed to this page:\n\n\n\nCreating a new space\n\n\nFill in Space name, choose a License and click Gradio. Then, scroll down to create space.\nNow, we have a Github repository that we can start working on. We can clone the repository by typing git clone https://huggingface.co/spaces/galopyz/Exciting_project in the terminal. If you are not familiar with Github, here is a tutorial.\nWe will be working in this repository for the rest of this article. So, let’s head there and open up a Jupyter notebook.",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.2 Deploying"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#grab-the-model-and-get-libraries",
    "href": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#grab-the-model-and-get-libraries",
    "title": "Alien vs. Ghost Pt.2 Deploying",
    "section": "",
    "text": "First, grab the model we trained from part 1 and import Gradio and FastAI. We need Gradio to create widgets and FastAI to make predictions.\nIf you don’t have Gradio installed on your computer, you can use pip install gradio to install it on the terminal or !pip install gradio in the Jupyter notebook cell like this:\n!pip install gradio\n\nbulla bulla (downloading stuff)\n\nThen, we import libraries. As we can see, we have #| export at the top of the cell. This is called a directive. This simply lets nbdev know that we need this cell when we convert this Jupyter notebook into a module later. Without this, nbdev will skip this cell and move onto the next one with #| export. Because our module needs to import those libraries in order to make predictions and create widgets, we need this cell to be exported.\n#| export\nfrom fastai.vision.all import *\nimport gradio as gr\nWe assign learn using load_learner on the pickled model. This is same learn that we created in part 1, so we can also make predictions with it.\n#| export\nlearn = load_learner('alien_vs_ghost.pkl')\n\nlearn.dls.vocab\n\n['alien', 'ghost']\n\n\nAgain, we are using export directive to load learner. However, we are not exporting the cell containing learn.dls.vocab because we are just checking vocab (categories or labels we trained the model with). So, cells that we are experimenting and checking values do not have to be exported.\nLet’s download a new image and test the model. You can go to a search engine and download an image and test it to see if it still works!\n\npath = Path('alien1.jpeg')\nalien1 = PILImage.create(path)\nalien1.thumbnail((192, 192))\nalien1\n\n\n\n\n\n\n\n\n\nlearn.predict(alien1)\n\n\n\n\n\n\n\n\n('alien', TensorBase(0), TensorBase([9.9972e-01, 2.7873e-04]))\n\n\nThat worked! Now let’s create a function that classifies images and returns the probabilities on each category. Then, we can use this for the widget.\n#|export\ncategories = learn.dls.vocab\ndef classify_images(img):\n    \"\"\"classifies images and returns the probabilities on each categories.\"\"\"\n    pred, pred_idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float, probs)))\n\nclassify_images(alien1)\n\n\n\n\n\n\n\n\n{'alien': 0.9997212290763855, 'ghost': 0.00027872977079823613}\n\n\nWonderful! Now, we can use Gradio to try out different images. You can download more images and put them in examples list so that we can try different ones. Once we run this, we will get a link to a local URL so that we can try it out. You will have something similar to this.\n#|export\nimage = gr.inputs.Image(shape=(192, 192))\nlabel = gr.outputs.Label()\n# If you have more or less examples, edit this list.\nexamples = ['alien1.jpeg', 'alien2.jpeg', 'ghost1.jpeg', 'ghost2.jpeg', 'dunno1.jpeg', 'dunno2.jpeg']\n\ninf = gr.Interface(fn=classify_images, inputs=image, outputs=label, examples=examples)\ninf.launch(inline=False)\n\nRunning on local URL:  http://127.0.0.1:7860\nTo create a public link, set `share=True` in `launch()`.\n(&lt;gradio.routes.App&gt;, 'http://127.0.0.1:7860/', None)\n\n.\nThat’s cool! Let’s deploy it to Hugging Face Spaces!",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.2 Deploying"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#export-using-nbdev",
    "href": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#export-using-nbdev",
    "title": "Alien vs. Ghost Pt.2 Deploying",
    "section": "",
    "text": "This is the time nbdev will do the magic. Create a cell like this on the top of our Jupyter notebook:\n#| default_exp app\nAs you can see from #|, this is a directive. This simply sets a name of the module we are about to export. So, resulting python module will be called app.py.\nIf nbdev is not installed yet, you can do pip install nbdev or conda install -c fastai nbdev.\nThen, we simply import it, and export it. First argument for nbdev.export.nb_export is the name of the Jupyter notebook we are currently on right now, and second argument is a path where nbdev will create a new python module. So, in this example, nbdev will create a python module from the notebook called index.ipynb into the current directory.\n\nimport nbdev\n\n\nnbdev.export.nb_export('index.ipynb', '.')\n\nNow, check out app.py that is generated automatically! How cool is this?\n\n!cat app.py\n\n# AUTOGENERATED! DO NOT EDIT! File to edit: index.ipynb.\n\n# %% auto 0\n__all__ = ['learn', 'categories', 'image', 'label', 'examples', 'inf', 'classify_images']\n\n# %% index.ipynb 14\nfrom fastai.vision.all import *\nimport gradio as gr\n\n# %% index.ipynb 17\nlearn = load_learner('alien_vs_ghost.pkl')\n\n# %% index.ipynb 25\ncategories = learn.dls.vocab\ndef classify_images(img):\n    \"\"\"classifies images and returns the probabilities on each categories.\"\"\"\n    pred, pred_idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float, probs)))\n\n# %% index.ipynb 29\nimage = gr.inputs.Image(shape=(192, 192))\nlabel = gr.outputs.Label()\n# If you have more or less examples, edit this list.\nexamples = ['alien1.jpeg', 'alien2.jpeg', 'ghost1.jpeg', 'ghost2.jpeg', 'dunno1.jpeg', 'dunno2.jpeg']\n\ninf = gr.Interface(fn=classify_images, inputs=image, outputs=label, examples=examples)\ninf.launch(inline=False)\n\n\nImagine if we did not use nbdev. We would have to go back to the beginning of the notebook and try to find out which lines of code need to be copied into a python module. This is a small notebook, but imagine having a long notebook! nbdev takes care of all the headache.",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.2 Deploying"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#dependency-requirement",
    "href": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#dependency-requirement",
    "title": "Alien vs. Ghost Pt.2 Deploying",
    "section": "",
    "text": "In the deployment environment, there is no FastAI. So, we have to create a file called requirements.txt. Inside of the file, write\nfastai\ntorch==1.12.1\nin there. You can take a peak here.",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.2 Deploying"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#upload-files-to-github",
    "href": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#upload-files-to-github",
    "title": "Alien vs. Ghost Pt.2 Deploying",
    "section": "",
    "text": "Finally, last step is uploading files to Github. We have to upload everything in this directory except for the Jupyter notebook we worked on. One problem with uploading is that our pickled model is too big to be uploaded into Github directly. So, we need Git LFS (Git Large File Storage).\n\nwe install it with git lfs install\nwe track big files with git lfs track \"*.pkl\"\nwe save the settings with git add .gitattributes\nwe add everything in the directory to Github with git add .\nLet’s commit with git commit -am \"Deploying to huggingface spaces\"\nLastly, we push with git push\n\nThat’s it. It should now be running in your Hugging Spaces in a minute!",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.2 Deploying"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#conclusion",
    "href": "blog/posts/2022-11-11-alien-vs.-ghost-pt2/index.html#conclusion",
    "title": "Alien vs. Ghost Pt.2 Deploying",
    "section": "",
    "text": "As we have seen, training a model and deploying it does not cost any money. Also, it did not take a very long time to do it either. This is something anybody can do if they are willing to learn some tools, such as Github, Jupyter notebook, Python, nbdev, and Hugging Face Spaces.\nSo, now that you have an amazing project, why don’t you share it with others? You can share here as well.\nI hope you had a lot of fun and more in the future!",
    "crumbs": [
      "Home",
      "Blog",
      "Alien vs. Ghost Pt.2 Deploying"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "I have been taking Part 2 of FastAI courses, which covers matrix multiplication to stable diffusion. In the course, we built MiniAI from scratch. MiniAI is a small and flexible library that makes building AI tools very easy. I am almost done with the course, and I want to share some utility functions I have built on top of MiniAI. These are very useful to me, and I hope you find it that way as well. Here’s a code for this notebook.\nSome additional functionalities added to miniai. - When training models, output time along with loss and metrics. - Get data from pytorch datasets. - Plot weight statistics after fit using ActivationStats. - MyNorm is a mixture of LayerNorm’s simplicity and Batchnorm’s way of taking means/stds.\n\n\nLet’s import some libraries first. Cells that start with #|export are exported using nbdev, which turns jupyter notebook into python script.\n\nimport torch\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nfrom torcheval.metrics import MulticlassAccuracy\nimport torch.nn.functional as F\n\n\n\n\nIn FastAI’s course22 part 2, we use Fashion MNIST datasets from Hugging Face. Hugging Face datasets store images with PyArrow, and we have to convert them into tensors each batch for training. Therefore, we could use get_dls to create DataLoaders from Pytorch datasets.\n\ndls = get_dls()\nxb, yb = next(iter(dls.train))\nxb.shape, yb\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4,  ..., 8, 0, 3]))\n\n\n\n\n\nThis is a function from fastprogress. It converts seconds to mm:ss and h:mm:ss if there is an hour.\n\nformat_time(3)\n\n'00:03'\n\n\nIt truncates all the decimals.\n\nformat_time(3.999)\n\n'00:03'\n\n\n\nformat_time(65)\n\n'01:05'\n\n\n\nformat_time(4888)\n\n'1:21:28'\n\n\n\n\n\nWe encountered MetricsCB in 09_learner.ipynb notebook. This callback computes loss and metric functions. After epoch, it logs current epoch, whether in training or not, loss and metric values.\nI thought it would be useful to also add time into the callback, so I used format_time for that.\nLet’s take a look at an example.\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics]\nlearn = Learner(nn.Sequential(*cnn_layers()), dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(2)\n\n{'accuracy': '0.298', 'loss': '2.076', 'epoch': 0, 'train': True, 'time': '00:01'}\n{'accuracy': '0.370', 'loss': '1.962', 'epoch': 0, 'train': False, 'time': '00:00'}\n{'accuracy': '0.636', 'loss': '0.972', 'epoch': 1, 'train': True, 'time': '00:01'}\n{'accuracy': '0.740', 'loss': '0.692', 'epoch': 1, 'train': False, 'time': '00:00'}\n\n\n\n\n\nLet’s take a look at ProgressCB, which we also encountered in 09_learner.ipynb notebook. This callback adds nice progress bar, which we can visualize the progress. It can also plot train and valid losses in different frequencies. When plot is True, update_freq can be batch, epoch, or fit. Since updating each batch is too frequent, I wanted to add some options.\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(nn.Sequential(*cnn_layers()), dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.286\n2.120\n0\nTrue\n00:01\n\n\n0.571\n1.095\n0\nFalse\n00:00\n\n\n0.663\n0.907\n1\nTrue\n00:01\n\n\n0.672\n0.817\n1\nFalse\n00:00\n\n\n0.741\n0.676\n2\nTrue\n00:01\n\n\n0.763\n0.629\n2\nFalse\n00:00\n\n\n0.777\n0.589\n3\nTrue\n00:01\n\n\n0.794\n0.556\n3\nFalse\n00:00\n\n\n0.804\n0.522\n4\nTrue\n00:01\n\n\n0.805\n0.507\n4\nFalse\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI only added after_fit to ActivationStats, which simply plots parameter statistics after fitting. It just saves typing.\n\n\n\nMyNorm is a normalization technique which combines the simplicity of LayerNorm and calculation of mean and standard deviation like BatchNorm. Normalization techniquea re covered in (11_initializing.ipynb)[https://github.com/galopyz/course22p2/blob/master/nbs/11_initializing.ipynb] I tested on 14_augment.ipynb notebook, and MyNorm performs as good as nn.BatchNorm2d. In 14_augment-Copy1, I used BatchNorm, LayerNorm, and MyNorm without momentum. In 14_augment-Copy2, I used MyNorm with momentum. In these notebooks, total time spent is not very accurate. Simply adding time for each epochs gave me much higher value.\nThis MyNorm has momentum like BatchNorm. It does not necessarily train better than one without momentum. If we do a momentum with 1, it is same as having no momentum.\n\n\n\nWe looked at some utility functions, and I found them to be very useful when I was studying the course. For instance, finding out roughly how long it took to train an epoch was helpful. MyNorm was fun to try out in different models. I found out that when training for 50 epochs with data augmentation, it really didn’t matter which normalization technique I used. They were about the same. Therefore, it was better to use optimized Pytorch’s nn.BatchNorm2d for speed. I hope you found these useful as well.",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#import-libraries",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#import-libraries",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "Let’s import some libraries first. Cells that start with #|export are exported using nbdev, which turns jupyter notebook into python script.\n\nimport torch\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nfrom torcheval.metrics import MulticlassAccuracy\nimport torch.nn.functional as F",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#get_dls",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#get_dls",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "In FastAI’s course22 part 2, we use Fashion MNIST datasets from Hugging Face. Hugging Face datasets store images with PyArrow, and we have to convert them into tensors each batch for training. Therefore, we could use get_dls to create DataLoaders from Pytorch datasets.\n\ndls = get_dls()\nxb, yb = next(iter(dls.train))\nxb.shape, yb\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4,  ..., 8, 0, 3]))",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#format_time",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#format_time",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "This is a function from fastprogress. It converts seconds to mm:ss and h:mm:ss if there is an hour.\n\nformat_time(3)\n\n'00:03'\n\n\nIt truncates all the decimals.\n\nformat_time(3.999)\n\n'00:03'\n\n\n\nformat_time(65)\n\n'01:05'\n\n\n\nformat_time(4888)\n\n'1:21:28'",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#metricscb",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#metricscb",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "We encountered MetricsCB in 09_learner.ipynb notebook. This callback computes loss and metric functions. After epoch, it logs current epoch, whether in training or not, loss and metric values.\nI thought it would be useful to also add time into the callback, so I used format_time for that.\nLet’s take a look at an example.\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\ndef cnn_layers():\n    return [\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        nn.Flatten()]\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics]\nlearn = Learner(nn.Sequential(*cnn_layers()), dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(2)\n\n{'accuracy': '0.298', 'loss': '2.076', 'epoch': 0, 'train': True, 'time': '00:01'}\n{'accuracy': '0.370', 'loss': '1.962', 'epoch': 0, 'train': False, 'time': '00:00'}\n{'accuracy': '0.636', 'loss': '0.972', 'epoch': 1, 'train': True, 'time': '00:01'}\n{'accuracy': '0.740', 'loss': '0.692', 'epoch': 1, 'train': False, 'time': '00:00'}",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#progresscb",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#progresscb",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "Let’s take a look at ProgressCB, which we also encountered in 09_learner.ipynb notebook. This callback adds nice progress bar, which we can visualize the progress. It can also plot train and valid losses in different frequencies. When plot is True, update_freq can be batch, epoch, or fit. Since updating each batch is too frequent, I wanted to add some options.\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\nlearn = Learner(nn.Sequential(*cnn_layers()), dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.286\n2.120\n0\nTrue\n00:01\n\n\n0.571\n1.095\n0\nFalse\n00:00\n\n\n0.663\n0.907\n1\nTrue\n00:01\n\n\n0.672\n0.817\n1\nFalse\n00:00\n\n\n0.741\n0.676\n2\nTrue\n00:01\n\n\n0.763\n0.629\n2\nFalse\n00:00\n\n\n0.777\n0.589\n3\nTrue\n00:01\n\n\n0.794\n0.556\n3\nFalse\n00:00\n\n\n0.804\n0.522\n4\nTrue\n00:01\n\n\n0.805\n0.507\n4\nFalse\n00:00",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#activationstats",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#activationstats",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "I only added after_fit to ActivationStats, which simply plots parameter statistics after fitting. It just saves typing.",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#mynorm",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#mynorm",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "MyNorm is a normalization technique which combines the simplicity of LayerNorm and calculation of mean and standard deviation like BatchNorm. Normalization techniquea re covered in (11_initializing.ipynb)[https://github.com/galopyz/course22p2/blob/master/nbs/11_initializing.ipynb] I tested on 14_augment.ipynb notebook, and MyNorm performs as good as nn.BatchNorm2d. In 14_augment-Copy1, I used BatchNorm, LayerNorm, and MyNorm without momentum. In 14_augment-Copy2, I used MyNorm with momentum. In these notebooks, total time spent is not very accurate. Simply adding time for each epochs gave me much higher value.\nThis MyNorm has momentum like BatchNorm. It does not necessarily train better than one without momentum. If we do a momentum with 1, it is same as having no momentum.",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2024-01-13-MiniAI_utilities/index.html#conclusion",
    "href": "blog/posts/2024-01-13-MiniAI_utilities/index.html#conclusion",
    "title": "MiniAI Utilities",
    "section": "",
    "text": "We looked at some utility functions, and I found them to be very useful when I was studying the course. For instance, finding out roughly how long it took to train an epoch was helpful. MyNorm was fun to try out in different models. I found out that when training for 50 epochs with data augmentation, it really didn’t matter which normalization technique I used. They were about the same. Therefore, it was better to use optimized Pytorch’s nn.BatchNorm2d for speed. I hope you found these useful as well.",
    "crumbs": [
      "Home",
      "Blog",
      "MiniAI Utilities"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-23-bllms_2/index.html",
    "href": "blog/posts/2025-02-23-bllms_2/index.html",
    "title": "Building LLM part2-Processing Text",
    "section": "",
    "text": "In this blog, we will go through chapter 2 of “Build a Large Language Model From Scratch” by Sebastian Raschka. This chapter is about working with text. It goes over preparing text for LLMs, splitting text into word and subword tokens, byte pair encoding, sliding window for dataloader sampling, and converting tokens into embeddings.\nFollow along on google colab!\nImage generated from copilot.\nHere is an outline:",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part2-Processing Text"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-23-bllms_2/index.html#introduction",
    "href": "blog/posts/2025-02-23-bllms_2/index.html#introduction",
    "title": "Building LLM part2-Processing Text",
    "section": "Introduction",
    "text": "Introduction\nIn the last blog, we went over an introduction to Large Language Models (LLMs). In this blog, we will go over preparing text data for the training. First, we tokenize the text into numbers. Second, we build the dataloader. Lastly, we turn it into embeddings. As a bonus, we will also go over byte pair encoding in the end. Materials for this blog are from chapter 2 of “Build a Large Language Model From Scratch” by Sebastian Raschka with some adjustments. And the images and code are from the book and the author’s github repo.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part2-Processing Text"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-23-bllms_2/index.html#tokenizing-text",
    "href": "blog/posts/2025-02-23-bllms_2/index.html#tokenizing-text",
    "title": "Building LLM part2-Processing Text",
    "section": "Tokenizing Text",
    "text": "Tokenizing Text\nThere are many ways to tokenize text, but we will keep it simple. We will only split the text into words, punctuations, and special characters and then convert them into numbers. This is called encoding. On the other hand, decoding is converting the numbers back into the text. We will use a dictionary to map each token to a number, and this will become a vocabulary. We will also add some special tokens to the vocabulary, such as &lt;|unk|&gt; for unknown tokens. Then we can test our encoding and decoding.\n\n\n\n\n\n\nNote\n\n\n\nByte Pair Encoding\nIn practice, texts are not tokenized by each word as we did here. This is only for demonstration purpose to keep it simple and easy to understand. One drawback from this technique is unknown words. There are so many vocabulary words, and training would cost so much resources. To solve this problem, texts can be tokenized by each alphabet. Problem with this is that individual alphabet does not carry enough information, and it would have to train more.\nThis is where Byte Pair Encoding (BPE) comes in. This is in the middle ground between the two. We will go over BPE at the end of this blog.\n\n\n\nPreprocessing text\nIn preprocessing step, we will split the text into words, punctuations, and special characters. We will use the re module to split the text into tokens.\n\nimport re\ntext = \"The cat sat on the mat!\"\nres = re.split(r'(\\s)', text)\nres\n\n['The', ' ', 'cat', ' ', 'sat', ' ', 'on', ' ', 'the', ' ', 'mat!']\n\n\nLet’s also split on special characters and punctuation.\n\nres = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nres\n\n['The', ' ', 'cat', ' ', 'sat', ' ', 'on', ' ', 'the', ' ', 'mat', '!', '']\n\n\nWe can remove the white space using list comprehension.\n\nres = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n[o.strip() for o in res if o.strip()]\n\n['The', 'cat', 'sat', 'on', 'the', 'mat', '!']\n\n\nLet’s use a bigger text.\n\nraw_text = '''The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'''\nraw_text\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\n\nprep = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\nprep = [o.strip() for o in prep if o.strip()]\nprep[:15]\n\n['The',\n 'cat',\n 'sat',\n 'on',\n 'the',\n 'mat',\n '!',\n 'She',\n 'saw',\n 'a',\n 'red',\n 'ball',\n 'rolling',\n 'by',\n ',']\n\n\n\n\nCreating vocab\nNow that we have preprocessed the text, we can create a vocabulary. The vocabulary is a mapping from each token to a number. We will use a dictionary to store the mapping.\n\nall_words = sorted(set(prep))\nvocab_size = len(all_words)\nvocab_size\n\n38\n\n\n\nvocab = {token:integer for integer,token in enumerate(all_words)}\nvocab\n\n{'!': 0,\n '\"': 1,\n ',': 2,\n '.': 3,\n '123': 4,\n '?': 5,\n '@': 6,\n 'Main': 7,\n 'Meow': 8,\n 'She': 9,\n 'Street': 10,\n 'The': 11,\n 'a': 12,\n 'and': 13,\n 'at': 14,\n 'ball': 15,\n 'by': 16,\n 'cat': 17,\n 'chase': 18,\n 'it': 19,\n 'jumped': 20,\n 'looking': 21,\n 'mat': 22,\n 'now': 23,\n 'on': 24,\n 'red': 25,\n 'rolling': 26,\n 'sat': 27,\n 'saw': 28,\n 'she': 29,\n 'table': 30,\n 'that': 31,\n 'the': 32,\n 'to': 33,\n 'under': 34,\n 'up': 35,\n 'was': 36,\n 'wondered': 37}\n\n\nWe also need a way to reverse the mapping.\n\nrev_vocab = {i:s for s,i in vocab.items()}\nrev_vocab\n\n{0: '!',\n 1: '\"',\n 2: ',',\n 3: '.',\n 4: '123',\n 5: '?',\n 6: '@',\n 7: 'Main',\n 8: 'Meow',\n 9: 'She',\n 10: 'Street',\n 11: 'The',\n 12: 'a',\n 13: 'and',\n 14: 'at',\n 15: 'ball',\n 16: 'by',\n 17: 'cat',\n 18: 'chase',\n 19: 'it',\n 20: 'jumped',\n 21: 'looking',\n 22: 'mat',\n 23: 'now',\n 24: 'on',\n 25: 'red',\n 26: 'rolling',\n 27: 'sat',\n 28: 'saw',\n 29: 'she',\n 30: 'table',\n 31: 'that',\n 32: 'the',\n 33: 'to',\n 34: 'under',\n 35: 'up',\n 36: 'was',\n 37: 'wondered'}\n\n\n\n\nEncoding and Decoding\nNow that we have the vocabulary, we can encode and decode text.\n\ntokens = [vocab[s] for s in prep]\ntokens[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\nAnd here is how we decode. We first turn it back into a list of tokens, then join them together.\n\nstrs = [rev_vocab[i] for i in tokens]\nstrs[:10]\n\n['The', 'cat', 'sat', 'on', 'the', 'mat', '!', 'She', 'saw', 'a']\n\n\n\nstrs = ' '.join(strs)\nstrs\n\n'The cat sat on the mat ! She saw a red ball rolling by , and jumped up to chase it . \" Meow ? \" she wondered , looking at the ball that was now under the table @ 123 Main Street .'\n\n\nThere are extra spaces in the decoded text. Let’s remove them using re.sub.\n\nre.sub(r'\\s+([,.?!\"()\\'])', r'\\1', strs)\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\n\nraw_text\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\nLet’s compare it to the original text. It looks pretty good, except it.\" Meow?\". It should be it. \"Meow?\". But it’s not a big deal. We can fix it later.\nPutting it all together, here is SimpleTokenizerV1 class that we can use to encode and decode text.\n\nclass SimpleTokenizerV1:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n    \n    def encode(self, text):\n        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        return [self.str_to_int[s] for s in preprocessed]\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\n\n\ntokenizer = SimpleTokenizerV1(vocab)\ntokenizer.encode(raw_text)[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\n\ntokens[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\n\ntokenizer.decode(tokenizer.encode(raw_text))\n\n'The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered, looking at the ball that was now under the table @ 123 Main Street.'\n\n\n\n\nAdding special tokens\nThere are some special tokens such as &lt;|unk|&gt; that we need to add to the vocabulary. We will add them to the vocabulary and update the tokenizer.\nSpecial tokens in LLMs serve specific functional purposes and are typically added to the vocabulary with reserved IDs (usually at the beginning). Here are the most common ones:\n\n[UNK] or &lt;|unk|&gt; - Used for unknown tokens not in vocabulary\n[PAD] - Used to pad sequences to a fixed length in a batch\n[BOS] or &lt;|startoftext|&gt; - Marks the beginning of a sequence\n[EOS] or &lt;|endoftext|&gt; - Marks the end of a sequence\n[SEP] - Used to separate different segments of text (common in BERT)\n[CLS] - Special classification token (used in BERT-like models)\n[MASK] - Used for masked language modeling tasks\n\nThese tokens are crucial because they: - Help models understand sequence boundaries - Enable batch processing of variable-length sequences - Support specific training objectives - Handle out-of-vocabulary words\nThere are many special tokens. They help the model understand the sequence boundaries, handle out-of-vocabulary words, and support specific training objectives. However, GPT-2 only used &lt;|endoftext|&gt; because it could also be used for padding. This token is also used for separating documents, such as wikipedia articles. It signals the model that the article ended.\nOur tokenizer fails when it encounters a token that is not in the vocabulary. Let’s add a special token &lt;|unk|&gt; to the vocabulary and update the tokenizer.\n\ntokenizer.encode(\"wassup yo\")\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 tokenizer.encode(\"wassup yo\")\n\nCell In[14], line 9, in SimpleTokenizerV1.encode(self, text)\n      7 preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n      8 preprocessed = [item.strip() for item in preprocessed if item.strip()]\n----&gt; 9 return [self.str_to_int[s] for s in preprocessed]\n\nCell In[14], line 9, in &lt;listcomp&gt;(.0)\n      7 preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n      8 preprocessed = [item.strip() for item in preprocessed if item.strip()]\n----&gt; 9 return [self.str_to_int[s] for s in preprocessed]\n\nKeyError: 'wassup'\n\n\n\n\nlen(vocab)\n\n38\n\n\n\nvocab['&lt;|endoftext|&gt;'] = len(vocab)\nvocab['&lt;|unk|&gt;'] = len(vocab)\nvocab\n\n{'!': 0,\n '\"': 1,\n ',': 2,\n '.': 3,\n '123': 4,\n '?': 5,\n '@': 6,\n 'Main': 7,\n 'Meow': 8,\n 'She': 9,\n 'Street': 10,\n 'The': 11,\n 'a': 12,\n 'and': 13,\n 'at': 14,\n 'ball': 15,\n 'by': 16,\n 'cat': 17,\n 'chase': 18,\n 'it': 19,\n 'jumped': 20,\n 'looking': 21,\n 'mat': 22,\n 'now': 23,\n 'on': 24,\n 'red': 25,\n 'rolling': 26,\n 'sat': 27,\n 'saw': 28,\n 'she': 29,\n 'table': 30,\n 'that': 31,\n 'the': 32,\n 'to': 33,\n 'under': 34,\n 'up': 35,\n 'was': 36,\n 'wondered': 37,\n '&lt;|endoftext|&gt;': 38,\n '&lt;|unk|&gt;': 39}\n\n\nNow, we can update our encoder to use special tokens.\n\nclass SimpleTokenizerV2:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n    \n    def encode(self, text):\n        prep = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n        prep = [item.strip() for item in prep if item.strip()]\n        prep = [item if item in self.str_to_int else \"&lt;|unk|&gt;\" for item in prep]\n        return [self.str_to_int[s] for s in prep]\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\n\n\ntokenizer = SimpleTokenizerV2(vocab)\n\ntext1 = \"Wassup yo, how's it going?\"\ntext2 = \"The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \\\"Meow?\\\" she wondered\"\ntext = \" &lt;|endoftext|&gt; \".join((text1, text2))\nprint(text)\n\nWassup yo, how's it going? &lt;|endoftext|&gt; The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it. \"Meow?\" she wondered\n\n\n\ntokenizer.encode(text)[:10]\n\n[39, 39, 2, 39, 39, 39, 19, 39, 5, 38]\n\n\n\ntokenizer.decode(tokenizer.encode(text))\n\n'&lt;|unk|&gt; &lt;|unk|&gt;, &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; it &lt;|unk|&gt;? &lt;|endoftext|&gt; The cat sat on the mat! She saw a red ball rolling by, and jumped up to chase it.\" Meow?\" she wondered'\n\n\nGreat. We can encode and decode without getting an error from the vocabulary. GPT-2 did not use the &lt;|unk|&gt; token. Instead, it used a byte pair encoding method to handle out-of-vocabulary words. We will go over byte pair encoding in the end.\n\n\n\n\n\n\nNote\n\n\n\nEvil &lt;|unk|&gt; token\nWhy would we use Byte Pair Encoding (BPE) when we could use &lt;|unk|&gt; token to encode? We’re not getting any error anymore so the problem is solved, right? Actually, there is another problem. When training Large Language Models, if the model sees many unknown tokens in the training data, it doesn’t learn very much.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part2-Processing Text"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-23-bllms_2/index.html#creating-dataloader",
    "href": "blog/posts/2025-02-23-bllms_2/index.html#creating-dataloader",
    "title": "Building LLM part2-Processing Text",
    "section": "Creating dataloader",
    "text": "Creating dataloader\nNow that we have tokenized the text, we can create a dataloader. Using dataloader is an easy way to turn the encoded text into batches of data. In each batch, we have a sequence of tokens for x and another for y. The x sequence is the input, and the y sequence is the output. The y sequence is the same as the x sequence, but shifted by one token. This is because we want the model to predict the next token given the previous tokens. The dataloader is also responsible for batching the data and shuffling it.\n\n\ntokens = tokenizer.encode(raw_text)\ntokens[:10]\n\n[11, 17, 27, 24, 32, 22, 0, 9, 28, 12]\n\n\nWe set the context_size as 4. This means x and y are 4 tokens long. This is only a toy example, but in GPT, context size is way bigger. For example, GPT-2 has a context size of 1024. This means that the model can see up to 1024 tokens in the past. This is why GPT-2 is so good at generating text. It can see the context of the text and generate text that is more coherent. However, longer context size means more memory usage.\n\ncontext_size = 4\n\nx = tokens[:context_size]\ny = tokens[1:context_size+1]\n\nprint(f\"x: {x}\")\nprint(f\"y:      {y}\")\n\nx: [11, 17, 27, 24]\ny:      [17, 27, 24, 32]\n\n\nWhen training, this is what the model sees as x and y:\n\nfor i in range(1, context_size+1):\n    x = tokens[:i]\n    y = tokens[i]\n\n    print(x, \"----&gt;\", y)\n\n[11] ----&gt; 17\n[11, 17] ----&gt; 27\n[11, 17, 27] ----&gt; 24\n[11, 17, 27, 24] ----&gt; 32\n\n\nFor more readability, decoded version is here:\n\nfor i in range(1, context_size+1):\n    x = tokens[:i]\n    y = tokens[i]\n\n    print(tokenizer.decode(x), \"----&gt;\", tokenizer.decode([y]))\n\nThe ----&gt; cat\nThe cat ----&gt; sat\nThe cat sat ----&gt; on\nThe cat sat on ----&gt; the\n\n\nLet’s create a pytorch dataset. As long as we have __len__ and __getitem__, we can use it with pytorch dataloader.\n\nclass GPTDatasetV1:\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        token_ids = tokenizer.encode(txt, allowed_special={\"&lt;|endoftext|&gt;\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self): return len(self.input_ids)\n\n    def __getitem__(self, idx): return self.input_ids[idx], self.target_ids[idx]\n\n\n\n\n\n\n\nNote\n\n\n\nSliding Window\nSliding window is a common algorithm used in computer science. This is best understood as an example. This stackoverflow answer has diagrams, which are very easy to understand. It uses Javascript, but it is literally a range of values moving along like sliding a window in an array or a list.\n\n\nWe will use tiktoken library to get an encoding from gpt2. We’ve pretty much looked at everything in this code except stride. max_length is the context size. stride is the number of tokens to skip when creating the next sequence. For example, if max_length is 4 and stride is 2, then the next sequence will start 2 tokens after the previous sequence. This is to avoid having the same sequence in the dataset multiple times. This is a common technique in NLP. It is called sliding window. It is also called sliding window attention. By avoiding the same sequence multiple times, we can reduce the size of the dataset. This is important because we want to use as much data as possible to train the model. It also reduces overfitting.\n\nimport tiktoken\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nds = GPTDatasetV1(raw_text, tiktoken.get_encoding(\"gpt2\"), max_length=10, stride=5)\nlen(ds)\n\n7\n\n\n\ndef create_dataloader_v1(txt, batch_size=4, max_length=256, \n                         stride=128, shuffle=True, drop_last=True,\n                         num_workers=0):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers\n    )\n\nDataloader returns x and y. Let’s take a look at what stride does in a dataloader. Here is a simple example of batch_size of 1 and stride 1.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319]]), tensor([[3797, 3332,  319,  262]])]\n\n\nLet’s look at batch_size of 2. Both x and y are 2 sequences long. The first sequence is the same as the previous example. The second sequence is the next sequence in the text.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=1, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319],\n         [3797, 3332,  319,  262]]),\n tensor([[3797, 3332,  319,  262],\n         [3332,  319,  262, 2603]])]\n\n\nWhen we increase the stride to 2, the second sequence is 2 tokens after the first sequence. This is because we skipped 2 tokens when creating the second sequence. Instead of starting the second x with 3797, we start with 3332.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=2, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319],\n         [3332,  319,  262, 2603]]),\n tensor([[3797, 3332,  319,  262],\n         [ 319,  262, 2603,    0]])]\n\n\nWhen we have the same stride and max_length, we can see that the second sequence is the same as the first sequence. This is because we skipped 4 tokens when creating the second sequence. Now, there is no overlap between the x sequences and y sequences.\n\ndataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=False)\nnext(iter(dataloader))\n\n[tensor([[ 464, 3797, 3332,  319],\n         [ 262, 2603,    0, 1375]]),\n tensor([[3797, 3332,  319,  262],\n         [2603,    0, 1375, 2497]])]\n\n\nNote that we also have drop_last parameter. This is to drop the last batch if it is smaller than batch_size. This is important during training because it can cause loss spikes.\n\n\n\n\n\n\nNote\n\n\n\nSmooth Training\nWhen training, it is important to keep the loss go down smoothly. If the loss spikes up, it may not come down, and the model has to be trained again from the start. Using drop_last parameter when training helps. There are also other ways to keep it from spiking, such as using bigger batch sizes and using a high quality data. Data could have particularly noisy and unclean gibberish. If these are concentrated in one batch, loss goes up to spike, and the training is over.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part2-Processing Text"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-23-bllms_2/index.html#what-are-embeddings",
    "href": "blog/posts/2025-02-23-bllms_2/index.html#what-are-embeddings",
    "title": "Building LLM part2-Processing Text",
    "section": "What Are Embeddings?",
    "text": "What Are Embeddings?\nAn embedding is a way to represent words or phrases as vectors of numbers. These vectors capture the semantic meaning of the words, allowing us to perform mathematical operations on them. For example, we can calculate the distance between two words to see how similar they are. Embeddings are used in many NLP tasks, such as machine translation, text classification, and question answering. They are also used in recommendation systems, image recognition, and other machine learning tasks. Embeddings are a powerful tool for understanding and processing text data.\nFor example, in this space: - “Cat” and “dog” might be close together because they’re both pets - “Run” and “sprint” would be nearby as they’re similar actions - “Hot” might be positioned opposite to “cold” - “King”, “queen”, “prince”, and “princess” would form a cluster showing both their royal relationships and gender differences\nIn modern LLMs like GPT-2, each token (word or subword) is represented by a vector of 768 numbers, while larger models like GPT-3 use even bigger vectors (2048 or more dimensions). These numbers aren’t randomly assigned - they’re learned during training to capture meaningful relationships between words.\n\nWhy Do We Need Embeddings?\nTraditional one-hot encoding represents each word as a vector of zeros with a single ‘1’, where the vector length equals vocabulary size. For a 50,000-word vocabulary, each word requires a 50,000-dimensional vector! This approach has several major problems:\n\nInefficiency:\n\nMassive storage requirements\nSparse representations (mostly zeros)\nComputationally expensive to process\n\nNo Semantic Information:\n\n“cat” and “kitten” are as different as “cat” and “motorcycle”\nNo way to measure word similarity\nNo capture of analogies or relationships\n\n\nEmbeddings solve these problems by: 1. Dense Representation: - Using much smaller vectors (768 vs 50,000 dimensions) - Every dimension carries meaningful information - Efficient storage and computation\n\nSemantic Relationships:\n\nSimilar words have similar vectors\nEnable amazing arithmetic: king - man + woman ≈ queen\nCapture multiple types of relationships:\n\nSemantic (car/automobile)\nSyntactic (run/running)\nConceptual (France/Paris :: Japan/Tokyo)\n\n\nLearning and Adaptation:\n\nEmbeddings improve during model training\nCan capture domain-specific meanings\nTransfer learning from one task to another\n\nMathematical Operations:\n\nCalculate similarity using cosine distance\nFind analogies through vector arithmetic\nCluster related concepts together\n\n\n\n\nEmbeddings Beyond Language Models\nThe power of embeddings extends far beyond just processing text. The same fundamental concept - representing complex objects as dense vectors in high-dimensional space - has revolutionized many fields:\n\nRecommendation Systems:\n\nNetflix maps both users and movies into the same embedding space\nUser embeddings capture viewing preferences and habits\nMovie embeddings represent genre, style, mood, and other features\nSimilarity between vectors predicts what you might like to watch\nEven time-of-day and viewing context can be embedded\n\nComputer Vision:\n\nImages are embedded into high-dimensional spaces\nSimilar images cluster together automatically\nEnables powerful features like:\n\nFace recognition\nObject detection\nImage similarity search\nStyle transfer\n\nTransfer learning from pre-trained vision models\n\nBioinformatics:\n\nProtein sequences represented as embeddings\nCaptures complex biochemical properties\nPredicts protein folding and interactions\nHelps in drug discovery\nEnables rapid searching of similar compounds\n\nAudio Processing:\n\nSpotify embeds songs based on:\n\nMusical features (tempo, key, instruments)\nListening patterns\nCultural context\nUser behavior\n\nVoice recognition systems use embeddings\nSound classification and similarity detection\n\nGraph Networks:\n\nSocial networks embed users and relationships\nKnowledge graphs embed concepts and connections\nTraffic networks embed locations and routes\nFraud detection systems embed transaction patterns\n\n\n\n\nCreating Embeddings in Practice\nCreating embeddings involves several key components and considerations:\n\nThe Embedding Layer:\n\nembedding_dim = 768  # typical size\nvocab_size = 50257  # GPT-2 vocabulary size\nembedding_layer = nn.Embedding(vocab_size, embedding_dim)\n\nInitialization Strategies:\n\nRandom initialization\nPre-trained embeddings (Word2Vec, GloVe)\nXavier/Glorot initialization\nCustom initialization based on domain knowledge\n\nTraining Approaches:\n\nEnd-to-end with model\nSeparate pre-training\nFine-tuning existing embeddings\nFrozen pre-trained embeddings\n\nAdvanced Techniques:\n\nSubword tokenization\nContextual embeddings\nMulti-modal embeddings\nHierarchical embeddings\n\n\n\n\nThe Position Problem\nPosition information is crucial in language understanding, but transformers are inherently position-agnostic. Here’s how we solve this:\n\nLearned Positional Embeddings:\n\nModel learns position representations\nCan capture common patterns\nMore flexible but needs training\nUsed in models like GPT\n\nSinusoidal Embeddings:\n\nFixed mathematical patterns\nUses sine and cosine functions\nDifferent frequencies for different dimensions\nOriginal transformer approach\nNo training required\nCan extrapolate to longer sequences\n\nRelative Positional Embeddings:\n\nEncode relative distances between tokens\nBetter for certain tasks\nMore computationally intensive\nUsed in modern architectures like T5\n\nHybrid Approaches:\n\nCombining different types\nTask-specific adaptations\nNovel architectural innovations\n\n\n\n\nTrade-offs and Challenges\nThe use of embeddings, while powerful, comes with important considerations and challenges:\n\nTraining Data Requirements:\n\nNeed massive amounts of quality data\nData must be representative and balanced\nDomain-specific data often required\nPoor quality data leads to poor embeddings\nData cleaning and preprocessing crucial\n\nComputational Costs:\n\nTraining embeddings is resource-intensive\nLarge memory requirements\nGPU/TPU hardware often necessary\nInference time can be significant\nStorage costs for large embedding tables\n\nBias and Fairness:\n\nEmbeddings inherit biases from training data\nCan amplify societal prejudices\nGender, racial, and cultural biases common\nDebiasing techniques available but imperfect\nEthical considerations in deployment\n\nTechnical Challenges:\n\nChoosing optimal embedding dimension\nHandling out-of-vocabulary words\nDealing with rare words/tokens\nManaging embedding table size\nUpdating embeddings efficiently\n\nPerformance Trade-offs:\n\nAccuracy vs. computation speed\nMemory usage vs. embedding size\nTraining time vs. model quality\nGeneralization vs. specialization\nReal-time requirements vs. model complexity",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part2-Processing Text"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-23-bllms_2/index.html#the-future-of-embeddings",
    "href": "blog/posts/2025-02-23-bllms_2/index.html#the-future-of-embeddings",
    "title": "Building LLM part2-Processing Text",
    "section": "The Future of Embeddings",
    "text": "The Future of Embeddings\n\nArchitectural Innovations:\n\nMore efficient embedding architectures\nSparse embedding techniques\nDynamic embedding sizes\nAdaptive embedding strategies\nNovel initialization methods\n\nMulti-modal Developments:\n\nCross-modal embeddings\nUniversal embeddings across domains\nJoint learning of different modalities\nTransfer learning improvements\nDomain adaptation techniques\n\nEfficiency Improvements:\n\nCompression techniques\nQuantization methods\nPruning strategies\nDistributed embedding systems\nHardware-specific optimizations\n\nEthical Considerations:\n\nBetter debiasing techniques\nFairness-aware embeddings\nInterpretable embeddings\nPrivacy-preserving methods\nRobust evaluation metrics\n\nEmerging Applications:\n\nQuantum computing embeddings\nNeuromorphic hardware adaptation\nEdge device implementations\nReal-time embedding updates\nFederated learning approaches\n\nResearch Directions:\n\nTheoretical understanding improvements\nFormal mathematical frameworks\nStability and robustness studies\nScaling laws investigation\nNovel training objectives\n\n\nThe field of embeddings continues to be a crucial area of research and development in machine learning, with new breakthroughs and applications emerging regularly. As we better understand their properties and capabilities, we can expect to see even more innovative uses across various domains.\nTo learn more about embeddings, please refer to the following resources: - Google’s tutorial on word embeddings, document search, and applications: https://github.com/google/generative-ai-docs/tree/main/site/en/gemini-api/tutorials\n\n\n\n\n\n\nTip\n\n\n\nWord Embeddings Size\nWord embeddings size with multiples of 64 have hardware optimization.\n\n\n\nUsing Embeddings\nLet’s create embeddings with pytorch. We will use a simple example.\n\ntoken_ids = torch.tensor([2,1,0])\ntoken_ids\n\ntensor([2, 1, 0])\n\n\n\nvocab_size = 3\noutput_dim = 4\n\ntorch.manual_seed(42)\nemb = torch.nn.Embedding(vocab_size, output_dim)\nemb\n\nEmbedding(3, 4)\n\n\n\nemb.weight\n\nParameter containing:\ntensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)\n\n\nAn embedding layer has weights defined by the vocab size and the output dimension. The weights are normally distributed with mean of 0 and standard deviation of 1. These weights are learnable parameters. With these embedding layer, we can convert the token ids into embeddings by simply calling the embedding layer with the token ids.\n\n# First embedding\nemb(torch.tensor([0]))\n\ntensor([[0.3367, 0.1288, 0.2345, 0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n# Second embedding\nemb(torch.tensor([1]))\n\ntensor([[-1.1229, -0.1863,  2.2082, -0.6380]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\nWe can also simply select using an index.\n\nemb.weight[0]\n\ntensor([0.3367, 0.1288, 0.2345, 0.2303], grad_fn=&lt;SelectBackward0&gt;)\n\n\nOr we can select multiple embeddings at once in whatever order we want.\n\nemb(torch.tensor([1,0]))\n\ntensor([[-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\nemb(token_ids)\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\nOne-hot encoding\nAn older way to do this is using one hot encoding.\nWe can manually do one hot encoding\n\nparams = torch.nn.Parameter(emb.weight)\nparams\n\nParameter containing:\ntensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)\n\n\n\nonehot = torch.nn.functional.one_hot(token_ids)\nonehot\n\ntensor([[0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0]])\n\n\n\nonehot.float()@params\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;MmBackward0&gt;)\n\n\nWe can also use a linear layer\n\nlinear = torch.nn.Linear(vocab_size, output_dim, bias=False)\nlinear.weight = torch.nn.Parameter(emb.weight.T)\nlinear.weight\n\nParameter containing:\ntensor([[ 0.3367, -1.1229,  0.4617],\n        [ 0.1288, -0.1863,  0.2674],\n        [ 0.2345,  2.2082,  0.5349],\n        [ 0.2303, -0.6380,  0.8094]], requires_grad=True)\n\n\n\nlinear(onehot.float())\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;MmBackward0&gt;)\n\n\nUsing torch.nn.Embedding is the most efficient way to convert token ids into embeddings. It’s faster and more memory-efficient than using one-hot encoding or a linear layer.\n\n\n\nPositional Embeddings\nPositional embeddings are added to the token embeddings to encode the position of the token in the sequence. This is because transformers do not have any inherent sense of order. There are two main types of positional embeddings: relative positional embeddings and absolute positional embeddings.\n\nRelative positional embeddings\n\nRelative positional embeddings encode the distance between tokens. This is useful because the model can learn to pay attention to tokens that are close to each other. However, relative positional embeddings are not as efficient as absolute positional embeddings.\n\nAbsolute positional embeddings\n\nAbsolute positional embeddings encode the absolute position of the token in the sequence. This is useful because the model can learn to pay attention to tokens that are in certain positions. However, absolute positional embeddings are not as flexible as relative positional embeddings. It can be difficult to change the size of the context length of the model because it was fixed during the training.\nUsually, these embeddings are added to the token embeddings. \nLet’s create absolute positional embeddings for simplicity. GPT-2 also used this. We have token_ids and emb from before.\n\ntoken_ids\n\ntensor([2, 1, 0])\n\n\n\nemb\n\nEmbedding(3, 4)\n\n\nLet’s get token embeddings again.\n\ntoken_emb = emb(token_ids)\ntoken_emb\n\ntensor([[ 0.4617,  0.2674,  0.5349,  0.8094],\n        [-1.1229, -0.1863,  2.2082, -0.6380],\n        [ 0.3367,  0.1288,  0.2345,  0.2303]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\nPositional embedding is another embedding layer with the same output dimension as the token embedding layer. The input dimension is the context length. The context length is the maximum length of the sequence that the model can handle. In this case, we will use a context length of 3. And output size is 4. Since vocab size and output size are the same, we can use the same embedding layer for both token and positional embeddings.\n\nemb2 = torch.nn.Embedding(3, 4)\nemb2\n\nEmbedding(3, 4)\n\n\n\npos_emb = emb2(torch.arange(3))\npos_emb\n\ntensor([[-0.7658, -0.7506,  1.3525,  0.6863],\n        [-0.3278,  0.7950,  0.2815,  0.0562],\n        [ 0.5227, -0.2384, -0.0499,  0.5263]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\nFinally, we can get the input embedding for the model by adding the token embeddings and the positional embeddings.\n\ninp_emb = token_emb + pos_emb\ninp_emb\n\ntensor([[-0.3042, -0.4833,  1.8875,  1.4957],\n        [-1.4506,  0.6086,  2.4897, -0.5818],\n        [ 0.8594, -0.1095,  0.1846,  0.7567]], grad_fn=&lt;AddBackward0&gt;)\n\n\nTo learn more about positional embeddings, please refer to the following resources: - A blog on Rotary Position Encoding (ROPE) by Akash Nain: https://aakashkumarnain.github.io/posts/ml_dl_concepts/rope - Reformer paper: https://arxiv.org/pdf/2104.09864",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part2-Processing Text"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-23-bllms_2/index.html#byte-pair-encoding-bpe",
    "href": "blog/posts/2025-02-23-bllms_2/index.html#byte-pair-encoding-bpe",
    "title": "Building LLM part2-Processing Text",
    "section": "Byte pair encoding (BPE)",
    "text": "Byte pair encoding (BPE)\nByte pair encoding (BPE) is a data compression technique that is used to create a vocabulary of subword units. It was a bit confusing for me to understand this because I didn’t know about bytes, hexdecimal, ASCII, and UTF-8. We can just think of byte as a tiny thing that makes up a character. The algorithm is very simple. It works by iteratively merging the most frequent pair of bytes in the text. This process is repeated until the desired vocabulary size is reached. The resulting vocabulary consists of the most frequent subword units in the text.\nThe book does not cover details of BPE, but bpe-from-scratch is included in the github. This version focuses on education purposes and skips some steps, such as converting the text into bytes. To learn more about bpe, I recommend minbpe by Karpathy. The code has many comments and is easy to understand.\n\n\n\n\n\n\nNote\n\n\n\nWhat are bytes, hexadecimal, ASCII, and UTF-8? And what do they have to do with BPE?\nIt is not necessary to know those concepts to understand how BPE works in a big picture. However, I had a lot of fun learning about these. And it gives a bit more in depth understanding of BPE and computers.\nBriefly, a byte is eight bits, and each bit is a number consists of 0 or 1. For instance, “00000000” and “10101100” are bytes. There are 2**8 or 256 ways of creatinga byte. Instead of writing eight characters long for each byte, we can use hexadecimal to write two characters for each byte. In simple terms, ASCII is an old way to convert or convert back a byte into a character and only has characters on english keyboard, such as english alphabet, numbers, +, -, etc. UTF-8 is modern way that includes characters from other languages and emojis. Using hexadecimal is useful because UTF-8 uses multiple bytes.\n\n\n\nHow BPE works\nBriefly, this is how to train BPE:\n\nVocabulary is initialized with first 256 ASCII characters.\nText is converted into bytes.\nUntil vocabulary size is reached:\n\nCount the frequency of each pair of bytes in the text.\nMerge the most frequent pair of bytes into a single byte.\nAdd the new byte to the vocabulary.\n\n\nThat’s it. BPE is a simple and effective way to create a vocabulary of subword units. It is used in many NLP models, including GPT-2, GPT-3, and BERT. I was planning on explaining BPE in more detail, but I think it is better to leave it as an exercise for the reader. Maybe I will write a blog on it in the future with some information about bytes, hexadecimal digits, ASCII, UTF-8, and such. Of course it is not necessary to understand BPE, but they are related and are fun to learn about.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part2-Processing Text"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-12-live_coding1/index.html",
    "href": "blog/posts/2022-11-12-live_coding1/index.html",
    "title": "Live coding 1",
    "section": "",
    "text": "This is a blog based on Jeremy Howard’s Youtube video on Live coding 1. Here, we will learn how to use terminal, install python the right way, and install packages using conda/mamba.\n\n\n\n\n\n\nNoteRequired knowledge\n\n\n\nAnybody with basic python knowledge can follow along the Youtube video and/or this blog. I recommend you to follow along if you are not familiar with anything in this article.\n\n\n\n\n\n\n\n\nImportantSystem requirements\n\n\n\n\nComputers running Windows, Mac, and Linux can follow along. Chromebooks may work, but there may be many problems along the way.\nComputers do not have to have GPUs.\n\n\n\n\n\nWhat is a terminal? A terminal is a device for entering an input to a computer. We type commands here, and the computer will give us output. It looks like this on my computer:\n\n\n\nA terminal on my linux.\n\n\nHow to open a terminal? On Windows, you can download WSL (Windows Subsystem for Linux). This way, you are installing Linux distribution within Windows, and you can open it like any other app. On Mac or on Linux, you can search it on a search engine how to open it.\nOnce you open it, you will see something close to my terminal. Inside of this terminal, a shell is running. The difference between terminal and shell is not important for us, and we will use these terms interchangeably.\nWith a terminal, we can open up directories by typing commands, instead of clicking them in explorer or finder. Here, a directory is a container for files and directories. It’s also called a folder.\nLet’s try ls (list) command. Let’s type it in the terminal and press Enter.\n\n\n\nan output for ls command\n\n\nThe computer will output directories or files in the current directory. You may have different output because you have different directories or folders. Cool thing about using commands this way is that we can also provide options. Let’s try typing ls -a.\n\n\n\nAn output for ls -a command\n\n\nWhat happened? There are more stuff now than before. What does -a do? This reveals ‘hidden’ files. Hidden files are files starting with .(dot). There are many other options associated with commands, which make commands very powerful. However, it also means more things to memorize and learn about! You may be wondering what’s the point of learning about all these? As we will find out later, using commands gives us many benefits, such as command history. With history, we can automate commands easily or find out what went wrong.\nTry ls -ah. It also lists file/directory sizes. Options can be used together. It could also be executed as ls -a -h.\n\n\n\n\n\n\nNote\n\n\n\nAlthough terminal has a learning curve, it can be a very useful tool because it can boost productivity. Let’s go over some tips.\n\n\n\n\nHow can we learn more about different commands?\n\nSearch commands on a search engine: There are many resources online, but it may be a little tricky which one to trust.\nUse man command: man provides a manual for a command. It’s a simple and easy way to learn about a command.\n\nLet’s try using man itself.\n\n\n\nUsing man by itself.\n\n\nThat’s not very interesting because we did not provide what command we want to know about. We can find more about man itself!\n\n\n\nUsing man on man\n\n\n\n\n\nman man output\n\n\nIt lists name, synopsis, description, examples, and others for the command. We can press f (forward) to scroll down, b (backward) to scroll up, and q (quit) to quit. Try other commands yourself, like ls that we just used.\nYou may be overwhelmed by information it provides. Do not try to memorize every single option. There are just too many. You can use this if you forgot about something and quickly look it up. Or you can always search commands online.\n\n\n\npwd (print working directory) command prints out the current location. /home/kappa is same as ~, and we refer it as home directory. ~ right before $ in kappa@keepo:~$ also shows where I am located. It could have written kappa@keepo:/home/kappa$, which means the same thing.\n\n\n\npwd output\n\n\n\n\n\nmkdir (make directory) command makes a directory. We can execute mkdir new_directory, and it will be created. We can check it with ls command.\n\n\n\ncd (change directory) command changes directory. We can execute cd new_directory to move into that new_directory. After changing directory, try pwd. To move one level up from current directory, do cd ... .. is a parent directory.\n\n\n\n\n\n\nTipUsing Tab\n\n\n\nHere, instead of typing cd new_directory, I can type cd new_ and press Tab. Rest of the directory name is automatically finished. If there are many directories starting with new_, it will not automatically complete the directory name because the computer doesn’t know which one I want. I can press Tab one more time, and the computer outputs with differnt options. I can type more characters and press Tab again until it completes.\n\n\ncd can also be called by itself. It will change directory to home directory.\n\n\n\nThere are some other useful shortcuts.\n\n!mk: run command starting with mk from command history.\n!!: run last command.\nCtrl-r: reverse-i-search. Try typing commands that were used before.\nCtrl-a: Move the cursor to the beginning.\nCtrl-e: Move the cursor to the end.\nCtrl-left_arrow: Move left word by word.\nCtrl-right_arrow: Move right word by word.\n\nMake sure to try out the commands rather than just reading them over. Also, practice makes perfect!\n\n\n\n\nEven on new computers, there is python already installed. However, we do not want to use this because this is for our computers to run programs related to operating system. We want to keep system python from our python because we can debug more easily.\nSo, let’s learn how to install the right python. If you already have installed ipython, jupyter, pytorch, or anything like that, delete them. We will install from the scratch.\nIf you have conda installed, you can remove it by using rm -rf your_conda. Then, restart your terminal so that you do not have (base) in the beginning of the prompt.\n\n\n\nRemoving conda\n\n\nAlso, if you installed ipython or jupyter notebook using apt on Ubuntu or brew on Mac, uninstall them. When you type ipython or jupyter notebook, it should not work anymore.\nWe will use miniforge to install conda. In readme, we can find Download section.\n\n\n\nMiniforge download\n\n\nRight click on the one that matches with your OS and architecture and right click on it. Then select ‘Copy link address’.\n\n\n\nRight click on Download\n\n\nI am using Linux with amd64 architecture. Next, let’s head back into our terminal and type wget your_link in my Downloads directory. You can paste your link into terminal with Ctrl-Shift-V.\n\n\n\nUsing wget\n\n\nTry using ls command. We now see that we have a file starting with Miniforge3-whatever. Pay attention to the end of the file. It ends with .sh. This means we can execute it using bash. This file consists of commands we type into the shell, such as ls and pwd. You can take a peak at the file by using head Miniforge3-Linux-x86_64.sh. So, we can execute it to install Miniforge by typing bash Miniforge3-Linux-x86_64.sh.\n\n\n\nExecute the script\n\n\nIt will start a Miniforge installation process. We press Enter.\n\n\n\nMiniforge prompt1\n\n\nType yes to agree to the license terms.\n\n\n\nMiniforge prompt2\n\n\nThen we press Enter to confirm the location.\n\n\n\nMiniforge prompt3\n\n\nThen type yes to initialize conda.\n\n\n\nMiniforge prompt4\n\n\nNow, we restart our terminal. We see we have ‘(base)’ in front the prompt line. It means we have conda running!\n\n\n\nConda is running!\n\n\nTry typing which python. It should be python inside of miniforge.\n\n\n\nmamba’s python\n\n\nType conda, and we see something like a manual page for conda.\n\n\n\nconda help\n\n\nLet’s type mamba and see what we get. If you get the same output as conda, you are good. If you get this screen, we have some work to do.\n\n\n\nmamba not found\n\n\n\n\n\n\n\n\nImportantWhat is conda and mamba vs. pip?\n\n\n\nWe use conda to install python packages such as ipython, Jupyter notebook, and Pytorch. Mamba is just a faster version of conda. We can always replace mamba with conda for anything and get it done more quickly. Pip is also another way of installing packages, but not recommended because it does not install as smart as conda/mamba. It can cause headaches later on. So always install with conda/mamba.\n\n\nTo install mamba, type conda install mamba -n base -c conda-forge. If mamba works, you can skip this installation steps.\n\n\n\nInstalling mamba\n\n\nConda checks for dependencies and figures out what needs to be installed and updated. We proceed with y.\n\n\n\nInstalling mamba 2\n\n\nTry mamba. You should see this screen.\n\n\n\nMamba installed\n\n\n\n\n\nIt’s time to install Jupyterlab with mamba install jupyterlab.\n\n\n\nInstalling jupyterlab\n\n\n\n\n\nInstalling jupyterlab\n\n\n\n\n\nInstalling jupyterlab in prompt1\n\n\nPretty straight forward steps. mamba looks for dependencies, find out packages to update or install, and we confirm. Now we have Jupyterlab install. Let’s see what it does. Let’s try it with jupyter notebook.\n\n\n\nExecuting Jupyter notebook\n\n\nIt will automatically open up Jupyter notebook if you are on Mac or Linux. However, if you are on Windows, you have to hold Ctrl key and click on one of the links on the bottom of the output. To close Jupyter notebook, press Ctrl-C or Ctrl-D.\n\n\n\n\n\n\nTip\n\n\n\nIf you were thinking typing out jupyter notebook seems too long, there is a solution. Let’s shorten it to jl! Type alias jl=\"jupyter notebook\". Then, type jl. It is nice, but it will not work if we close the terminal and open it up later. So, we have to save it into .bashrc file in the home directory. Open the ~/.bashrc file and add alias jl=\"jupyter notebook\" at the end of the file. Now on, we can use jl shortcut even after restarting our terminal.\n\n\nWe see something like this on the browser.\n\n\n\nJupyter notebook on the browser.\n\n\nOn the top right, we can click New to create a new notebook or a folder. Create a Folder called nbs (for notebooks) and click on it to get inside of it.\n\n\n\nCreating a new file or folder in Jupyter\n\n\n\n\n\nInside of nbs folder\n\n\nNow, let’s create a new notebook. Click ‘New’ again and click Python 3 this time.\n\n\n\nJupyter notebook\n\n\nClick inside of a cell and type 1 + 1. Then press Shift + Enter. The cell will be executed and show output on the bottom.\n\n\n\nJupyter notebook 2\n\n\nWhen you are typing in the cell, you are in Edit Mode. You can press Esc key to get out of it, and you are in Command Mode. Different mode has different command or shortcut you can use. You can press h to learn about them.\n\n\n\nLast thing we will install is Pytorch. On the Pytorch page, we see this:\n\n\n\nInstall Pytorch\n\n\nYou can pick and choose your options, such as OS or compute platform. Then, we can copy the command and execute it in our terminal. Remember to use mamba in the place of conda to get it started faster.\n\n\n\nInstall Pytorch using mamba\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt takes a little while to install this time. In deep learning, there will be many times when we have to wait for the computer to execute programs, esepcially when training deep learning models. I recommend spending this time wisely. There are many things to do, such as reading ahead or practicing terminal commands. It doesn’t seem like a big deal, but it will make a huge difference later on.\n\n\n\n\n\nWe covered lots of materials in this blog. If you haven’t already, please watch Live coding 1 by Jeremy Howard because it will be very helpful. Also, practice terminal commands.\n\n\n\n\n\n\nTip\n\n\n\nWhen learning something, it is great to learn with different medium or context. For instance, watching videos, reading books, and getting hands on will all boost your skills. Try out all different ways and be creative!",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 1"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-12-live_coding1/index.html#terminal",
    "href": "blog/posts/2022-11-12-live_coding1/index.html#terminal",
    "title": "Live coding 1",
    "section": "",
    "text": "What is a terminal? A terminal is a device for entering an input to a computer. We type commands here, and the computer will give us output. It looks like this on my computer:\n\n\n\nA terminal on my linux.\n\n\nHow to open a terminal? On Windows, you can download WSL (Windows Subsystem for Linux). This way, you are installing Linux distribution within Windows, and you can open it like any other app. On Mac or on Linux, you can search it on a search engine how to open it.\nOnce you open it, you will see something close to my terminal. Inside of this terminal, a shell is running. The difference between terminal and shell is not important for us, and we will use these terms interchangeably.\nWith a terminal, we can open up directories by typing commands, instead of clicking them in explorer or finder. Here, a directory is a container for files and directories. It’s also called a folder.\nLet’s try ls (list) command. Let’s type it in the terminal and press Enter.\n\n\n\nan output for ls command\n\n\nThe computer will output directories or files in the current directory. You may have different output because you have different directories or folders. Cool thing about using commands this way is that we can also provide options. Let’s try typing ls -a.\n\n\n\nAn output for ls -a command\n\n\nWhat happened? There are more stuff now than before. What does -a do? This reveals ‘hidden’ files. Hidden files are files starting with .(dot). There are many other options associated with commands, which make commands very powerful. However, it also means more things to memorize and learn about! You may be wondering what’s the point of learning about all these? As we will find out later, using commands gives us many benefits, such as command history. With history, we can automate commands easily or find out what went wrong.\nTry ls -ah. It also lists file/directory sizes. Options can be used together. It could also be executed as ls -a -h.\n\n\n\n\n\n\nNote\n\n\n\nAlthough terminal has a learning curve, it can be a very useful tool because it can boost productivity. Let’s go over some tips.\n\n\n\n\nHow can we learn more about different commands?\n\nSearch commands on a search engine: There are many resources online, but it may be a little tricky which one to trust.\nUse man command: man provides a manual for a command. It’s a simple and easy way to learn about a command.\n\nLet’s try using man itself.\n\n\n\nUsing man by itself.\n\n\nThat’s not very interesting because we did not provide what command we want to know about. We can find more about man itself!\n\n\n\nUsing man on man\n\n\n\n\n\nman man output\n\n\nIt lists name, synopsis, description, examples, and others for the command. We can press f (forward) to scroll down, b (backward) to scroll up, and q (quit) to quit. Try other commands yourself, like ls that we just used.\nYou may be overwhelmed by information it provides. Do not try to memorize every single option. There are just too many. You can use this if you forgot about something and quickly look it up. Or you can always search commands online.\n\n\n\npwd (print working directory) command prints out the current location. /home/kappa is same as ~, and we refer it as home directory. ~ right before $ in kappa@keepo:~$ also shows where I am located. It could have written kappa@keepo:/home/kappa$, which means the same thing.\n\n\n\npwd output\n\n\n\n\n\nmkdir (make directory) command makes a directory. We can execute mkdir new_directory, and it will be created. We can check it with ls command.\n\n\n\ncd (change directory) command changes directory. We can execute cd new_directory to move into that new_directory. After changing directory, try pwd. To move one level up from current directory, do cd ... .. is a parent directory.\n\n\n\n\n\n\nTipUsing Tab\n\n\n\nHere, instead of typing cd new_directory, I can type cd new_ and press Tab. Rest of the directory name is automatically finished. If there are many directories starting with new_, it will not automatically complete the directory name because the computer doesn’t know which one I want. I can press Tab one more time, and the computer outputs with differnt options. I can type more characters and press Tab again until it completes.\n\n\ncd can also be called by itself. It will change directory to home directory.\n\n\n\nThere are some other useful shortcuts.\n\n!mk: run command starting with mk from command history.\n!!: run last command.\nCtrl-r: reverse-i-search. Try typing commands that were used before.\nCtrl-a: Move the cursor to the beginning.\nCtrl-e: Move the cursor to the end.\nCtrl-left_arrow: Move left word by word.\nCtrl-right_arrow: Move right word by word.\n\nMake sure to try out the commands rather than just reading them over. Also, practice makes perfect!",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 1"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-12-live_coding1/index.html#installing-python",
    "href": "blog/posts/2022-11-12-live_coding1/index.html#installing-python",
    "title": "Live coding 1",
    "section": "",
    "text": "Even on new computers, there is python already installed. However, we do not want to use this because this is for our computers to run programs related to operating system. We want to keep system python from our python because we can debug more easily.\nSo, let’s learn how to install the right python. If you already have installed ipython, jupyter, pytorch, or anything like that, delete them. We will install from the scratch.\nIf you have conda installed, you can remove it by using rm -rf your_conda. Then, restart your terminal so that you do not have (base) in the beginning of the prompt.\n\n\n\nRemoving conda\n\n\nAlso, if you installed ipython or jupyter notebook using apt on Ubuntu or brew on Mac, uninstall them. When you type ipython or jupyter notebook, it should not work anymore.\nWe will use miniforge to install conda. In readme, we can find Download section.\n\n\n\nMiniforge download\n\n\nRight click on the one that matches with your OS and architecture and right click on it. Then select ‘Copy link address’.\n\n\n\nRight click on Download\n\n\nI am using Linux with amd64 architecture. Next, let’s head back into our terminal and type wget your_link in my Downloads directory. You can paste your link into terminal with Ctrl-Shift-V.\n\n\n\nUsing wget\n\n\nTry using ls command. We now see that we have a file starting with Miniforge3-whatever. Pay attention to the end of the file. It ends with .sh. This means we can execute it using bash. This file consists of commands we type into the shell, such as ls and pwd. You can take a peak at the file by using head Miniforge3-Linux-x86_64.sh. So, we can execute it to install Miniforge by typing bash Miniforge3-Linux-x86_64.sh.\n\n\n\nExecute the script\n\n\nIt will start a Miniforge installation process. We press Enter.\n\n\n\nMiniforge prompt1\n\n\nType yes to agree to the license terms.\n\n\n\nMiniforge prompt2\n\n\nThen we press Enter to confirm the location.\n\n\n\nMiniforge prompt3\n\n\nThen type yes to initialize conda.\n\n\n\nMiniforge prompt4\n\n\nNow, we restart our terminal. We see we have ‘(base)’ in front the prompt line. It means we have conda running!\n\n\n\nConda is running!\n\n\nTry typing which python. It should be python inside of miniforge.\n\n\n\nmamba’s python\n\n\nType conda, and we see something like a manual page for conda.\n\n\n\nconda help\n\n\nLet’s type mamba and see what we get. If you get the same output as conda, you are good. If you get this screen, we have some work to do.\n\n\n\nmamba not found\n\n\n\n\n\n\n\n\nImportantWhat is conda and mamba vs. pip?\n\n\n\nWe use conda to install python packages such as ipython, Jupyter notebook, and Pytorch. Mamba is just a faster version of conda. We can always replace mamba with conda for anything and get it done more quickly. Pip is also another way of installing packages, but not recommended because it does not install as smart as conda/mamba. It can cause headaches later on. So always install with conda/mamba.\n\n\nTo install mamba, type conda install mamba -n base -c conda-forge. If mamba works, you can skip this installation steps.\n\n\n\nInstalling mamba\n\n\nConda checks for dependencies and figures out what needs to be installed and updated. We proceed with y.\n\n\n\nInstalling mamba 2\n\n\nTry mamba. You should see this screen.\n\n\n\nMamba installed",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 1"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-12-live_coding1/index.html#installing-jupyterlab",
    "href": "blog/posts/2022-11-12-live_coding1/index.html#installing-jupyterlab",
    "title": "Live coding 1",
    "section": "",
    "text": "It’s time to install Jupyterlab with mamba install jupyterlab.\n\n\n\nInstalling jupyterlab\n\n\n\n\n\nInstalling jupyterlab\n\n\n\n\n\nInstalling jupyterlab in prompt1\n\n\nPretty straight forward steps. mamba looks for dependencies, find out packages to update or install, and we confirm. Now we have Jupyterlab install. Let’s see what it does. Let’s try it with jupyter notebook.\n\n\n\nExecuting Jupyter notebook\n\n\nIt will automatically open up Jupyter notebook if you are on Mac or Linux. However, if you are on Windows, you have to hold Ctrl key and click on one of the links on the bottom of the output. To close Jupyter notebook, press Ctrl-C or Ctrl-D.\n\n\n\n\n\n\nTip\n\n\n\nIf you were thinking typing out jupyter notebook seems too long, there is a solution. Let’s shorten it to jl! Type alias jl=\"jupyter notebook\". Then, type jl. It is nice, but it will not work if we close the terminal and open it up later. So, we have to save it into .bashrc file in the home directory. Open the ~/.bashrc file and add alias jl=\"jupyter notebook\" at the end of the file. Now on, we can use jl shortcut even after restarting our terminal.\n\n\nWe see something like this on the browser.\n\n\n\nJupyter notebook on the browser.\n\n\nOn the top right, we can click New to create a new notebook or a folder. Create a Folder called nbs (for notebooks) and click on it to get inside of it.\n\n\n\nCreating a new file or folder in Jupyter\n\n\n\n\n\nInside of nbs folder\n\n\nNow, let’s create a new notebook. Click ‘New’ again and click Python 3 this time.\n\n\n\nJupyter notebook\n\n\nClick inside of a cell and type 1 + 1. Then press Shift + Enter. The cell will be executed and show output on the bottom.\n\n\n\nJupyter notebook 2\n\n\nWhen you are typing in the cell, you are in Edit Mode. You can press Esc key to get out of it, and you are in Command Mode. Different mode has different command or shortcut you can use. You can press h to learn about them.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 1"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-12-live_coding1/index.html#installing-pytorch",
    "href": "blog/posts/2022-11-12-live_coding1/index.html#installing-pytorch",
    "title": "Live coding 1",
    "section": "",
    "text": "Last thing we will install is Pytorch. On the Pytorch page, we see this:\n\n\n\nInstall Pytorch\n\n\nYou can pick and choose your options, such as OS or compute platform. Then, we can copy the command and execute it in our terminal. Remember to use mamba in the place of conda to get it started faster.\n\n\n\nInstall Pytorch using mamba\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt takes a little while to install this time. In deep learning, there will be many times when we have to wait for the computer to execute programs, esepcially when training deep learning models. I recommend spending this time wisely. There are many things to do, such as reading ahead or practicing terminal commands. It doesn’t seem like a big deal, but it will make a huge difference later on.",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 1"
    ]
  },
  {
    "objectID": "blog/posts/2022-11-12-live_coding1/index.html#conclusion",
    "href": "blog/posts/2022-11-12-live_coding1/index.html#conclusion",
    "title": "Live coding 1",
    "section": "",
    "text": "We covered lots of materials in this blog. If you haven’t already, please watch Live coding 1 by Jeremy Howard because it will be very helpful. Also, practice terminal commands.\n\n\n\n\n\n\nTip\n\n\n\nWhen learning something, it is great to learn with different medium or context. For instance, watching videos, reading books, and getting hands on will all boost your skills. Try out all different ways and be creative!",
    "crumbs": [
      "Home",
      "Blog",
      "Live coding 1"
    ]
  },
  {
    "objectID": "blog/posts/2025-07-24-RoPE/index.html",
    "href": "blog/posts/2025-07-24-RoPE/index.html",
    "title": "RoPE",
    "section": "",
    "text": "In order to let large language models (LLMs) know the order of the tokens, we use positional embeddings. There are absolute positional embedding and relative positional embeding. Today, we will talk about relative positional embedding with Rotary Position Embedding (RoPE) from RoFormer: Enhanced Transformer with Rotary Position Embedding by Su et al.\n\n\nThere are two ways to implement RoPE. One from Llama and another from Hugging Face.\n\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple\n\n\nbatch_size = 2\ncontext_len = 3\nnum_heads = 4\nhead_dim = 16\n\n\ntorch.manual_seed(123)\nqueries = torch.randn(batch_size, context_len, num_heads, head_dim)\nkeys = torch.randn(batch_size, context_len, num_heads, head_dim)\nqueries.shape, keys.shape\n\n(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]))\n\n\n\n\nBelow is the code from Llama 2 implementation. Let’s try to understand it.\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    \"\"\"\n    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n    The returned tensor contains complex values in complex64 data type.\n\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        end (int): End index for precomputing frequencies.\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n\n    Returns:\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n    \"\"\"\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device)  # type: ignore\n    freqs = torch.outer(t, freqs).float()  # type: ignore\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis\n\nLet’s go through code step by step. First, we need the arguments to pass.\n\ndim = head_dim\nend = context_len\ntheta = 10_000\n\nWe want to create a frequency tensor. We are creating a dim // 2 tensor because we get cos and sin from each frequency. And these are applied to each column as we will see.\n\ntorch.arange(0, dim, 2)[: (dim // 2)].float()\n\ntensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14.])\n\n\nThen we normalize by dim.\n\n(torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)\n\ntensor([0.0000, 0.1250, 0.2500, 0.3750, 0.5000, 0.6250, 0.7500, 0.8750])\n\n\n\ntheta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)\n\ntensor([1.0000e+00, 3.1623e+00, 1.0000e+01, 3.1623e+01, 1.0000e+02, 3.1623e+02,\n        1.0000e+03, 3.1623e+03])\n\n\n\n1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n\ntensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n        1.0000e-03, 3.1623e-04])\n\n\n\nfreqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\nfreqs\n\ntensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n        1.0000e-03, 3.1623e-04])\n\n\n\nHigh frequencies (like 1.0) rotate quickly → capture short-range positional relationships\nLow frequencies (like 3.1623e-04 = 0.00031623) rotate slowly → capture long-range positional relationships\n\nThink of it like a clock: - A second hand (high frequency) completes many rotations quickly - good for distinguishing nearby moments - An hour hand (low frequency) rotates slowly - good for distinguishing longer time periods\nIn RoPE, the high frequency dimensions help the model distinguish between adjacent tokens, while the low frequency dimensions help distinguish between tokens that are far apart in the sequence.\n\nt = torch.arange(end, device=freqs.device)\nt\n\ntensor([0, 1, 2])\n\n\n\nfreqs = torch.outer(t, freqs).float()\nfreqs.shape, freqs\n\n(torch.Size([3, 8]),\n tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00],\n         [1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n          1.0000e-03, 3.1623e-04],\n         [2.0000e+00, 6.3246e-01, 2.0000e-01, 6.3246e-02, 2.0000e-02, 6.3246e-03,\n          2.0000e-03, 6.3246e-04]]))\n\n\n\nLower dimension indices → higher frequencies → faster rotation\nHigher dimension indices → lower frequencies → slower rotation\n\n\nfreqs_cis = torch.polar(torch.ones_like(freqs), freqs)\nfreqs_cis.shape, freqs_cis\n\n(torch.Size([3, 8]),\n tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n           1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n           1.0000+0.0000e+00j,  1.0000+0.0000e+00j],\n         [ 0.5403+8.4147e-01j,  0.9504+3.1098e-01j,  0.9950+9.9833e-02j,\n           0.9995+3.1618e-02j,  0.9999+9.9998e-03j,  1.0000+3.1623e-03j,\n           1.0000+1.0000e-03j,  1.0000+3.1623e-04j],\n         [-0.4161+9.0930e-01j,  0.8066+5.9113e-01j,  0.9801+1.9867e-01j,\n           0.9980+6.3203e-02j,  0.9998+1.9999e-02j,  1.0000+6.3245e-03j,\n           1.0000+2.0000e-03j,  1.0000+6.3246e-04j]]))\n\n\n\n\n\nFigure 2: Long-term decay of RoPE\n\n\nImage from the RoPE paper.\n\ndef apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply rotary embeddings to input tensors using the given frequency tensor.\n\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n    returned as real tensors.\n\n    Args:\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n    \"\"\"\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\nqueries and keys have shape (batch_size, context_len, num_heads, head_dim).\n\nxq = queries\nxk = keys\n\nxq.shape, xk.shape, freqs_cis.shape\n\n(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]), torch.Size([3, 8]))\n\n\n\nxq.shape[:-1]\n\ntorch.Size([2, 3, 4])\n\n\nExpand last dimension of query (head_dim), 16, into 2. One for cos, the other for sin.\n\nxq.float().reshape(*xq.shape[:-1], -1, 2).shape\n\ntorch.Size([2, 3, 4, 8, 2])\n\n\n\nxq[0,0,0]\n\ntensor([ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n         0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692])\n\n\n\nxq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\nxq_.shape\n\ntorch.Size([2, 3, 4, 8])\n\n\n\nxq_[0,0,0]\n\ntensor([ 0.3374-0.1778j, -0.3035-0.5880j,  0.3486+0.6603j, -0.2196-0.3792j,\n         0.7671-1.1925j,  0.6984-1.4097j,  0.1794+1.8951j,  0.4954+0.2692j])\n\n\nThen, reshape_for_broadcast happens.\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    \"\"\"\n    Reshape frequency tensor for broadcasting it with another tensor.\n\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\n\n    Args:\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\n\n    Returns:\n        torch.Tensor: Reshaped frequency tensor.\n\n    Raises:\n        AssertionError: If the frequency tensor doesn't match the expected shape.\n        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n    \"\"\"\n    ndim = x.ndim\n    assert 0 &lt;= 1 &lt; ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(*shape)\n\nLet’s remind ourselves of the shapes.\n\nfreqs_cis.shape, queries.shape\n\n(torch.Size([3, 8]), torch.Size([2, 3, 4, 16]))\n\n\nRembmer queries and keys have shape: (batch_size, context_len, num_heads, head_dim). reshape_for_broadcast is used after queries and keys are transformed into complex space according to apply_rotary_emb. We want freqs_cis to be (1, context_len, 1, head_dim). Let’s use queries as x.\n\nx = xq_\nx.shape\n\ntorch.Size([2, 3, 4, 8])\n\n\n\nndim = x.ndim\nndim\n\n4\n\n\n\nassert 0 &lt;= 1 &lt; ndim\nassert freqs_cis.shape == (x.shape[1], x.shape[-1])\n\n\nshape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\nshape\n\n[1, 3, 1, 8]\n\n\nGoing back to apply_rotary_emb.\n\nfreqs_cis = reshape_for_broadcast(freqs_cis, xq_)\nfreqs_cis.shape\n\ntorch.Size([1, 3, 1, 8])\n\n\n\n(xq_ * freqs_cis).shape\n\ntorch.Size([2, 3, 4, 8])\n\n\n\nxq_[0,1,0]\n\ntensor([ 0.5146+0.9938j, -0.2587-1.0826j, -0.0444+1.6236j, -2.3229+1.0878j,\n         0.6716+0.6933j, -0.9487-0.0765j, -0.1526+0.1167j,  0.4403-1.4465j])\n\n\n\nfreqs_cis[0,1,0]\n\ntensor([0.5403+8.4147e-01j, 0.9504+3.1098e-01j, 0.9950+9.9833e-02j,\n        0.9995+3.1618e-02j, 0.9999+9.9998e-03j, 1.0000+3.1623e-03j,\n        1.0000+1.0000e-03j, 1.0000+3.1623e-04j])\n\n\n\n(xq_ * freqs_cis)[0,1,0]\n\ntensor([-0.5582+0.9700j,  0.0908-1.1093j, -0.2062+1.6110j, -2.3561+1.0138j,\n         0.6646+0.7000j, -0.9485-0.0795j, -0.1528+0.1166j,  0.4407-1.4464j])\n\n\n\n# Under the hood\n0.5146*0.5403-0.9938*8.4147e-01, 0.5146*8.4147e-01+0.9938*0.5403\n\n(-0.5582145060000001, 0.969970602)\n\n\nThe multiplication xq_ * freqs_cis is complex number multiplication, which performs the rotation.\nWhen you multiply two complex numbers:\n(a + bi) * (cos(θ) + i*sin(θ)) = (a*cos(θ) - b*sin(θ)) + i*(a*sin(θ) + b*cos(θ))\nThis is exactly the 2D rotation formula! So each complex number in xq_ gets rotated by the corresponding angle in freqs_cis.\nEach position gets its own rotation angles, and each dimension pair gets its own frequency, but all heads share the same positional encoding.\nThis is what the transformation looks like.\n\n(xq_ * freqs_cis)[0,0]\n\ntensor([[ 0.3374-0.1778j, -0.3035-0.5880j,  0.3486+0.6603j, -0.2196-0.3792j,\n          0.7671-1.1925j,  0.6984-1.4097j,  0.1794+1.8951j,  0.4954+0.2692j],\n        [-0.0770-1.0205j, -0.1690+0.9178j,  1.5810+1.3010j,  1.2753-0.2010j,\n          0.4965-1.5723j,  0.9666-1.1481j, -1.1589+0.3255j, -0.6315-2.8400j],\n        [-1.3250+0.1784j, -2.1338+1.0524j, -0.3885-0.9343j, -0.4991-1.0867j,\n          0.8805+1.5542j,  0.6266-0.1755j,  0.0983-0.0935j,  0.2662-0.5850j],\n        [ 0.8768+1.6221j, -1.4779+1.1331j, -1.2203+1.3139j,  1.0533+0.1388j,\n          2.2473-0.8036j, -0.2808+0.7697j, -0.6596-0.7979j,  0.1838+0.2293j]])\n\n\n\n\n\nFigure 1 from Paper\n\n\nImage from the RoPE paper.\nWhen we use torch.view_as_real,\n\ntorch.view_as_real(xq_ * freqs_cis)[0,0]\n\ntensor([[[ 0.3374, -0.1778],\n         [-0.3035, -0.5880],\n         [ 0.3486,  0.6603],\n         [-0.2196, -0.3792],\n         [ 0.7671, -1.1925],\n         [ 0.6984, -1.4097],\n         [ 0.1794,  1.8951],\n         [ 0.4954,  0.2692]],\n\n        [[-0.0770, -1.0205],\n         [-0.1690,  0.9178],\n         [ 1.5810,  1.3010],\n         [ 1.2753, -0.2010],\n         [ 0.4965, -1.5723],\n         [ 0.9666, -1.1481],\n         [-1.1589,  0.3255],\n         [-0.6315, -2.8400]],\n\n        [[-1.3250,  0.1784],\n         [-2.1338,  1.0524],\n         [-0.3885, -0.9343],\n         [-0.4991, -1.0867],\n         [ 0.8805,  1.5542],\n         [ 0.6266, -0.1755],\n         [ 0.0983, -0.0935],\n         [ 0.2662, -0.5850]],\n\n        [[ 0.8768,  1.6221],\n         [-1.4779,  1.1331],\n         [-1.2203,  1.3139],\n         [ 1.0533,  0.1388],\n         [ 2.2473, -0.8036],\n         [-0.2808,  0.7697],\n         [-0.6596, -0.7979],\n         [ 0.1838,  0.2293]]])\n\n\n\nxq_out[0,0]\n\ntensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n\n\n\nxq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\nxq_out.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\n\n\n\ntorchtune’s RotaryPositionalEmbeddings follows llama’s implementation.\n\nfrom torchtune.modules import RotaryPositionalEmbeddings\n\nWARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n\n\n\ntt_rope_emb = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=context_len, base=theta)\ntt_rope_emb\n\nRotaryPositionalEmbeddings()\n\n\n\ntt_queries_rot = tt_rope_emb(queries)\ntt_queries_rot.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\nThis gives us the same result as llama implementation.\n\nassert torch.allclose(tt_queries_rot, xq_out)\n\n\n\n\n\n\n# LitGPT code function `litgpt_build_rope_cache` from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py\n# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE\ndef litgpt_build_rope_cache(\n    seq_len: int,\n    n_elem: int,\n    device: Optional[torch.device] = None,\n    base: int = 10000,\n    condense_ratio: int = 1,\n    extra_config: Optional[dict] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Enhanced Transformer with Rotary Position Embedding.\n\n    Args:\n        seq_len (int): Sequence length.\n        n_elem (int): Number of elements (head dimension).\n        device (torch.device, optional): Device for tensor allocations.\n        base (int, optional): Base for computing inverse frequencies.\n        condense_ratio (int, optional): Ratio to condense the position indices.\n        extra_config (dict, optional): Configuration parameters for frequency adjustments (used by Llama 3.1 and 3.2)\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Cosine and sine caches for RoPE.\n    \"\"\"\n    # Compute the inverse frequencies theta\n    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, device=device).float() / n_elem))\n\n    if extra_config is not None:\n        orig_context_len = extra_config[\"original_max_seq_len\"]\n        factor = extra_config[\"factor\"]\n        low_freq_factor = extra_config[\"low_freq_factor\"]\n        high_freq_factor = extra_config[\"high_freq_factor\"]\n\n        wavelen = 2 * torch.pi / theta\n        ratio = orig_context_len / wavelen\n        smooth_factor = (ratio - low_freq_factor) / (high_freq_factor - low_freq_factor)\n        smooth_factor = torch.clamp(smooth_factor, min=0.0, max=1.0)\n\n        # Compute adjusted_theta without masked indexing\n        adjusted_theta = (1 - smooth_factor) * (theta / factor) + smooth_factor * theta\n        theta = adjusted_theta\n\n    # Create position indices `[0, 1, ..., seq_len - 1]`\n    seq_idx = torch.arange(seq_len, device=device) / condense_ratio\n\n    # Calculate the product of position index and $\\theta_i$\n    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n\n    return torch.cos(idx_theta), torch.sin(idx_theta)\n\n\nlitgpt_cos, litgpt_sin = litgpt_build_rope_cache(context_len, n_elem=head_dim, base=theta)\nlitgpt_cos.shape, litgpt_sin.shape\n\n(torch.Size([3, 16]), torch.Size([3, 16]))\n\n\n\nfreqs_cis = precompute_freqs_cis(dim=head_dim, end=context_len, theta=theta)\nfreqs_cis.shape\n\ntorch.Size([3, 8])\n\n\n\nlitgpt_cos[1], litgpt_sin[1]\n\n(tensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000, 0.5403,\n         0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000]),\n tensor([8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n         1.0000e-03, 3.1623e-04, 8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02,\n         9.9998e-03, 3.1623e-03, 1.0000e-03, 3.1623e-04]))\n\n\n\nfreqs_cis[1]\n\ntensor([0.5403+8.4147e-01j, 0.9504+3.1098e-01j, 0.9950+9.9833e-02j,\n        0.9995+3.1618e-02j, 0.9999+9.9998e-03j, 1.0000+3.1623e-03j,\n        1.0000+1.0000e-03j, 1.0000+3.1623e-04j])\n\n\n\ntorch.cos(freqs)[1]\n\ntensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000])\n\n\n\ntorch.cos(freqs_cis)[1]\n\ntensor([1.1795-4.8577e-01j, 0.6097-2.5713e-01j, 0.5472-8.3876e-02j,\n        0.5410-2.6601e-02j, 0.5404-8.4144e-03j, 0.5403-2.6610e-03j,\n        0.5403-8.4147e-04j, 0.5403-2.6610e-04j])\n\n\n\ntorch.stack([litgpt_cos, litgpt_sin], dim=-1).shape\n\ntorch.Size([3, 16, 2])\n\n\n\ntorch.view_as_real(freqs_cis).shape, torch.view_as_real(freqs_cis)[1]\n\n(torch.Size([3, 8, 2]),\n tensor([[5.4030e-01, 8.4147e-01],\n         [9.5042e-01, 3.1098e-01],\n         [9.9500e-01, 9.9833e-02],\n         [9.9950e-01, 3.1618e-02],\n         [9.9995e-01, 9.9998e-03],\n         [9.9999e-01, 3.1623e-03],\n         [1.0000e+00, 1.0000e-03],\n         [1.0000e+00, 3.1623e-04]]))\n\n\n\ntorch.view_as_real(freqs_cis)[..., 0][1]\n\ntensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000])\n\n\n\nassert torch.allclose(litgpt_cos, torch.view_as_real(freqs_cis)[..., 0].repeat(1,2))\n\n\n# LitGPT code from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py\n# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE\ndef litgpt_apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -&gt; torch.Tensor:\n    head_size = x.size(-1)\n    a = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n    b = x[..., head_size // 2:]  # (B, nh, T, hs/2)\n    rotated = torch.cat((-b, a), dim=-1)  # (B, nh, T, hs)\n    if cos.dim() &gt; 1:\n        # batch dimensions must align\n        # sin/cos are (B, T, hs) so we unsqeeze -3 for nh\n        # we count from back because all of apply_rope does\n        cos = cos.unsqueeze(-3)\n        sin = sin.unsqueeze(-3)\n\n    roped = (x * cos) + (rotated * sin)\n    return roped.to(dtype=x.dtype)\n\n(a + bi) * (cos(θ) + i*sin(θ)) = (acos(θ) - bsin(θ)) + i(asin(θ) + bcos(θ))\n(acos(θ) - bsin(θ)) + i(asin(θ) + bcos(θ)) = (acos(θ)+ibcos(θ)) + (-bsin(θ)+iasin(θ))\n\nqueries.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\n\nassert torch.allclose(tt_queries_rot, freqs_cis)\n\n\n\n\nHugging Face modeling_llama.\n\nqueires_t = queries.transpose(1,2)\nkeys_t = keys.transpose(1,2)\nqueires_t.shape, keys_t.shape\n\n(torch.Size([2, 4, 3, 16]), torch.Size([2, 4, 3, 16]))\n\n\n\nqueries\n\ntensor([[[[ 3.3737e-01, -1.7778e-01, -3.0353e-01, -5.8801e-01,  3.4861e-01,\n            6.6034e-01, -2.1964e-01, -3.7917e-01,  7.6711e-01, -1.1925e+00,\n            6.9835e-01, -1.4097e+00,  1.7938e-01,  1.8951e+00,  4.9545e-01,\n            2.6920e-01],\n          [-7.7020e-02, -1.0205e+00, -1.6896e-01,  9.1776e-01,  1.5810e+00,\n            1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n            9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n           -2.8400e+00],\n          [-1.3250e+00,  1.7843e-01, -2.1338e+00,  1.0524e+00, -3.8848e-01,\n           -9.3435e-01, -4.9914e-01, -1.0867e+00,  8.8054e-01,  1.5542e+00,\n            6.2662e-01, -1.7549e-01,  9.8284e-02, -9.3507e-02,  2.6621e-01,\n           -5.8504e-01],\n          [ 8.7684e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n            1.3139e+00,  1.0533e+00,  1.3881e-01,  2.2473e+00, -8.0364e-01,\n           -2.8084e-01,  7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n            2.2935e-01]],\n\n         [[ 5.1463e-01,  9.9376e-01, -2.5873e-01, -1.0826e+00, -4.4382e-02,\n            1.6236e+00, -2.3229e+00,  1.0878e+00,  6.7155e-01,  6.9330e-01,\n           -9.4872e-01, -7.6507e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n           -1.4465e+00],\n          [ 2.5529e-01, -5.4963e-01,  1.0042e+00,  8.2723e-01, -3.9481e-01,\n            4.8923e-01, -2.1681e-01, -1.7472e+00, -1.6025e+00, -1.0764e+00,\n            9.0315e-01, -7.2184e-01, -5.9508e-01, -7.1122e-01,  6.2296e-01,\n           -1.3729e+00],\n          [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1861e-02,\n           -4.7896e-01,  7.6681e-01,  2.7468e-02,  1.9929e+00,  1.3708e+00,\n           -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3745e-03, -9.8955e-01,\n            7.0161e-01],\n          [-9.4053e-01, -4.6806e-01,  1.0322e+00, -2.8300e-01,  4.9275e-01,\n           -1.4078e-02, -2.7466e-01, -7.6409e-01,  1.3966e+00, -9.9491e-01,\n           -1.5822e-03,  1.2471e+00, -7.7105e-02,  1.2774e+00, -1.4596e+00,\n           -2.1595e+00]],\n\n         [[-2.5822e-01, -2.0407e+00, -8.0156e-01, -8.1830e-01, -1.1820e+00,\n           -2.8774e-01, -6.0430e-01,  6.0024e-01, -1.4053e+00, -5.9217e-01,\n           -2.5479e-01,  1.1517e+00, -1.7858e-02,  4.2640e-01, -7.6574e-01,\n           -5.4514e-02],\n          [-1.2743e+00,  4.5128e-01, -2.2801e-01,  9.2238e-01,  2.0561e-01,\n           -4.9696e-01,  5.8206e-01,  2.0532e-01, -3.0177e-01, -6.7030e-01,\n           -6.1710e-01, -8.3339e-01,  4.8387e-01, -1.3493e-01,  2.1187e-01,\n           -8.7140e-01],\n          [ 6.8508e-01,  2.0024e+00, -5.4688e-01,  1.6014e+00, -2.2577e+00,\n           -1.8009e+00,  7.0147e-01,  5.7028e-01, -1.1766e+00, -2.0524e+00,\n            1.1318e-01,  1.4353e+00,  8.8307e-02, -1.2037e+00,  1.0964e+00,\n            2.4210e+00],\n          [ 1.5382e-01, -4.4516e-01,  5.5035e-01,  6.5788e-02,  6.8050e-01,\n            1.2064e+00,  1.6250e+00,  3.4595e-01,  1.3425e-01,  7.6623e-01,\n            2.2760e+00, -1.3255e+00, -8.9702e-01,  1.1318e-01,  8.3647e-01,\n            2.8520e-02]]],\n\n\n        [[[-9.7969e-01, -2.1126e+00, -2.7214e-01, -3.5100e-01,  1.1152e+00,\n           -6.1722e-01, -2.2708e+00, -1.3819e+00,  1.1721e+00, -4.3716e-01,\n           -4.0527e-01,  7.0864e-01,  9.5331e-01, -1.3035e-02, -1.3009e-01,\n           -8.7660e-02],\n          [-6.7349e-02,  2.4674e-01, -9.3917e-01, -1.0448e+00,  1.2783e+00,\n            4.1903e-01, -5.0727e-01, -6.0623e-01, -1.0532e+00,  1.8386e+00,\n           -1.0954e-01, -3.3161e-01,  9.0084e-01,  4.8398e-01, -1.3237e+00,\n            7.8692e-01],\n          [ 1.3818e+00, -6.9367e-02, -7.6117e-01,  2.4163e-01, -5.8781e-01,\n           -1.1506e+00,  1.0164e+00,  1.2343e-01,  1.1311e+00, -8.5805e-02,\n           -5.9727e-02,  3.5527e-01, -1.4355e+00,  7.2748e-02,  1.0528e-01,\n           -1.0311e+00],\n          [ 1.3113e+00, -3.5963e-02,  2.1181e-01, -8.6248e-03,  1.8576e+00,\n            2.1321e+00, -5.0561e-01, -7.9884e-01, -1.0944e+00, -1.0197e+00,\n           -5.3986e-01,  1.2117e+00, -8.6321e-01,  1.3337e+00,  7.7101e-02,\n           -5.2181e-02]],\n\n         [[ 2.3862e-01,  1.4106e-01, -1.3354e+00, -2.9340e+00,  1.1411e-01,\n           -1.2072e+00, -3.0083e-01,  1.4274e-01, -1.3027e+00, -4.9187e-01,\n           -2.1429e+00,  9.4881e-01, -5.6842e-01, -6.4643e-02,  6.6467e-01,\n           -2.7836e+00],\n          [ 1.1366e+00,  9.0886e-01,  9.4943e-01,  2.6565e-02, -9.2207e-01,\n            7.0338e-01, -3.6590e-01, -1.9654e-01, -9.2071e-01,  3.1535e-01,\n           -2.1734e-02,  3.4414e-01,  2.2710e-01, -4.5969e-01, -6.1831e-01,\n            2.4612e-01],\n          [-4.0549e-01, -8.3681e-01,  1.2277e+00, -4.2971e-01, -2.2121e+00,\n           -3.7802e-01,  9.8382e-01, -1.0895e+00,  2.0171e-01,  2.2145e-02,\n           -1.7753e+00, -7.4896e-01,  2.7808e-01, -9.6208e-01, -4.2228e-01,\n           -1.1036e+00],\n          [ 2.4727e-01,  1.4549e+00, -2.8351e-01, -3.7675e-01, -3.0577e-02,\n           -8.9448e-02, -1.9652e-01, -9.7133e-01,  9.0046e-01, -2.5233e-01,\n            1.0669e+00, -2.9846e-01,  8.5576e-01,  1.6098e+00, -1.1893e+00,\n            1.1677e+00]],\n\n         [[ 3.2765e-01, -8.3307e-01, -1.6179e+00,  2.2651e-01, -4.3815e-01,\n            3.2652e-01, -1.5786e+00, -1.3995e+00,  5.4460e-01, -8.3004e-02,\n           -1.1753e+00,  1.7825e+00,  1.7524e+00, -2.1347e-01,  4.0949e-01,\n            4.6454e-02],\n          [ 6.3669e-01, -1.9433e-01, -8.6139e-01,  5.3384e-01,  9.3758e-01,\n           -9.2248e-01,  7.0466e-01, -2.7221e-01,  1.4419e-02, -6.4115e-01,\n            2.3902e+00, -1.4256e+00, -4.6192e-01, -1.5539e+00, -3.3382e-01,\n            2.4049e-01],\n          [ 2.1065e+00,  5.5087e-01, -2.9364e-01, -1.8027e+00, -6.9333e-01,\n            1.7409e+00,  2.6979e-01,  9.5949e-01, -1.0253e+00, -5.5049e-01,\n            1.0264e+00, -5.6696e-01, -2.6584e-01, -1.1116e+00, -1.3696e+00,\n           -6.5336e-01],\n          [-1.6125e+00, -2.2840e-01,  1.8388e+00, -9.4727e-01,  1.4192e-01,\n            3.6959e-01, -1.7425e-02, -9.5746e-01, -8.1691e-01, -2.8655e-01,\n            4.3434e-01, -1.3402e-01, -2.1467e+00, -1.7984e+00, -6.8222e-01,\n           -5.1905e-01]]]])\n\n\n\ndef permute(w, n_heads, dim1=dim, dim2=dim):\n    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\n\nqueries.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\n\nqueries[0,0]\n\ntensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n\n\n\nqueries.transpose(2,3).shape\n\ntorch.Size([2, 3, 16, 4])\n\n\n\nqueries.transpose(2,3)[0,0]\n\ntensor([[ 0.3374, -0.0770, -1.3250,  0.8768],\n        [-0.1778, -1.0205,  0.1784,  1.6221],\n        [-0.3035, -0.1690, -2.1338, -1.4779],\n        [-0.5880,  0.9178,  1.0524,  1.1331],\n        [ 0.3486,  1.5810, -0.3885, -1.2203],\n        [ 0.6603,  1.3010, -0.9343,  1.3139],\n        [-0.2196,  1.2753, -0.4991,  1.0533],\n        [-0.3792, -0.2010, -1.0867,  0.1388],\n        [ 0.7671,  0.4965,  0.8805,  2.2473],\n        [-1.1925, -1.5723,  1.5542, -0.8036],\n        [ 0.6984,  0.9666,  0.6266, -0.2808],\n        [-1.4097, -1.1481, -0.1755,  0.7697],\n        [ 0.1794, -1.1589,  0.0983, -0.6596],\n        [ 1.8951,  0.3255, -0.0935, -0.7979],\n        [ 0.4954, -0.6315,  0.2662,  0.1838],\n        [ 0.2692, -2.8400, -0.5850,  0.2293]])\n\n\n\nqueries.numel()\n\n384\n\n\n\nqueries.transpose(2,3).view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4).shape\n\ntorch.Size([2, 3, 4, 2, 2, 4])\n\n\n\nqueries.transpose(2,3).view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4).transpose(-2, -3).shape\n\ntorch.Size([2, 3, 4, 2, 2, 4])\n\n\n\n(queries.transpose(2,3)\n .view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4)\n .transpose(-2, -3)\n .reshape(batch_size, context_len,16, 4)).shape\n\ntorch.Size([2, 3, 16, 4])\n\n\n\n(queries.transpose(2,3)\n .view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4)\n .transpose(-2, -3)\n .reshape(batch_size, context_len,16, 4)).transpose(2,3)[0,0]\n\ntensor([[ 0.3374, -0.3035, -0.1778, -0.5880,  0.3486, -0.2196,  0.6603, -0.3792,\n          0.7671,  0.6984, -1.1925, -1.4097,  0.1794,  0.4954,  1.8951,  0.2692],\n        [-0.0770, -0.1690, -1.0205,  0.9178,  1.5810,  1.2753,  1.3010, -0.2010,\n          0.4965,  0.9666, -1.5723, -1.1481, -1.1589, -0.6315,  0.3255, -2.8400],\n        [-1.3250, -2.1338,  0.1784,  1.0524, -0.3885, -0.4991, -0.9343, -1.0867,\n          0.8805,  0.6266,  1.5542, -0.1755,  0.0983,  0.2662, -0.0935, -0.5850],\n        [ 0.8768, -1.4779,  1.6221,  1.1331, -1.2203,  1.0533,  1.3139,  0.1388,\n          2.2473, -0.2808, -0.8036,  0.7697, -0.6596,  0.1838, -0.7979,  0.2293]])\n\n\n\npermute(queries.transpose(2,3), num_heads, 16, 4).shape\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[90], line 1\n----&gt; 1 permute(queries.transpose(2,3), num_heads, 16, 4).shape\n\nCell In[71], line 2, in permute(w, n_heads, dim1, dim2)\n      1 def permute(w, n_heads, dim1=dim, dim2=dim):\n----&gt; 2     return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\nRuntimeError: shape '[4, 2, 2, 4]' is invalid for input of size 384\n\n\n\n\nimport transformers\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, apply_rotary_pos_emb\n\n\ntransformers_version = transformers.__version__\ntransformers_version\n\n'4.50.1'\n\n\n\nclass RoPEConfig:\n    dim: int = head_dim\n    rope_theta = theta\n    max_position_embeddings: int = context_len\n    hidden_size = head_dim * num_heads\n    num_attention_heads = num_heads\n\nconfig = RoPEConfig()\nhf_rot_emb = LlamaRotaryEmbedding(config=config)\n\n\nposition_ids = torch.arange(context_len, dtype=torch.long).unsqueeze(0)\nposition_ids\n\ntensor([[0, 1, 2]])\n\n\n\nhf_cos, hf_sin = hf_rot_emb(queries, position_ids)\nhf_queries_t_rot, hf_keys_t_rot = apply_rotary_pos_emb(queires_t, keys_t, hf_cos, hf_sin)\nhf_queries_t_rot.shape, hf_keys_t_rot.shape\n\n(torch.Size([2, 4, 3, 16]), torch.Size([2, 4, 3, 16]))\n\n\n\nhf_queries_rot = hf_queries_t_rot.transpose(1,2)\nhf_keys_rot = hf_keys_t_rot.transpose(1,2)\nhf_queries_rot.shape, hf_keys_rot.shape\n\n(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]))\n\n\n\nassert torch.allclose(tt_queries_rot, permute(hf_queries_rot, n_heads=num_heads))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[73], line 1\n----&gt; 1 assert torch.allclose(tt_queries_rot, permute(hf_queries_rot, n_heads=num_heads))\n\nCell In[71], line 2, in permute(w, n_heads, dim1, dim2)\n      1 def permute(w, n_heads, dim1=dim, dim2=dim):\n----&gt; 2     return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\nRuntimeError: shape '[4, 2, 2, 16]' is invalid for input of size 384\n\n\n\n\ntorch.testing.assert_close(sin.repeat(1,2), ref_sin.squeeze(0))\ntorch.testing.assert_close(cos.repeat(1,2), ref_cos.squeeze(0))\ntorch.testing.assert_close(keys_rot, ref_keys_rot)\ntorch.testing.assert_close(queries_rot, ref_queries_rot)\n\n\ndef permute(w, n_heads, dim1=dim, dim2=dim):\n    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\nFor HF implementation to match with torchtune, we have to permute this.\n\ndim=8\nt = torch.arange(8*8).reshape(dim,dim)\nt\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31],\n        [32, 33, 34, 35, 36, 37, 38, 39],\n        [40, 41, 42, 43, 44, 45, 46, 47],\n        [48, 49, 50, 51, 52, 53, 54, 55],\n        [56, 57, 58, 59, 60, 61, 62, 63]])\n\n\n\nn_heads=2\nt_ = t.view(n_heads, dim//n_heads//2, 2, dim)\nt_.shape, t_\n\n(torch.Size([2, 2, 2, 8]),\n tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],\n           [ 8,  9, 10, 11, 12, 13, 14, 15]],\n \n          [[16, 17, 18, 19, 20, 21, 22, 23],\n           [24, 25, 26, 27, 28, 29, 30, 31]]],\n \n \n         [[[32, 33, 34, 35, 36, 37, 38, 39],\n           [40, 41, 42, 43, 44, 45, 46, 47]],\n \n          [[48, 49, 50, 51, 52, 53, 54, 55],\n           [56, 57, 58, 59, 60, 61, 62, 63]]]]))\n\n\n\nt_t = t_.transpose(1,2)\nt_t.transpose(1,2).shape, t_t\n\n(torch.Size([2, 2, 2, 8]),\n tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],\n           [16, 17, 18, 19, 20, 21, 22, 23]],\n \n          [[ 8,  9, 10, 11, 12, 13, 14, 15],\n           [24, 25, 26, 27, 28, 29, 30, 31]]],\n \n \n         [[[32, 33, 34, 35, 36, 37, 38, 39],\n           [48, 49, 50, 51, 52, 53, 54, 55]],\n \n          [[40, 41, 42, 43, 44, 45, 46, 47],\n           [56, 57, 58, 59, 60, 61, 62, 63]]]]))\n\n\n\nt_t.reshape(dim,dim)\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [24, 25, 26, 27, 28, 29, 30, 31],\n        [32, 33, 34, 35, 36, 37, 38, 39],\n        [48, 49, 50, 51, 52, 53, 54, 55],\n        [40, 41, 42, 43, 44, 45, 46, 47],\n        [56, 57, 58, 59, 60, 61, 62, 63]])\n\n\nPermuting on columns.\n\nt.T\n\ntensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n        [ 1,  9, 17, 25, 33, 41, 49, 57],\n        [ 2, 10, 18, 26, 34, 42, 50, 58],\n        [ 3, 11, 19, 27, 35, 43, 51, 59],\n        [ 4, 12, 20, 28, 36, 44, 52, 60],\n        [ 5, 13, 21, 29, 37, 45, 53, 61],\n        [ 6, 14, 22, 30, 38, 46, 54, 62],\n        [ 7, 15, 23, 31, 39, 47, 55, 63]])\n\n\n\npermute(t.T, n_heads, dim, dim)\n\ntensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n        [ 2, 10, 18, 26, 34, 42, 50, 58],\n        [ 1,  9, 17, 25, 33, 41, 49, 57],\n        [ 3, 11, 19, 27, 35, 43, 51, 59],\n        [ 4, 12, 20, 28, 36, 44, 52, 60],\n        [ 6, 14, 22, 30, 38, 46, 54, 62],\n        [ 5, 13, 21, 29, 37, 45, 53, 61],\n        [ 7, 15, 23, 31, 39, 47, 55, 63]])\n\n\n\npermute(t.T, n_heads, dim, dim).T\n\ntensor([[ 0,  2,  1,  3,  4,  6,  5,  7],\n        [ 8, 10,  9, 11, 12, 14, 13, 15],\n        [16, 18, 17, 19, 20, 22, 21, 23],\n        [24, 26, 25, 27, 28, 30, 29, 31],\n        [32, 34, 33, 35, 36, 38, 37, 39],\n        [40, 42, 41, 43, 44, 46, 45, 47],\n        [48, 50, 49, 51, 52, 54, 53, 55],\n        [56, 58, 57, 59, 60, 62, 61, 63]])\n\n\nLet’s add a batch dimension.\n\ndim=8\nbs=2\nt = torch.arange(dim*dim*bs).reshape(bs,dim,dim)\nt\n\ntensor([[[  0,   1,   2,   3,   4,   5,   6,   7],\n         [  8,   9,  10,  11,  12,  13,  14,  15],\n         [ 16,  17,  18,  19,  20,  21,  22,  23],\n         [ 24,  25,  26,  27,  28,  29,  30,  31],\n         [ 32,  33,  34,  35,  36,  37,  38,  39],\n         [ 40,  41,  42,  43,  44,  45,  46,  47],\n         [ 48,  49,  50,  51,  52,  53,  54,  55],\n         [ 56,  57,  58,  59,  60,  61,  62,  63]],\n\n        [[ 64,  65,  66,  67,  68,  69,  70,  71],\n         [ 72,  73,  74,  75,  76,  77,  78,  79],\n         [ 80,  81,  82,  83,  84,  85,  86,  87],\n         [ 88,  89,  90,  91,  92,  93,  94,  95],\n         [ 96,  97,  98,  99, 100, 101, 102, 103],\n         [104, 105, 106, 107, 108, 109, 110, 111],\n         [112, 113, 114, 115, 116, 117, 118, 119],\n         [120, 121, 122, 123, 124, 125, 126, 127]]])\n\n\n\nn_heads=2\nt_ = t.view(bs, n_heads, dim//n_heads//2, 2, dim)\nt_.shape, t_\n\n(torch.Size([2, 2, 2, 2, 8]),\n tensor([[[[[  0,   1,   2,   3,   4,   5,   6,   7],\n            [  8,   9,  10,  11,  12,  13,  14,  15]],\n \n           [[ 16,  17,  18,  19,  20,  21,  22,  23],\n            [ 24,  25,  26,  27,  28,  29,  30,  31]]],\n \n \n          [[[ 32,  33,  34,  35,  36,  37,  38,  39],\n            [ 40,  41,  42,  43,  44,  45,  46,  47]],\n \n           [[ 48,  49,  50,  51,  52,  53,  54,  55],\n            [ 56,  57,  58,  59,  60,  61,  62,  63]]]],\n \n \n \n         [[[[ 64,  65,  66,  67,  68,  69,  70,  71],\n            [ 72,  73,  74,  75,  76,  77,  78,  79]],\n \n           [[ 80,  81,  82,  83,  84,  85,  86,  87],\n            [ 88,  89,  90,  91,  92,  93,  94,  95]]],\n \n \n          [[[ 96,  97,  98,  99, 100, 101, 102, 103],\n            [104, 105, 106, 107, 108, 109, 110, 111]],\n \n           [[112, 113, 114, 115, 116, 117, 118, 119],\n            [120, 121, 122, 123, 124, 125, 126, 127]]]]]))\n\n\n\nt_t = t_.transpose(1,2)\nt_t.transpose(1,2).shape, t_t\n\n(torch.Size([2, 2, 2, 2, 8]),\n tensor([[[[[  0,   1,   2,   3,   4,   5,   6,   7],\n            [  8,   9,  10,  11,  12,  13,  14,  15]],\n \n           [[ 32,  33,  34,  35,  36,  37,  38,  39],\n            [ 40,  41,  42,  43,  44,  45,  46,  47]]],\n \n \n          [[[ 16,  17,  18,  19,  20,  21,  22,  23],\n            [ 24,  25,  26,  27,  28,  29,  30,  31]],\n \n           [[ 48,  49,  50,  51,  52,  53,  54,  55],\n            [ 56,  57,  58,  59,  60,  61,  62,  63]]]],\n \n \n \n         [[[[ 64,  65,  66,  67,  68,  69,  70,  71],\n            [ 72,  73,  74,  75,  76,  77,  78,  79]],\n \n           [[ 96,  97,  98,  99, 100, 101, 102, 103],\n            [104, 105, 106, 107, 108, 109, 110, 111]]],\n \n \n          [[[ 80,  81,  82,  83,  84,  85,  86,  87],\n            [ 88,  89,  90,  91,  92,  93,  94,  95]],\n \n           [[112, 113, 114, 115, 116, 117, 118, 119],\n            [120, 121, 122, 123, 124, 125, 126, 127]]]]]))\n\n\n\nt_t.reshape(bs,dim,dim)\n\ntensor([[[  0,   1,   2,   3,   4,   5,   6,   7],\n         [  8,   9,  10,  11,  12,  13,  14,  15],\n         [ 32,  33,  34,  35,  36,  37,  38,  39],\n         [ 40,  41,  42,  43,  44,  45,  46,  47],\n         [ 16,  17,  18,  19,  20,  21,  22,  23],\n         [ 24,  25,  26,  27,  28,  29,  30,  31],\n         [ 48,  49,  50,  51,  52,  53,  54,  55],\n         [ 56,  57,  58,  59,  60,  61,  62,  63]],\n\n        [[ 64,  65,  66,  67,  68,  69,  70,  71],\n         [ 72,  73,  74,  75,  76,  77,  78,  79],\n         [ 96,  97,  98,  99, 100, 101, 102, 103],\n         [104, 105, 106, 107, 108, 109, 110, 111],\n         [ 80,  81,  82,  83,  84,  85,  86,  87],\n         [ 88,  89,  90,  91,  92,  93,  94,  95],\n         [112, 113, 114, 115, 116, 117, 118, 119],\n         [120, 121, 122, 123, 124, 125, 126, 127]]])\n\n\nPermuting on columns.\n\nt.transpose(1,2)\n\ntensor([[[  0,   8,  16,  24,  32,  40,  48,  56],\n         [  1,   9,  17,  25,  33,  41,  49,  57],\n         [  2,  10,  18,  26,  34,  42,  50,  58],\n         [  3,  11,  19,  27,  35,  43,  51,  59],\n         [  4,  12,  20,  28,  36,  44,  52,  60],\n         [  5,  13,  21,  29,  37,  45,  53,  61],\n         [  6,  14,  22,  30,  38,  46,  54,  62],\n         [  7,  15,  23,  31,  39,  47,  55,  63]],\n\n        [[ 64,  72,  80,  88,  96, 104, 112, 120],\n         [ 65,  73,  81,  89,  97, 105, 113, 121],\n         [ 66,  74,  82,  90,  98, 106, 114, 122],\n         [ 67,  75,  83,  91,  99, 107, 115, 123],\n         [ 68,  76,  84,  92, 100, 108, 116, 124],\n         [ 69,  77,  85,  93, 101, 109, 117, 125],\n         [ 70,  78,  86,  94, 102, 110, 118, 126],\n         [ 71,  79,  87,  95, 103, 111, 119, 127]]])\n\n\n\npermute(t.T, n_heads, dim, dim)\n\ntensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n        [ 2, 10, 18, 26, 34, 42, 50, 58],\n        [ 1,  9, 17, 25, 33, 41, 49, 57],\n        [ 3, 11, 19, 27, 35, 43, 51, 59],\n        [ 4, 12, 20, 28, 36, 44, 52, 60],\n        [ 6, 14, 22, 30, 38, 46, 54, 62],\n        [ 5, 13, 21, 29, 37, 45, 53, 61],\n        [ 7, 15, 23, 31, 39, 47, 55, 63]])\n\n\n\npermute(t.T, n_heads, dim, dim).T\n\ntensor([[ 0,  2,  1,  3,  4,  6,  5,  7],\n        [ 8, 10,  9, 11, 12, 14, 13, 15],\n        [16, 18, 17, 19, 20, 22, 21, 23],\n        [24, 26, 25, 27, 28, 30, 29, 31],\n        [32, 34, 33, 35, 36, 38, 37, 39],\n        [40, 42, 41, 43, 44, 46, 45, 47],\n        [48, 50, 49, 51, 52, 54, 53, 55],\n        [56, 58, 57, 59, 60, 62, 61, 63]])\n\n\n\npermute(t, 2, dim, dim)\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [24, 25, 26, 27, 28, 29, 30, 31],\n        [32, 33, 34, 35, 36, 37, 38, 39],\n        [48, 49, 50, 51, 52, 53, 54, 55],\n        [40, 41, 42, 43, 44, 45, 46, 47],\n        [56, 57, 58, 59, 60, 61, 62, 63]])"
  },
  {
    "objectID": "blog/posts/2025-07-24-RoPE/index.html#implementation",
    "href": "blog/posts/2025-07-24-RoPE/index.html#implementation",
    "title": "RoPE",
    "section": "",
    "text": "There are two ways to implement RoPE. One from Llama and another from Hugging Face.\n\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple\n\n\nbatch_size = 2\ncontext_len = 3\nnum_heads = 4\nhead_dim = 16\n\n\ntorch.manual_seed(123)\nqueries = torch.randn(batch_size, context_len, num_heads, head_dim)\nkeys = torch.randn(batch_size, context_len, num_heads, head_dim)\nqueries.shape, keys.shape\n\n(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]))\n\n\n\n\nBelow is the code from Llama 2 implementation. Let’s try to understand it.\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    \"\"\"\n    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n    The returned tensor contains complex values in complex64 data type.\n\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        end (int): End index for precomputing frequencies.\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n\n    Returns:\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n    \"\"\"\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device)  # type: ignore\n    freqs = torch.outer(t, freqs).float()  # type: ignore\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis\n\nLet’s go through code step by step. First, we need the arguments to pass.\n\ndim = head_dim\nend = context_len\ntheta = 10_000\n\nWe want to create a frequency tensor. We are creating a dim // 2 tensor because we get cos and sin from each frequency. And these are applied to each column as we will see.\n\ntorch.arange(0, dim, 2)[: (dim // 2)].float()\n\ntensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14.])\n\n\nThen we normalize by dim.\n\n(torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)\n\ntensor([0.0000, 0.1250, 0.2500, 0.3750, 0.5000, 0.6250, 0.7500, 0.8750])\n\n\n\ntheta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)\n\ntensor([1.0000e+00, 3.1623e+00, 1.0000e+01, 3.1623e+01, 1.0000e+02, 3.1623e+02,\n        1.0000e+03, 3.1623e+03])\n\n\n\n1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n\ntensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n        1.0000e-03, 3.1623e-04])\n\n\n\nfreqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\nfreqs\n\ntensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n        1.0000e-03, 3.1623e-04])\n\n\n\nHigh frequencies (like 1.0) rotate quickly → capture short-range positional relationships\nLow frequencies (like 3.1623e-04 = 0.00031623) rotate slowly → capture long-range positional relationships\n\nThink of it like a clock: - A second hand (high frequency) completes many rotations quickly - good for distinguishing nearby moments - An hour hand (low frequency) rotates slowly - good for distinguishing longer time periods\nIn RoPE, the high frequency dimensions help the model distinguish between adjacent tokens, while the low frequency dimensions help distinguish between tokens that are far apart in the sequence.\n\nt = torch.arange(end, device=freqs.device)\nt\n\ntensor([0, 1, 2])\n\n\n\nfreqs = torch.outer(t, freqs).float()\nfreqs.shape, freqs\n\n(torch.Size([3, 8]),\n tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n          0.0000e+00, 0.0000e+00],\n         [1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n          1.0000e-03, 3.1623e-04],\n         [2.0000e+00, 6.3246e-01, 2.0000e-01, 6.3246e-02, 2.0000e-02, 6.3246e-03,\n          2.0000e-03, 6.3246e-04]]))\n\n\n\nLower dimension indices → higher frequencies → faster rotation\nHigher dimension indices → lower frequencies → slower rotation\n\n\nfreqs_cis = torch.polar(torch.ones_like(freqs), freqs)\nfreqs_cis.shape, freqs_cis\n\n(torch.Size([3, 8]),\n tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n           1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n           1.0000+0.0000e+00j,  1.0000+0.0000e+00j],\n         [ 0.5403+8.4147e-01j,  0.9504+3.1098e-01j,  0.9950+9.9833e-02j,\n           0.9995+3.1618e-02j,  0.9999+9.9998e-03j,  1.0000+3.1623e-03j,\n           1.0000+1.0000e-03j,  1.0000+3.1623e-04j],\n         [-0.4161+9.0930e-01j,  0.8066+5.9113e-01j,  0.9801+1.9867e-01j,\n           0.9980+6.3203e-02j,  0.9998+1.9999e-02j,  1.0000+6.3245e-03j,\n           1.0000+2.0000e-03j,  1.0000+6.3246e-04j]]))\n\n\n\n\n\nFigure 2: Long-term decay of RoPE\n\n\nImage from the RoPE paper.\n\ndef apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply rotary embeddings to input tensors using the given frequency tensor.\n\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n    returned as real tensors.\n\n    Args:\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n    \"\"\"\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\nqueries and keys have shape (batch_size, context_len, num_heads, head_dim).\n\nxq = queries\nxk = keys\n\nxq.shape, xk.shape, freqs_cis.shape\n\n(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]), torch.Size([3, 8]))\n\n\n\nxq.shape[:-1]\n\ntorch.Size([2, 3, 4])\n\n\nExpand last dimension of query (head_dim), 16, into 2. One for cos, the other for sin.\n\nxq.float().reshape(*xq.shape[:-1], -1, 2).shape\n\ntorch.Size([2, 3, 4, 8, 2])\n\n\n\nxq[0,0,0]\n\ntensor([ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n         0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692])\n\n\n\nxq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\nxq_.shape\n\ntorch.Size([2, 3, 4, 8])\n\n\n\nxq_[0,0,0]\n\ntensor([ 0.3374-0.1778j, -0.3035-0.5880j,  0.3486+0.6603j, -0.2196-0.3792j,\n         0.7671-1.1925j,  0.6984-1.4097j,  0.1794+1.8951j,  0.4954+0.2692j])\n\n\nThen, reshape_for_broadcast happens.\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    \"\"\"\n    Reshape frequency tensor for broadcasting it with another tensor.\n\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\n\n    Args:\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\n\n    Returns:\n        torch.Tensor: Reshaped frequency tensor.\n\n    Raises:\n        AssertionError: If the frequency tensor doesn't match the expected shape.\n        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n    \"\"\"\n    ndim = x.ndim\n    assert 0 &lt;= 1 &lt; ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(*shape)\n\nLet’s remind ourselves of the shapes.\n\nfreqs_cis.shape, queries.shape\n\n(torch.Size([3, 8]), torch.Size([2, 3, 4, 16]))\n\n\nRembmer queries and keys have shape: (batch_size, context_len, num_heads, head_dim). reshape_for_broadcast is used after queries and keys are transformed into complex space according to apply_rotary_emb. We want freqs_cis to be (1, context_len, 1, head_dim). Let’s use queries as x.\n\nx = xq_\nx.shape\n\ntorch.Size([2, 3, 4, 8])\n\n\n\nndim = x.ndim\nndim\n\n4\n\n\n\nassert 0 &lt;= 1 &lt; ndim\nassert freqs_cis.shape == (x.shape[1], x.shape[-1])\n\n\nshape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\nshape\n\n[1, 3, 1, 8]\n\n\nGoing back to apply_rotary_emb.\n\nfreqs_cis = reshape_for_broadcast(freqs_cis, xq_)\nfreqs_cis.shape\n\ntorch.Size([1, 3, 1, 8])\n\n\n\n(xq_ * freqs_cis).shape\n\ntorch.Size([2, 3, 4, 8])\n\n\n\nxq_[0,1,0]\n\ntensor([ 0.5146+0.9938j, -0.2587-1.0826j, -0.0444+1.6236j, -2.3229+1.0878j,\n         0.6716+0.6933j, -0.9487-0.0765j, -0.1526+0.1167j,  0.4403-1.4465j])\n\n\n\nfreqs_cis[0,1,0]\n\ntensor([0.5403+8.4147e-01j, 0.9504+3.1098e-01j, 0.9950+9.9833e-02j,\n        0.9995+3.1618e-02j, 0.9999+9.9998e-03j, 1.0000+3.1623e-03j,\n        1.0000+1.0000e-03j, 1.0000+3.1623e-04j])\n\n\n\n(xq_ * freqs_cis)[0,1,0]\n\ntensor([-0.5582+0.9700j,  0.0908-1.1093j, -0.2062+1.6110j, -2.3561+1.0138j,\n         0.6646+0.7000j, -0.9485-0.0795j, -0.1528+0.1166j,  0.4407-1.4464j])\n\n\n\n# Under the hood\n0.5146*0.5403-0.9938*8.4147e-01, 0.5146*8.4147e-01+0.9938*0.5403\n\n(-0.5582145060000001, 0.969970602)\n\n\nThe multiplication xq_ * freqs_cis is complex number multiplication, which performs the rotation.\nWhen you multiply two complex numbers:\n(a + bi) * (cos(θ) + i*sin(θ)) = (a*cos(θ) - b*sin(θ)) + i*(a*sin(θ) + b*cos(θ))\nThis is exactly the 2D rotation formula! So each complex number in xq_ gets rotated by the corresponding angle in freqs_cis.\nEach position gets its own rotation angles, and each dimension pair gets its own frequency, but all heads share the same positional encoding.\nThis is what the transformation looks like.\n\n(xq_ * freqs_cis)[0,0]\n\ntensor([[ 0.3374-0.1778j, -0.3035-0.5880j,  0.3486+0.6603j, -0.2196-0.3792j,\n          0.7671-1.1925j,  0.6984-1.4097j,  0.1794+1.8951j,  0.4954+0.2692j],\n        [-0.0770-1.0205j, -0.1690+0.9178j,  1.5810+1.3010j,  1.2753-0.2010j,\n          0.4965-1.5723j,  0.9666-1.1481j, -1.1589+0.3255j, -0.6315-2.8400j],\n        [-1.3250+0.1784j, -2.1338+1.0524j, -0.3885-0.9343j, -0.4991-1.0867j,\n          0.8805+1.5542j,  0.6266-0.1755j,  0.0983-0.0935j,  0.2662-0.5850j],\n        [ 0.8768+1.6221j, -1.4779+1.1331j, -1.2203+1.3139j,  1.0533+0.1388j,\n          2.2473-0.8036j, -0.2808+0.7697j, -0.6596-0.7979j,  0.1838+0.2293j]])\n\n\n\n\n\nFigure 1 from Paper\n\n\nImage from the RoPE paper.\nWhen we use torch.view_as_real,\n\ntorch.view_as_real(xq_ * freqs_cis)[0,0]\n\ntensor([[[ 0.3374, -0.1778],\n         [-0.3035, -0.5880],\n         [ 0.3486,  0.6603],\n         [-0.2196, -0.3792],\n         [ 0.7671, -1.1925],\n         [ 0.6984, -1.4097],\n         [ 0.1794,  1.8951],\n         [ 0.4954,  0.2692]],\n\n        [[-0.0770, -1.0205],\n         [-0.1690,  0.9178],\n         [ 1.5810,  1.3010],\n         [ 1.2753, -0.2010],\n         [ 0.4965, -1.5723],\n         [ 0.9666, -1.1481],\n         [-1.1589,  0.3255],\n         [-0.6315, -2.8400]],\n\n        [[-1.3250,  0.1784],\n         [-2.1338,  1.0524],\n         [-0.3885, -0.9343],\n         [-0.4991, -1.0867],\n         [ 0.8805,  1.5542],\n         [ 0.6266, -0.1755],\n         [ 0.0983, -0.0935],\n         [ 0.2662, -0.5850]],\n\n        [[ 0.8768,  1.6221],\n         [-1.4779,  1.1331],\n         [-1.2203,  1.3139],\n         [ 1.0533,  0.1388],\n         [ 2.2473, -0.8036],\n         [-0.2808,  0.7697],\n         [-0.6596, -0.7979],\n         [ 0.1838,  0.2293]]])\n\n\n\nxq_out[0,0]\n\ntensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n\n\n\nxq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\nxq_out.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\n\n\n\ntorchtune’s RotaryPositionalEmbeddings follows llama’s implementation.\n\nfrom torchtune.modules import RotaryPositionalEmbeddings\n\nWARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n\n\n\ntt_rope_emb = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=context_len, base=theta)\ntt_rope_emb\n\nRotaryPositionalEmbeddings()\n\n\n\ntt_queries_rot = tt_rope_emb(queries)\ntt_queries_rot.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\nThis gives us the same result as llama implementation.\n\nassert torch.allclose(tt_queries_rot, xq_out)"
  },
  {
    "objectID": "blog/posts/2025-07-24-RoPE/index.html#litgpt-implementation",
    "href": "blog/posts/2025-07-24-RoPE/index.html#litgpt-implementation",
    "title": "RoPE",
    "section": "",
    "text": "# LitGPT code function `litgpt_build_rope_cache` from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py\n# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE\ndef litgpt_build_rope_cache(\n    seq_len: int,\n    n_elem: int,\n    device: Optional[torch.device] = None,\n    base: int = 10000,\n    condense_ratio: int = 1,\n    extra_config: Optional[dict] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Enhanced Transformer with Rotary Position Embedding.\n\n    Args:\n        seq_len (int): Sequence length.\n        n_elem (int): Number of elements (head dimension).\n        device (torch.device, optional): Device for tensor allocations.\n        base (int, optional): Base for computing inverse frequencies.\n        condense_ratio (int, optional): Ratio to condense the position indices.\n        extra_config (dict, optional): Configuration parameters for frequency adjustments (used by Llama 3.1 and 3.2)\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Cosine and sine caches for RoPE.\n    \"\"\"\n    # Compute the inverse frequencies theta\n    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, device=device).float() / n_elem))\n\n    if extra_config is not None:\n        orig_context_len = extra_config[\"original_max_seq_len\"]\n        factor = extra_config[\"factor\"]\n        low_freq_factor = extra_config[\"low_freq_factor\"]\n        high_freq_factor = extra_config[\"high_freq_factor\"]\n\n        wavelen = 2 * torch.pi / theta\n        ratio = orig_context_len / wavelen\n        smooth_factor = (ratio - low_freq_factor) / (high_freq_factor - low_freq_factor)\n        smooth_factor = torch.clamp(smooth_factor, min=0.0, max=1.0)\n\n        # Compute adjusted_theta without masked indexing\n        adjusted_theta = (1 - smooth_factor) * (theta / factor) + smooth_factor * theta\n        theta = adjusted_theta\n\n    # Create position indices `[0, 1, ..., seq_len - 1]`\n    seq_idx = torch.arange(seq_len, device=device) / condense_ratio\n\n    # Calculate the product of position index and $\\theta_i$\n    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n\n    return torch.cos(idx_theta), torch.sin(idx_theta)\n\n\nlitgpt_cos, litgpt_sin = litgpt_build_rope_cache(context_len, n_elem=head_dim, base=theta)\nlitgpt_cos.shape, litgpt_sin.shape\n\n(torch.Size([3, 16]), torch.Size([3, 16]))\n\n\n\nfreqs_cis = precompute_freqs_cis(dim=head_dim, end=context_len, theta=theta)\nfreqs_cis.shape\n\ntorch.Size([3, 8])\n\n\n\nlitgpt_cos[1], litgpt_sin[1]\n\n(tensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000, 0.5403,\n         0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000]),\n tensor([8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n         1.0000e-03, 3.1623e-04, 8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02,\n         9.9998e-03, 3.1623e-03, 1.0000e-03, 3.1623e-04]))\n\n\n\nfreqs_cis[1]\n\ntensor([0.5403+8.4147e-01j, 0.9504+3.1098e-01j, 0.9950+9.9833e-02j,\n        0.9995+3.1618e-02j, 0.9999+9.9998e-03j, 1.0000+3.1623e-03j,\n        1.0000+1.0000e-03j, 1.0000+3.1623e-04j])\n\n\n\ntorch.cos(freqs)[1]\n\ntensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000])\n\n\n\ntorch.cos(freqs_cis)[1]\n\ntensor([1.1795-4.8577e-01j, 0.6097-2.5713e-01j, 0.5472-8.3876e-02j,\n        0.5410-2.6601e-02j, 0.5404-8.4144e-03j, 0.5403-2.6610e-03j,\n        0.5403-8.4147e-04j, 0.5403-2.6610e-04j])\n\n\n\ntorch.stack([litgpt_cos, litgpt_sin], dim=-1).shape\n\ntorch.Size([3, 16, 2])\n\n\n\ntorch.view_as_real(freqs_cis).shape, torch.view_as_real(freqs_cis)[1]\n\n(torch.Size([3, 8, 2]),\n tensor([[5.4030e-01, 8.4147e-01],\n         [9.5042e-01, 3.1098e-01],\n         [9.9500e-01, 9.9833e-02],\n         [9.9950e-01, 3.1618e-02],\n         [9.9995e-01, 9.9998e-03],\n         [9.9999e-01, 3.1623e-03],\n         [1.0000e+00, 1.0000e-03],\n         [1.0000e+00, 3.1623e-04]]))\n\n\n\ntorch.view_as_real(freqs_cis)[..., 0][1]\n\ntensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000])\n\n\n\nassert torch.allclose(litgpt_cos, torch.view_as_real(freqs_cis)[..., 0].repeat(1,2))\n\n\n# LitGPT code from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py\n# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE\ndef litgpt_apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -&gt; torch.Tensor:\n    head_size = x.size(-1)\n    a = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n    b = x[..., head_size // 2:]  # (B, nh, T, hs/2)\n    rotated = torch.cat((-b, a), dim=-1)  # (B, nh, T, hs)\n    if cos.dim() &gt; 1:\n        # batch dimensions must align\n        # sin/cos are (B, T, hs) so we unsqeeze -3 for nh\n        # we count from back because all of apply_rope does\n        cos = cos.unsqueeze(-3)\n        sin = sin.unsqueeze(-3)\n\n    roped = (x * cos) + (rotated * sin)\n    return roped.to(dtype=x.dtype)\n\n(a + bi) * (cos(θ) + i*sin(θ)) = (acos(θ) - bsin(θ)) + i(asin(θ) + bcos(θ))\n(acos(θ) - bsin(θ)) + i(asin(θ) + bcos(θ)) = (acos(θ)+ibcos(θ)) + (-bsin(θ)+iasin(θ))\n\nqueries.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\n\nassert torch.allclose(tt_queries_rot, freqs_cis)"
  },
  {
    "objectID": "blog/posts/2025-07-24-RoPE/index.html#hugging-face-implementation",
    "href": "blog/posts/2025-07-24-RoPE/index.html#hugging-face-implementation",
    "title": "RoPE",
    "section": "",
    "text": "Hugging Face modeling_llama.\n\nqueires_t = queries.transpose(1,2)\nkeys_t = keys.transpose(1,2)\nqueires_t.shape, keys_t.shape\n\n(torch.Size([2, 4, 3, 16]), torch.Size([2, 4, 3, 16]))\n\n\n\nqueries\n\ntensor([[[[ 3.3737e-01, -1.7778e-01, -3.0353e-01, -5.8801e-01,  3.4861e-01,\n            6.6034e-01, -2.1964e-01, -3.7917e-01,  7.6711e-01, -1.1925e+00,\n            6.9835e-01, -1.4097e+00,  1.7938e-01,  1.8951e+00,  4.9545e-01,\n            2.6920e-01],\n          [-7.7020e-02, -1.0205e+00, -1.6896e-01,  9.1776e-01,  1.5810e+00,\n            1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n            9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n           -2.8400e+00],\n          [-1.3250e+00,  1.7843e-01, -2.1338e+00,  1.0524e+00, -3.8848e-01,\n           -9.3435e-01, -4.9914e-01, -1.0867e+00,  8.8054e-01,  1.5542e+00,\n            6.2662e-01, -1.7549e-01,  9.8284e-02, -9.3507e-02,  2.6621e-01,\n           -5.8504e-01],\n          [ 8.7684e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n            1.3139e+00,  1.0533e+00,  1.3881e-01,  2.2473e+00, -8.0364e-01,\n           -2.8084e-01,  7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n            2.2935e-01]],\n\n         [[ 5.1463e-01,  9.9376e-01, -2.5873e-01, -1.0826e+00, -4.4382e-02,\n            1.6236e+00, -2.3229e+00,  1.0878e+00,  6.7155e-01,  6.9330e-01,\n           -9.4872e-01, -7.6507e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n           -1.4465e+00],\n          [ 2.5529e-01, -5.4963e-01,  1.0042e+00,  8.2723e-01, -3.9481e-01,\n            4.8923e-01, -2.1681e-01, -1.7472e+00, -1.6025e+00, -1.0764e+00,\n            9.0315e-01, -7.2184e-01, -5.9508e-01, -7.1122e-01,  6.2296e-01,\n           -1.3729e+00],\n          [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1861e-02,\n           -4.7896e-01,  7.6681e-01,  2.7468e-02,  1.9929e+00,  1.3708e+00,\n           -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3745e-03, -9.8955e-01,\n            7.0161e-01],\n          [-9.4053e-01, -4.6806e-01,  1.0322e+00, -2.8300e-01,  4.9275e-01,\n           -1.4078e-02, -2.7466e-01, -7.6409e-01,  1.3966e+00, -9.9491e-01,\n           -1.5822e-03,  1.2471e+00, -7.7105e-02,  1.2774e+00, -1.4596e+00,\n           -2.1595e+00]],\n\n         [[-2.5822e-01, -2.0407e+00, -8.0156e-01, -8.1830e-01, -1.1820e+00,\n           -2.8774e-01, -6.0430e-01,  6.0024e-01, -1.4053e+00, -5.9217e-01,\n           -2.5479e-01,  1.1517e+00, -1.7858e-02,  4.2640e-01, -7.6574e-01,\n           -5.4514e-02],\n          [-1.2743e+00,  4.5128e-01, -2.2801e-01,  9.2238e-01,  2.0561e-01,\n           -4.9696e-01,  5.8206e-01,  2.0532e-01, -3.0177e-01, -6.7030e-01,\n           -6.1710e-01, -8.3339e-01,  4.8387e-01, -1.3493e-01,  2.1187e-01,\n           -8.7140e-01],\n          [ 6.8508e-01,  2.0024e+00, -5.4688e-01,  1.6014e+00, -2.2577e+00,\n           -1.8009e+00,  7.0147e-01,  5.7028e-01, -1.1766e+00, -2.0524e+00,\n            1.1318e-01,  1.4353e+00,  8.8307e-02, -1.2037e+00,  1.0964e+00,\n            2.4210e+00],\n          [ 1.5382e-01, -4.4516e-01,  5.5035e-01,  6.5788e-02,  6.8050e-01,\n            1.2064e+00,  1.6250e+00,  3.4595e-01,  1.3425e-01,  7.6623e-01,\n            2.2760e+00, -1.3255e+00, -8.9702e-01,  1.1318e-01,  8.3647e-01,\n            2.8520e-02]]],\n\n\n        [[[-9.7969e-01, -2.1126e+00, -2.7214e-01, -3.5100e-01,  1.1152e+00,\n           -6.1722e-01, -2.2708e+00, -1.3819e+00,  1.1721e+00, -4.3716e-01,\n           -4.0527e-01,  7.0864e-01,  9.5331e-01, -1.3035e-02, -1.3009e-01,\n           -8.7660e-02],\n          [-6.7349e-02,  2.4674e-01, -9.3917e-01, -1.0448e+00,  1.2783e+00,\n            4.1903e-01, -5.0727e-01, -6.0623e-01, -1.0532e+00,  1.8386e+00,\n           -1.0954e-01, -3.3161e-01,  9.0084e-01,  4.8398e-01, -1.3237e+00,\n            7.8692e-01],\n          [ 1.3818e+00, -6.9367e-02, -7.6117e-01,  2.4163e-01, -5.8781e-01,\n           -1.1506e+00,  1.0164e+00,  1.2343e-01,  1.1311e+00, -8.5805e-02,\n           -5.9727e-02,  3.5527e-01, -1.4355e+00,  7.2748e-02,  1.0528e-01,\n           -1.0311e+00],\n          [ 1.3113e+00, -3.5963e-02,  2.1181e-01, -8.6248e-03,  1.8576e+00,\n            2.1321e+00, -5.0561e-01, -7.9884e-01, -1.0944e+00, -1.0197e+00,\n           -5.3986e-01,  1.2117e+00, -8.6321e-01,  1.3337e+00,  7.7101e-02,\n           -5.2181e-02]],\n\n         [[ 2.3862e-01,  1.4106e-01, -1.3354e+00, -2.9340e+00,  1.1411e-01,\n           -1.2072e+00, -3.0083e-01,  1.4274e-01, -1.3027e+00, -4.9187e-01,\n           -2.1429e+00,  9.4881e-01, -5.6842e-01, -6.4643e-02,  6.6467e-01,\n           -2.7836e+00],\n          [ 1.1366e+00,  9.0886e-01,  9.4943e-01,  2.6565e-02, -9.2207e-01,\n            7.0338e-01, -3.6590e-01, -1.9654e-01, -9.2071e-01,  3.1535e-01,\n           -2.1734e-02,  3.4414e-01,  2.2710e-01, -4.5969e-01, -6.1831e-01,\n            2.4612e-01],\n          [-4.0549e-01, -8.3681e-01,  1.2277e+00, -4.2971e-01, -2.2121e+00,\n           -3.7802e-01,  9.8382e-01, -1.0895e+00,  2.0171e-01,  2.2145e-02,\n           -1.7753e+00, -7.4896e-01,  2.7808e-01, -9.6208e-01, -4.2228e-01,\n           -1.1036e+00],\n          [ 2.4727e-01,  1.4549e+00, -2.8351e-01, -3.7675e-01, -3.0577e-02,\n           -8.9448e-02, -1.9652e-01, -9.7133e-01,  9.0046e-01, -2.5233e-01,\n            1.0669e+00, -2.9846e-01,  8.5576e-01,  1.6098e+00, -1.1893e+00,\n            1.1677e+00]],\n\n         [[ 3.2765e-01, -8.3307e-01, -1.6179e+00,  2.2651e-01, -4.3815e-01,\n            3.2652e-01, -1.5786e+00, -1.3995e+00,  5.4460e-01, -8.3004e-02,\n           -1.1753e+00,  1.7825e+00,  1.7524e+00, -2.1347e-01,  4.0949e-01,\n            4.6454e-02],\n          [ 6.3669e-01, -1.9433e-01, -8.6139e-01,  5.3384e-01,  9.3758e-01,\n           -9.2248e-01,  7.0466e-01, -2.7221e-01,  1.4419e-02, -6.4115e-01,\n            2.3902e+00, -1.4256e+00, -4.6192e-01, -1.5539e+00, -3.3382e-01,\n            2.4049e-01],\n          [ 2.1065e+00,  5.5087e-01, -2.9364e-01, -1.8027e+00, -6.9333e-01,\n            1.7409e+00,  2.6979e-01,  9.5949e-01, -1.0253e+00, -5.5049e-01,\n            1.0264e+00, -5.6696e-01, -2.6584e-01, -1.1116e+00, -1.3696e+00,\n           -6.5336e-01],\n          [-1.6125e+00, -2.2840e-01,  1.8388e+00, -9.4727e-01,  1.4192e-01,\n            3.6959e-01, -1.7425e-02, -9.5746e-01, -8.1691e-01, -2.8655e-01,\n            4.3434e-01, -1.3402e-01, -2.1467e+00, -1.7984e+00, -6.8222e-01,\n           -5.1905e-01]]]])\n\n\n\ndef permute(w, n_heads, dim1=dim, dim2=dim):\n    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\n\nqueries.shape\n\ntorch.Size([2, 3, 4, 16])\n\n\n\nqueries[0,0]\n\ntensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n\n\n\nqueries.transpose(2,3).shape\n\ntorch.Size([2, 3, 16, 4])\n\n\n\nqueries.transpose(2,3)[0,0]\n\ntensor([[ 0.3374, -0.0770, -1.3250,  0.8768],\n        [-0.1778, -1.0205,  0.1784,  1.6221],\n        [-0.3035, -0.1690, -2.1338, -1.4779],\n        [-0.5880,  0.9178,  1.0524,  1.1331],\n        [ 0.3486,  1.5810, -0.3885, -1.2203],\n        [ 0.6603,  1.3010, -0.9343,  1.3139],\n        [-0.2196,  1.2753, -0.4991,  1.0533],\n        [-0.3792, -0.2010, -1.0867,  0.1388],\n        [ 0.7671,  0.4965,  0.8805,  2.2473],\n        [-1.1925, -1.5723,  1.5542, -0.8036],\n        [ 0.6984,  0.9666,  0.6266, -0.2808],\n        [-1.4097, -1.1481, -0.1755,  0.7697],\n        [ 0.1794, -1.1589,  0.0983, -0.6596],\n        [ 1.8951,  0.3255, -0.0935, -0.7979],\n        [ 0.4954, -0.6315,  0.2662,  0.1838],\n        [ 0.2692, -2.8400, -0.5850,  0.2293]])\n\n\n\nqueries.numel()\n\n384\n\n\n\nqueries.transpose(2,3).view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4).shape\n\ntorch.Size([2, 3, 4, 2, 2, 4])\n\n\n\nqueries.transpose(2,3).view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4).transpose(-2, -3).shape\n\ntorch.Size([2, 3, 4, 2, 2, 4])\n\n\n\n(queries.transpose(2,3)\n .view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4)\n .transpose(-2, -3)\n .reshape(batch_size, context_len,16, 4)).shape\n\ntorch.Size([2, 3, 16, 4])\n\n\n\n(queries.transpose(2,3)\n .view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4)\n .transpose(-2, -3)\n .reshape(batch_size, context_len,16, 4)).transpose(2,3)[0,0]\n\ntensor([[ 0.3374, -0.3035, -0.1778, -0.5880,  0.3486, -0.2196,  0.6603, -0.3792,\n          0.7671,  0.6984, -1.1925, -1.4097,  0.1794,  0.4954,  1.8951,  0.2692],\n        [-0.0770, -0.1690, -1.0205,  0.9178,  1.5810,  1.2753,  1.3010, -0.2010,\n          0.4965,  0.9666, -1.5723, -1.1481, -1.1589, -0.6315,  0.3255, -2.8400],\n        [-1.3250, -2.1338,  0.1784,  1.0524, -0.3885, -0.4991, -0.9343, -1.0867,\n          0.8805,  0.6266,  1.5542, -0.1755,  0.0983,  0.2662, -0.0935, -0.5850],\n        [ 0.8768, -1.4779,  1.6221,  1.1331, -1.2203,  1.0533,  1.3139,  0.1388,\n          2.2473, -0.2808, -0.8036,  0.7697, -0.6596,  0.1838, -0.7979,  0.2293]])\n\n\n\npermute(queries.transpose(2,3), num_heads, 16, 4).shape\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[90], line 1\n----&gt; 1 permute(queries.transpose(2,3), num_heads, 16, 4).shape\n\nCell In[71], line 2, in permute(w, n_heads, dim1, dim2)\n      1 def permute(w, n_heads, dim1=dim, dim2=dim):\n----&gt; 2     return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\nRuntimeError: shape '[4, 2, 2, 4]' is invalid for input of size 384\n\n\n\n\nimport transformers\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, apply_rotary_pos_emb\n\n\ntransformers_version = transformers.__version__\ntransformers_version\n\n'4.50.1'\n\n\n\nclass RoPEConfig:\n    dim: int = head_dim\n    rope_theta = theta\n    max_position_embeddings: int = context_len\n    hidden_size = head_dim * num_heads\n    num_attention_heads = num_heads\n\nconfig = RoPEConfig()\nhf_rot_emb = LlamaRotaryEmbedding(config=config)\n\n\nposition_ids = torch.arange(context_len, dtype=torch.long).unsqueeze(0)\nposition_ids\n\ntensor([[0, 1, 2]])\n\n\n\nhf_cos, hf_sin = hf_rot_emb(queries, position_ids)\nhf_queries_t_rot, hf_keys_t_rot = apply_rotary_pos_emb(queires_t, keys_t, hf_cos, hf_sin)\nhf_queries_t_rot.shape, hf_keys_t_rot.shape\n\n(torch.Size([2, 4, 3, 16]), torch.Size([2, 4, 3, 16]))\n\n\n\nhf_queries_rot = hf_queries_t_rot.transpose(1,2)\nhf_keys_rot = hf_keys_t_rot.transpose(1,2)\nhf_queries_rot.shape, hf_keys_rot.shape\n\n(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]))\n\n\n\nassert torch.allclose(tt_queries_rot, permute(hf_queries_rot, n_heads=num_heads))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[73], line 1\n----&gt; 1 assert torch.allclose(tt_queries_rot, permute(hf_queries_rot, n_heads=num_heads))\n\nCell In[71], line 2, in permute(w, n_heads, dim1, dim2)\n      1 def permute(w, n_heads, dim1=dim, dim2=dim):\n----&gt; 2     return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\nRuntimeError: shape '[4, 2, 2, 16]' is invalid for input of size 384\n\n\n\n\ntorch.testing.assert_close(sin.repeat(1,2), ref_sin.squeeze(0))\ntorch.testing.assert_close(cos.repeat(1,2), ref_cos.squeeze(0))\ntorch.testing.assert_close(keys_rot, ref_keys_rot)\ntorch.testing.assert_close(queries_rot, ref_queries_rot)\n\n\ndef permute(w, n_heads, dim1=dim, dim2=dim):\n    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n\nFor HF implementation to match with torchtune, we have to permute this.\n\ndim=8\nt = torch.arange(8*8).reshape(dim,dim)\nt\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31],\n        [32, 33, 34, 35, 36, 37, 38, 39],\n        [40, 41, 42, 43, 44, 45, 46, 47],\n        [48, 49, 50, 51, 52, 53, 54, 55],\n        [56, 57, 58, 59, 60, 61, 62, 63]])\n\n\n\nn_heads=2\nt_ = t.view(n_heads, dim//n_heads//2, 2, dim)\nt_.shape, t_\n\n(torch.Size([2, 2, 2, 8]),\n tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],\n           [ 8,  9, 10, 11, 12, 13, 14, 15]],\n \n          [[16, 17, 18, 19, 20, 21, 22, 23],\n           [24, 25, 26, 27, 28, 29, 30, 31]]],\n \n \n         [[[32, 33, 34, 35, 36, 37, 38, 39],\n           [40, 41, 42, 43, 44, 45, 46, 47]],\n \n          [[48, 49, 50, 51, 52, 53, 54, 55],\n           [56, 57, 58, 59, 60, 61, 62, 63]]]]))\n\n\n\nt_t = t_.transpose(1,2)\nt_t.transpose(1,2).shape, t_t\n\n(torch.Size([2, 2, 2, 8]),\n tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],\n           [16, 17, 18, 19, 20, 21, 22, 23]],\n \n          [[ 8,  9, 10, 11, 12, 13, 14, 15],\n           [24, 25, 26, 27, 28, 29, 30, 31]]],\n \n \n         [[[32, 33, 34, 35, 36, 37, 38, 39],\n           [48, 49, 50, 51, 52, 53, 54, 55]],\n \n          [[40, 41, 42, 43, 44, 45, 46, 47],\n           [56, 57, 58, 59, 60, 61, 62, 63]]]]))\n\n\n\nt_t.reshape(dim,dim)\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [24, 25, 26, 27, 28, 29, 30, 31],\n        [32, 33, 34, 35, 36, 37, 38, 39],\n        [48, 49, 50, 51, 52, 53, 54, 55],\n        [40, 41, 42, 43, 44, 45, 46, 47],\n        [56, 57, 58, 59, 60, 61, 62, 63]])\n\n\nPermuting on columns.\n\nt.T\n\ntensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n        [ 1,  9, 17, 25, 33, 41, 49, 57],\n        [ 2, 10, 18, 26, 34, 42, 50, 58],\n        [ 3, 11, 19, 27, 35, 43, 51, 59],\n        [ 4, 12, 20, 28, 36, 44, 52, 60],\n        [ 5, 13, 21, 29, 37, 45, 53, 61],\n        [ 6, 14, 22, 30, 38, 46, 54, 62],\n        [ 7, 15, 23, 31, 39, 47, 55, 63]])\n\n\n\npermute(t.T, n_heads, dim, dim)\n\ntensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n        [ 2, 10, 18, 26, 34, 42, 50, 58],\n        [ 1,  9, 17, 25, 33, 41, 49, 57],\n        [ 3, 11, 19, 27, 35, 43, 51, 59],\n        [ 4, 12, 20, 28, 36, 44, 52, 60],\n        [ 6, 14, 22, 30, 38, 46, 54, 62],\n        [ 5, 13, 21, 29, 37, 45, 53, 61],\n        [ 7, 15, 23, 31, 39, 47, 55, 63]])\n\n\n\npermute(t.T, n_heads, dim, dim).T\n\ntensor([[ 0,  2,  1,  3,  4,  6,  5,  7],\n        [ 8, 10,  9, 11, 12, 14, 13, 15],\n        [16, 18, 17, 19, 20, 22, 21, 23],\n        [24, 26, 25, 27, 28, 30, 29, 31],\n        [32, 34, 33, 35, 36, 38, 37, 39],\n        [40, 42, 41, 43, 44, 46, 45, 47],\n        [48, 50, 49, 51, 52, 54, 53, 55],\n        [56, 58, 57, 59, 60, 62, 61, 63]])\n\n\nLet’s add a batch dimension.\n\ndim=8\nbs=2\nt = torch.arange(dim*dim*bs).reshape(bs,dim,dim)\nt\n\ntensor([[[  0,   1,   2,   3,   4,   5,   6,   7],\n         [  8,   9,  10,  11,  12,  13,  14,  15],\n         [ 16,  17,  18,  19,  20,  21,  22,  23],\n         [ 24,  25,  26,  27,  28,  29,  30,  31],\n         [ 32,  33,  34,  35,  36,  37,  38,  39],\n         [ 40,  41,  42,  43,  44,  45,  46,  47],\n         [ 48,  49,  50,  51,  52,  53,  54,  55],\n         [ 56,  57,  58,  59,  60,  61,  62,  63]],\n\n        [[ 64,  65,  66,  67,  68,  69,  70,  71],\n         [ 72,  73,  74,  75,  76,  77,  78,  79],\n         [ 80,  81,  82,  83,  84,  85,  86,  87],\n         [ 88,  89,  90,  91,  92,  93,  94,  95],\n         [ 96,  97,  98,  99, 100, 101, 102, 103],\n         [104, 105, 106, 107, 108, 109, 110, 111],\n         [112, 113, 114, 115, 116, 117, 118, 119],\n         [120, 121, 122, 123, 124, 125, 126, 127]]])\n\n\n\nn_heads=2\nt_ = t.view(bs, n_heads, dim//n_heads//2, 2, dim)\nt_.shape, t_\n\n(torch.Size([2, 2, 2, 2, 8]),\n tensor([[[[[  0,   1,   2,   3,   4,   5,   6,   7],\n            [  8,   9,  10,  11,  12,  13,  14,  15]],\n \n           [[ 16,  17,  18,  19,  20,  21,  22,  23],\n            [ 24,  25,  26,  27,  28,  29,  30,  31]]],\n \n \n          [[[ 32,  33,  34,  35,  36,  37,  38,  39],\n            [ 40,  41,  42,  43,  44,  45,  46,  47]],\n \n           [[ 48,  49,  50,  51,  52,  53,  54,  55],\n            [ 56,  57,  58,  59,  60,  61,  62,  63]]]],\n \n \n \n         [[[[ 64,  65,  66,  67,  68,  69,  70,  71],\n            [ 72,  73,  74,  75,  76,  77,  78,  79]],\n \n           [[ 80,  81,  82,  83,  84,  85,  86,  87],\n            [ 88,  89,  90,  91,  92,  93,  94,  95]]],\n \n \n          [[[ 96,  97,  98,  99, 100, 101, 102, 103],\n            [104, 105, 106, 107, 108, 109, 110, 111]],\n \n           [[112, 113, 114, 115, 116, 117, 118, 119],\n            [120, 121, 122, 123, 124, 125, 126, 127]]]]]))\n\n\n\nt_t = t_.transpose(1,2)\nt_t.transpose(1,2).shape, t_t\n\n(torch.Size([2, 2, 2, 2, 8]),\n tensor([[[[[  0,   1,   2,   3,   4,   5,   6,   7],\n            [  8,   9,  10,  11,  12,  13,  14,  15]],\n \n           [[ 32,  33,  34,  35,  36,  37,  38,  39],\n            [ 40,  41,  42,  43,  44,  45,  46,  47]]],\n \n \n          [[[ 16,  17,  18,  19,  20,  21,  22,  23],\n            [ 24,  25,  26,  27,  28,  29,  30,  31]],\n \n           [[ 48,  49,  50,  51,  52,  53,  54,  55],\n            [ 56,  57,  58,  59,  60,  61,  62,  63]]]],\n \n \n \n         [[[[ 64,  65,  66,  67,  68,  69,  70,  71],\n            [ 72,  73,  74,  75,  76,  77,  78,  79]],\n \n           [[ 96,  97,  98,  99, 100, 101, 102, 103],\n            [104, 105, 106, 107, 108, 109, 110, 111]]],\n \n \n          [[[ 80,  81,  82,  83,  84,  85,  86,  87],\n            [ 88,  89,  90,  91,  92,  93,  94,  95]],\n \n           [[112, 113, 114, 115, 116, 117, 118, 119],\n            [120, 121, 122, 123, 124, 125, 126, 127]]]]]))\n\n\n\nt_t.reshape(bs,dim,dim)\n\ntensor([[[  0,   1,   2,   3,   4,   5,   6,   7],\n         [  8,   9,  10,  11,  12,  13,  14,  15],\n         [ 32,  33,  34,  35,  36,  37,  38,  39],\n         [ 40,  41,  42,  43,  44,  45,  46,  47],\n         [ 16,  17,  18,  19,  20,  21,  22,  23],\n         [ 24,  25,  26,  27,  28,  29,  30,  31],\n         [ 48,  49,  50,  51,  52,  53,  54,  55],\n         [ 56,  57,  58,  59,  60,  61,  62,  63]],\n\n        [[ 64,  65,  66,  67,  68,  69,  70,  71],\n         [ 72,  73,  74,  75,  76,  77,  78,  79],\n         [ 96,  97,  98,  99, 100, 101, 102, 103],\n         [104, 105, 106, 107, 108, 109, 110, 111],\n         [ 80,  81,  82,  83,  84,  85,  86,  87],\n         [ 88,  89,  90,  91,  92,  93,  94,  95],\n         [112, 113, 114, 115, 116, 117, 118, 119],\n         [120, 121, 122, 123, 124, 125, 126, 127]]])\n\n\nPermuting on columns.\n\nt.transpose(1,2)\n\ntensor([[[  0,   8,  16,  24,  32,  40,  48,  56],\n         [  1,   9,  17,  25,  33,  41,  49,  57],\n         [  2,  10,  18,  26,  34,  42,  50,  58],\n         [  3,  11,  19,  27,  35,  43,  51,  59],\n         [  4,  12,  20,  28,  36,  44,  52,  60],\n         [  5,  13,  21,  29,  37,  45,  53,  61],\n         [  6,  14,  22,  30,  38,  46,  54,  62],\n         [  7,  15,  23,  31,  39,  47,  55,  63]],\n\n        [[ 64,  72,  80,  88,  96, 104, 112, 120],\n         [ 65,  73,  81,  89,  97, 105, 113, 121],\n         [ 66,  74,  82,  90,  98, 106, 114, 122],\n         [ 67,  75,  83,  91,  99, 107, 115, 123],\n         [ 68,  76,  84,  92, 100, 108, 116, 124],\n         [ 69,  77,  85,  93, 101, 109, 117, 125],\n         [ 70,  78,  86,  94, 102, 110, 118, 126],\n         [ 71,  79,  87,  95, 103, 111, 119, 127]]])\n\n\n\npermute(t.T, n_heads, dim, dim)\n\ntensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n        [ 2, 10, 18, 26, 34, 42, 50, 58],\n        [ 1,  9, 17, 25, 33, 41, 49, 57],\n        [ 3, 11, 19, 27, 35, 43, 51, 59],\n        [ 4, 12, 20, 28, 36, 44, 52, 60],\n        [ 6, 14, 22, 30, 38, 46, 54, 62],\n        [ 5, 13, 21, 29, 37, 45, 53, 61],\n        [ 7, 15, 23, 31, 39, 47, 55, 63]])\n\n\n\npermute(t.T, n_heads, dim, dim).T\n\ntensor([[ 0,  2,  1,  3,  4,  6,  5,  7],\n        [ 8, 10,  9, 11, 12, 14, 13, 15],\n        [16, 18, 17, 19, 20, 22, 21, 23],\n        [24, 26, 25, 27, 28, 30, 29, 31],\n        [32, 34, 33, 35, 36, 38, 37, 39],\n        [40, 42, 41, 43, 44, 46, 45, 47],\n        [48, 50, 49, 51, 52, 54, 53, 55],\n        [56, 58, 57, 59, 60, 62, 61, 63]])\n\n\n\npermute(t, 2, dim, dim)\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [24, 25, 26, 27, 28, 29, 30, 31],\n        [32, 33, 34, 35, 36, 37, 38, 39],\n        [48, 49, 50, 51, 52, 53, 54, 55],\n        [40, 41, 42, 43, 44, 45, 46, 47],\n        [56, 57, 58, 59, 60, 61, 62, 63]])"
  },
  {
    "objectID": "blog/posts/2023-02-20-MNIST_base/index.html",
    "href": "blog/posts/2023-02-20-MNIST_base/index.html",
    "title": "MNIST base",
    "section": "",
    "text": "MNIST Project\nIn this blog, we will attempt to predict MNIST digits with very simple approach. Instead of using neural networks, we will find a mean of target images and compare an image with that. If the image is similar to one of our means, that’s our prediction.\nThis is part of Further Research from fastbook chapter 4.\nWe import libraries. Even though we’re not going to be using fastai’s neural network, it’s just easy to import everything rather than trying to figure out individual libraries and functions I need.\n\nfrom fastai.vision.all import *\n\n\nmatplotlib.rc('image', cmap='Greys')\n\n\npath = untar_data(URLs.MNIST)\nPath.BASE_PATH = path\npath.ls()\n\n(#2) [Path('training'),Path('testing')]\n\n\n\n(path/'training').ls()\n\n(#10) [Path('training/7'),Path('training/8'),Path('training/5'),Path('training/6'),Path('training/9'),Path('training/3'),Path('training/2'),Path('training/0'),Path('training/4'),Path('training/1')]\n\n\nI looked at the number of files for each number, and they are not the same.\n\n{n: len((path/'training'/str(n)).ls()) for n in range(10)}\n\n{0: 5923,\n 1: 6742,\n 2: 5958,\n 3: 6131,\n 4: 5842,\n 5: 5421,\n 6: 5918,\n 7: 6265,\n 8: 5851,\n 9: 5949}\n\n\nSince these are paths for images we have to convert them into pytorch tensors. We can use python list to calculate means and our predictions, but pytorch provides us with broadcasting ability. This speeds up a lot. We could’ve used numpy as well here since we’re not using GPU or calculating gradients.\nFirst, we turn image paths into numbers and return as L, which is an upgraded version of list.\n\ndef create_images(path):\n    return L(PILImage.create(x) for x in path.ls())\n\n\nones = create_images(path/'training'/'1')\nim1 = ones[0]\nim1\n\n\n\n\n\n\n\n\nWe need to turn them into tensors for speed. However, we have to check the shape of tensors to make sure they are broadcastable and the shape is one we’re expecting.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs])\n\n\nstacked_ones = create_tensor_imgs(ones)\nstacked_ones[0].shape\n\ntorch.Size([28, 28, 3])\n\n\n\nstacked_ones.shape\n\ntorch.Size([6742, 28, 28, 3])\n\n\nLooking at the shape of an image, it is interesting to find out that its shape is [28, 28, 3] (y, x, and color values). I was expecting to see [28, 28] (y and x values) because it is a black and white image. So, let’s see what’s going on here.\n\narray(im1)[10:15, 12:15]\n\narray([[[  0,   0,   0],\n        [221, 221, 221],\n        [251, 251, 251]],\n\n       [[ 64,  64,  64],\n        [236, 236, 236],\n        [251, 251, 251]],\n\n       [[127, 127, 127],\n        [251, 251, 251],\n        [251, 251, 251]],\n\n       [[127, 127, 127],\n        [251, 251, 251],\n        [251, 251, 251]],\n\n       [[128, 128, 128],\n        [253, 253, 253],\n        [253, 253, 253]]], dtype=uint8)\n\n\nSo, it is just filled with the same numbers per each row. We just need one column, so we can simply take a mean of last rank.\n\narray(im1)[10:15, 12:15].mean(-1)\n\narray([[  0., 221., 251.],\n       [ 64., 236., 251.],\n       [127., 251., 251.],\n       [127., 251., 251.],\n       [128., 253., 253.]])\n\n\nSo, here is the updated version of create_tensor_imgs that calculates a mean so that we can get rid of the last rank of our tensors.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)\n\n\nstacked_ones = create_tensor_imgs(ones)\nstacked_ones[0].shape\n\ntorch.Size([28, 28])\n\n\n\narray(stacked_ones[0])[10:15, 12:15]\n\narray([[  0., 221., 251.],\n       [ 64., 236., 251.],\n       [127., 251., 251.],\n       [127., 251., 251.],\n       [128., 253., 253.]], dtype=float32)\n\n\nLooks good. Now, we want those pixel values to be between 0 and 1 so that it is easier to train neural networks. However, we do not need to do it here as we’re simply finding a mean. It still gives the same result in the end, so we’ll divide them by 255.\n\ndef create_tensor_imgs(imgs):\n    \"Convert a list of images into a tensor\"\n    return torch.stack([tensor(im) for im in imgs]).mean(-1, dtype=torch.float)/255\n\n\nstacked_ones = create_tensor_imgs(ones)\narray(stacked_ones[0])[10:15, 12:15]\n\narray([[0.        , 0.8666667 , 0.9843137 ],\n       [0.2509804 , 0.9254902 , 0.9843137 ],\n       [0.49803922, 0.9843137 , 0.9843137 ],\n       [0.49803922, 0.9843137 , 0.9843137 ],\n       [0.5019608 , 0.99215686, 0.99215686]], dtype=float32)\n\n\nLet’s take a look at its mean.\n\nmean1 = stacked_ones.mean(0)\nshow_image(mean1);\n\n\n\n\n\n\n\n\nWell, that looks like 1. Now, we can use this to predict whether an image is 1 or not. This will give us a good baseline. If our deep learning model performs worse than this baseline, we know the model is not performing well.\nNow we have a mean for all images of 1, we can do the same for other numbers as well. It would be easier to define a function that does all the steps.\n\ndef stacked_nums(path):\n    \"Converts path into stacked tensors.\"\n    imgs = create_images(path)\n    return create_tensor_imgs(imgs)\n\n\nstacked_zeros = stacked_nums(path/'training'/'0')\nstacked_twos = stacked_nums(path/'training'/'2')\nstacked_threes = stacked_nums(path/'training'/'3')\nstacked_fours = stacked_nums(path/'training'/'4')\nstacked_fives = stacked_nums(path/'training'/'5')\nstacked_sixs = stacked_nums(path/'training'/'6')\nstacked_sevens = stacked_nums(path/'training'/'7')\nstacked_eights = stacked_nums(path/'training'/'8')\nstacked_nines = stacked_nums(path/'training'/'9')\n\nmean0 = stacked_zeros.mean(0)\nmean2 = stacked_twos.mean(0)\nmean3 = stacked_threes.mean(0)\nmean4 = stacked_fours.mean(0)\nmean5 = stacked_fives.mean(0)\nmean6 = stacked_sixs.mean(0)\nmean7 = stacked_sevens.mean(0)\nmean8 = stacked_eights.mean(0)\nmean9 = stacked_nines.mean(0)\n\n\nall_means = torch.stack([mean0, mean1, mean2, mean3, mean4, mean5, mean6, mean7, mean8, mean9])\n\n\nshow_images(all_means, \n            nrows=2,\n            titles=[0,1,2,3,4,5,6,7,8,9])\n\n\n\n\n\n\n\n\nThere we go, we got all the means for each number. Let’s do the same for testing data.\n\ntst_0_tens = stacked_nums(path/'testing'/'0')\ntst_1_tens = stacked_nums(path/'testing'/'1')\ntst_2_tens = stacked_nums(path/'testing'/'2')\ntst_3_tens = stacked_nums(path/'testing'/'3')\ntst_4_tens = stacked_nums(path/'testing'/'4')\ntst_5_tens = stacked_nums(path/'testing'/'5')\ntst_6_tens = stacked_nums(path/'testing'/'6')\ntst_7_tens = stacked_nums(path/'testing'/'7')\ntst_8_tens = stacked_nums(path/'testing'/'8')\ntst_9_tens = stacked_nums(path/'testing'/'9')\n\ntst_tens = L([tst_0_tens, tst_1_tens, tst_2_tens, tst_3_tens, tst_4_tens, \n              tst_5_tens, tst_6_tens, tst_7_tens, tst_8_tens, tst_9_tens])\ntst_1_tens.shape, tst_9_tens.shape\n\n(torch.Size([1135, 28, 28]), torch.Size([1009, 28, 28]))\n\n\nSo, now that our data is ready, let’s figure out how to make a prediction. One way to make a prediction is to calculate the difference between a given image and our means. For each mean, we will calculate the difference, which is a loss. Whatever has the minimum loss is our prediction. Let’s use 4 as an example here.\n\nfour = tst_4_tens[0]\nshow_image(four);\n\n\n\n\n\n\n\n\nWe’re calculating mean absolute error (MAE) to find the loss. We need to use absolute values because we do not want negative difference to cancel out with positive one.\n\n(four - mean0).abs().mean((-1,-2))\n\ntensor(0.1861)\n\n\n\ndef mnist_distance(x):\n    \"Calculate Mean Absolute Error for each mean.\"\n    return (x - all_means).abs().mean((-1, -2))\n\n\npreds = mnist_distance(four)\npreds\n\ntensor([0.1861, 0.1281, 0.1543, 0.1474, 0.1051, 0.1423, 0.1446, 0.1271, 0.1571,\n        0.1132])\n\n\nFrom these numbers, we are looking for an index of the smallest one. The smallest distance means a given image is the closest to that mean.\n\ndef mk_pred(x):\n    \"Make a prediction from a given image.\"\n    dist = mnist_distance(x)\n    return dist.argmin(0)\n\n\nmk_pred(four)\n\ntensor(4)\n\n\nThat’s good. Let’s try it on all the images and find out the accuracy of all of them. One problem is that we cannot simply use braodcasting because of their shapes. Let’s take a look at their shapes.\n\ntst_4_tens.shape, all_means.shape\n\n(torch.Size([982, 28, 28]), torch.Size([10, 28, 28]))\n\n\nFirst rank of tst_4_tens and all_means are different and neither is 1. That’s a problem, so we can add a dimension over there. We can either use slice or view.\n\nmnist_distance(tst_4_tens[0])\n\ntensor([0.1861, 0.1281, 0.1543, 0.1474, 0.1051, 0.1423, 0.1446, 0.1271, 0.1571,\n        0.1132])\n\n\n\nmnist_distance(tst_4_tens[:,None,::])[0]\n\ntensor([0.1861, 0.1281, 0.1543, 0.1474, 0.1051, 0.1423, 0.1446, 0.1271, 0.1571,\n        0.1132])\n\n\n\nmnist_distance(tst_4_tens.view(-1, 1, 28, 28))[0]\n\ntensor([0.1861, 0.1281, 0.1543, 0.1474, 0.1051, 0.1423, 0.1446, 0.1271, 0.1571,\n        0.1132])\n\n\nNow we have a way of finding mnist_distance efficiently, we have to tweak mk_pred a bit to account for it.\n\ndef mk_pred(ts):\n    \"Make predictions from tensors.\"\n    ranks = len(ts.shape)\n    if ranks == 2:\n        dist = mnist_distance(ts)\n        return dist.argmin(0)\n    elif ranks == 3:\n        dist = mnist_distance(ts.view(-1, 1, 28, 28))\n        return dist.argmin(1)\n    else:\n        raise Exception(f'Invalid ranks of the tensor. Got {ranks} when 2 or 3 are needed.')\n\n\n(mk_pred(tst_4_tens) == 4)[:5]\n\ntensor([True, True, True, True, True])\n\n\n\n[mk_pred(x)==4 for x in tst_4_tens[:5]]\n\n[tensor(True), tensor(True), tensor(True), tensor(True), tensor(True)]\n\n\nWith this new function, we can find the accuracy of the test set of numbers.\n\n(mk_pred(tst_4_tens) == 4).float().mean()\n\ntensor(0.6680)\n\n\n\n(mk_pred(tst_0_tens) == 0).float().mean()\n\ntensor(0.8153)\n\n\n\ndef get_acc(ts, targ):\n    \"Find an accuracy.\"\n    return (mk_pred(ts) == targ).float().mean() * 100\n\n\nget_acc(tst_0_tens, 0)\n\ntensor(81.5306)\n\n\nSo, here is the accuracy of all the numbers.\n\nL(get_acc(ts, targ) for ts, targ in zip(tst_tens, range(10)))\n\n(#10) [tensor(81.5306),tensor(99.8238),tensor(42.3450),tensor(60.8911),tensor(66.8024),tensor(32.6233),tensor(78.7056),tensor(76.4591),tensor(44.2505),tensor(77.6016)]\n\n\n\nacc_dict = {targ:array(get_acc(ts, targ)) for ts, targ in zip(tst_tens, range(10))}\nacc_dict\n\n{0: array(81.53061, dtype=float32),\n 1: array(99.82379, dtype=float32),\n 2: array(42.34496, dtype=float32),\n 3: array(60.89109, dtype=float32),\n 4: array(66.802444, dtype=float32),\n 5: array(32.623318, dtype=float32),\n 6: array(78.705635, dtype=float32),\n 7: array(76.459145, dtype=float32),\n 8: array(44.25051, dtype=float32),\n 9: array(77.601585, dtype=float32)}\n\n\nWe can create a graph and look at it visually.\n\ndf = pd.DataFrame(acc_dict, index=range(1))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n81.530609\n99.823792\n42.344959\n60.89109\n66.802444\n32.623318\n78.705635\n76.459145\n44.250511\n77.601585\n\n\n\n\n\n\n\n\ndf.plot.bar();\n\n\n\n\n\n\n\n\nSo, our model is very good at predicting 1, but not as good at predicting 5.\n\nsum(acc_dict.values())/10\n\n66.1033088684082\n\n\nOverall, we have 66% accuracy. That’s not bad for a starting point. Now, our next step is to build a better model using neural networks.",
    "crumbs": [
      "Home",
      "Blog",
      "MNIST base"
    ]
  },
  {
    "objectID": "blog/posts/2023-03-05-Fruit_Multi_pt2/index.html",
    "href": "blog/posts/2023-03-05-Fruit_Multi_pt2/index.html",
    "title": "Fruit Multi-Classifier pt.2 Deployment",
    "section": "",
    "text": "Fruit Multi-Classifier pt. 2 Deployment\nIn this blog, we will deploy our model into Hugging Face Space using gradio. I have written a detailed blog here on how to deploy. This blog omits some steps.\n\n\n\nImage of an apple, a pear, and an orange\n\n\nSo, we first need to install gradio.\n\n!pip install gradio\n\nRequirement already satisfied: gradio in /usr/local/lib/python3.9/dist-packages (3.20.0)\nRequirement already satisfied: orjson in /usr/local/lib/python3.9/dist-packages (from gradio) (3.8.7)\nRequirement already satisfied: altair&gt;=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\nRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from gradio) (5.3.1)\nRequirement already satisfied: pycryptodome in /usr/local/lib/python3.9/dist-packages (from gradio) (3.17)\nRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gradio) (1.23.1)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio) (1.9.1)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.9/dist-packages (from gradio) (0.3.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from gradio) (3.8.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (9.2.0)\nRequirement already satisfied: markdown-it-py[linkify]&gt;=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.2.0)\nRequirement already satisfied: fastapi in /usr/local/lib/python3.9/dist-packages (from gradio) (0.92.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.3.0)\nRequirement already satisfied: uvicorn in /usr/local/lib/python3.9/dist-packages (from gradio) (0.20.0)\nRequirement already satisfied: python-multipart in /usr/local/lib/python3.9/dist-packages (from gradio) (0.0.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gradio) (2.28.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio) (2022.5.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.5.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.4.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\nRequirement already satisfied: mdit-py-plugins&lt;=0.3.3 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.3.3)\nRequirement already satisfied: httpx in /usr/local/lib/python3.9/dist-packages (from gradio) (0.23.3)\nRequirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.1)\nRequirement already satisfied: websockets&gt;=10.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (10.4)\nRequirement already satisfied: aiofiles in /usr/local/lib/python3.9/dist-packages (from gradio) (23.1.0)\nRequirement already satisfied: pydub in /usr/local/lib/python3.9/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair&gt;=4.2.0-&gt;gradio) (0.12.0)\nRequirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair&gt;=4.2.0-&gt;gradio) (0.4)\nRequirement already satisfied: jsonschema&gt;=3.0 in /usr/local/lib/python3.9/dist-packages (from altair&gt;=4.2.0-&gt;gradio) (4.7.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]&gt;=2.0.0-&gt;gradio) (0.1.2)\nRequirement already satisfied: linkify-it-py&lt;3,&gt;=1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]&gt;=2.0.0-&gt;gradio) (2.0.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas-&gt;gradio) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas-&gt;gradio) (2022.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;gradio) (18.2.0)\nRequirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;gradio) (2.1.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;gradio) (6.0.2)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;gradio) (4.0.2)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;gradio) (1.3.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;gradio) (1.2.0)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;gradio) (1.7.2)\nRequirement already satisfied: starlette&lt;0.26.0,&gt;=0.25.0 in /usr/local/lib/python3.9/dist-packages (from fastapi-&gt;gradio) (0.25.0)\nRequirement already satisfied: httpcore&lt;0.17.0,&gt;=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx-&gt;gradio) (0.16.3)\nRequirement already satisfied: rfc3986[idna2008]&lt;2,&gt;=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx-&gt;gradio) (1.5.0)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx-&gt;gradio) (2019.11.28)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx-&gt;gradio) (1.2.0)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;gradio) (3.0.9)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;gradio) (1.4.3)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;gradio) (21.3)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;gradio) (4.34.4)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;gradio) (0.11.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/lib/python3/dist-packages (from requests-&gt;gradio) (2.8)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;gradio) (1.26.10)\nRequirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn-&gt;gradio) (8.1.3)\nRequirement already satisfied: h11&gt;=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn-&gt;gradio) (0.14.0)\nRequirement already satisfied: anyio&lt;5.0,&gt;=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore&lt;0.17.0,&gt;=0.15.0-&gt;httpx-&gt;gradio) (3.6.1)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema&gt;=3.0-&gt;altair&gt;=4.2.0-&gt;gradio) (0.18.1)\nRequirement already satisfied: uc-micro-py in /usr/local/lib/python3.9/dist-packages (from linkify-it-py&lt;3,&gt;=1-&gt;markdown-it-py[linkify]&gt;=2.0.0-&gt;gradio) (1.0.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/lib/python3/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;gradio) (1.14.0)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\nWe import fastai and gradio. We also need get_y function that we used for datablock to train our model.\nAnd we load our learner and check if everything works the same way as it did.\n\nlearn.dls.vocab\n\n['apple', 'orange', 'pear', 'random images']\n\n\n\npath = Path()\npath.ls()\n\n(#17) [Path('app.py'),Path('apple.jpg'),Path('random images.jpg'),Path('apple and orange.jpg'),Path('flagged'),Path('Deployment.ipynb'),Path('export.pkl'),Path('apple and pear.jpg'),Path('pear and orange.jpg'),Path('orange.jpg')...]\n\n\n\nim = PILImage.create(path/'apple and orange.jpg')\nim.to_thumb(256, 256)\n\n\n\n\n\n\n\n\n\nlearn.predict(im)\n\n\n\n\n\n\n\n\n((#2) ['apple','orange'],\n TensorBase([ True,  True, False, False]),\n TensorBase([0.9994, 0.9993, 0.0043, 0.0011]))\n\n\nLooks like it’s doing good as we trained it.\nNext, we want to get the vocab as categories to predict and use our learner to predict based on an input image.\n\nclassify_images(im)\n\n\n\n\n\n\n\n\n{'apple': 0.9994100332260132,\n 'orange': 0.9992908239364624,\n 'pear': 0.004278946202248335,\n 'random images': 0.0011345148086547852}\n\n\nThis is how we create gradio interface and try it before deploying into Hugging Face Space.\nAnd we will use nbdev to turn our notebook into a python script. By using nbdev, we do not have to copy and paste each cell into a file.\n\nimport nbdev\n\n\nnbdev.export.nb_export('Deployment.ipynb', '.', name='app')\n\n\n!cat app.py\n\n# AUTOGENERATED! DO NOT EDIT! File to edit: Deployment.ipynb.\n\n# %% auto 0\n__all__ = ['learn', 'categories', 'image', 'label', 'examples', 'inf', 'get_y', 'classify_images']\n\n# %% Deployment.ipynb 2\nfrom fastai.vision.all import *\nimport gradio as gr\n\ndef get_y(path):\n    return parent_label(path).split(' and ')\n\n# %% Deployment.ipynb 3\nlearn = load_learner('export.pkl')\n\n# %% Deployment.ipynb 8\ncategories = learn.dls.vocab\ndef classify_images(img):\n    \"\"\"classifies images and returns the probabilities on each categories.\"\"\"\n    pred, pred_idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float, probs)))\n\n# %% Deployment.ipynb 10\nimage = gr.inputs.Image(shape=(192, 192))\nlabel = gr.outputs.Label()\n# If you have more or less examples, edit this list.\nexamples = ['apple.jpg', 'orange.jpg', 'apple and orange.jpg', \n            'pear and orange.jpg', 'apple and pear.jpg', 'apple and pear and orange.jpg',\n            'random images.jpg']\n\ninf = gr.Interface(fn=classify_images, inputs=image, outputs=label, examples=examples)\ninf.launch(inline=False)\n\n\nThat’s it. To find out next steps, check out my detailed blog after nbdev. It goes over creating a requirements.txt file and uploading files into Hugging Face Space using git.",
    "crumbs": [
      "Home",
      "Blog",
      "Fruit Multi-Classifier pt.2 Deployment"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-03-instruct_gpt/index.html",
    "href": "blog/posts/2025-06-03-instruct_gpt/index.html",
    "title": "How InstructGPT is trained",
    "section": "",
    "text": "In machine learning, bigger models usually perform better. GPT-3 was a scaled-up version of GPT-2 with more parameters and data, and it significantly outperformed its predecessor. This seemed to confirm that model size matters. But when I read a paper called “Training language models to follow instructions with human feedback” (https://arxiv.org/pdf/2203.02155) also known as the InstructGPT paper, I learned something interesting - a 1.3 billion parameter model trained with human feedback was preferred by evaluators over the 175 billion parameter GPT-3, despite being 100 times smaller. The model also learned to follow instructions generally and could apply this skill to tasks it wasn’t explicitly trained on, like coding and different languages. This made me curious about how these models actually learn and why human feedback could be so effective.\n Training a large language model is like training puppies. Image generated with OpenAI.\n\n\nTraining useful large language models like ChatGPT involves three main steps. It’s similar to training puppies.\n\n\nFirst, models are pretrained on massive amounts of text data from the internet. During this step, the model learns general knowledge by predicting the next token in a sentence. For example, it learns that ice is cold or that flowers smell nice. The model can complete sentences, but it doesn’t know how to follow instructions. This is like curious puppies learning about the world through their senses - they develop good knowledge about different objects, but they don’t understand human directions yet.\nStep 2: Supervised Fine-tuning Next, the model learns to follow instructions through supervised fine-tuning. Instead of random web text, it trains on datasets like FLAN or T0, which contain prompts paired with target responses. Now the model can follow basic instructions. However, it’s not very helpful or safe. If someone asks harmful questions, the model might provide dangerous answers. This is like puppies learning to fetch sticks from different trainers, but they also follow bad instructions like sneaking into stores for snacks.\nStep 3: Reinforcement Learning from Human Feedback (RLHF) Finally, the model trains with human feedback to learn preferred behaviors. It receives rewards for responses that humans prefer. This is like training puppies with treats - they get rewards when their actions align with the trainer’s intent. We will dig deeper into how this works in this article.\n A figure from the papre that describes the three steps to train.\n\n\n\n\nRLHF wasn’t invented for this paper. It was originally developed around 2017 for training robots in simulated environments and Atari games. For language models, researchers first applied it to specific tasks like text summarization in 2019-2020. What made the InstructGPT paper groundbreaking was applying RLHF to a broad range of instruction-following tasks at scale.\nThe researchers used a three-step process to train their models:\nStep 1: Collect demonstration data and train a supervised policy\nThe researchers hired about 40 labelers from diverse backgrounds to create high-quality training examples. These labelers wrote prompts and demonstrated good responses, creating a dataset of 13,000 training examples. The team then fine-tuned GPT-3 using supervised learning on these demonstrations.\nThis is like having experienced dog trainers show puppies exactly how to behave in different situations. Instead of the puppies figuring things out on their own, they learn from watching good examples.\nStep 2: Collect comparison data and train a reward model\nNext, the researchers collected 33,000 prompts and had labelers rank between 4-9 responses for each prompt from best to worst. They used this data to train a separate reward model that learned to predict human preferences. Interestingly, they found that 6 billion parameter reward models worked better than 175 billion parameter ones - the larger models were too unstable during training.\nThe reward model uses a specific loss function:\nloss(θ) = -1/(K choose 2) * E(x,yw,yl)~D[log(σ(rθ(x,yw) - rθ(x,yl)))]\nWhere σ is the sigmoid function, rθ(x,y) is the reward score for prompt x and response y, yw is the preferred response, and yl is the less preferred one. The 1/(K choose 2) term normalizes for the different number of comparison pairs each prompt generates. The log(σ(…)) term represents the log likelihood of observing the human preference.\nTo prevent overfitting, they processed all comparison pairs from the same prompt together as one batch element, rather than shuffling them randomly.\nThis is like teaching our puppies’ trainer to recognize good behavior by comparing examples and learning what makes one action better than another.\nStep 3: Optimize a policy against the reward model using reinforcement learning\nFinally, they used the reward model to train the language model using “PPO-ptx” (Proximal Policy Optimization with pretraining mix). The model generates responses to prompts, gets scored by the reward model, and learns to maximize those scores through reinforcement learning.\nThe objective function for PPO-ptx combines three terms:\nobjective(φ) = E(x,y)~Dπφ^RL [rφ(x,y) - β*log(πφ^RL(y|x)/πSFT(y|x))] + γ*E(x)~Dpretrain[log πφ^RL(x)]\nNotations: - φ = parameters of the current RL model - πφ^RL = the current RL policy (model being trained) - πSFT = the supervised fine-tuned model from Step 1 - x = input prompt, y = generated response - Dπφ^RL = distribution of (prompt, response) pairs from the current RL model - Dpretrain = original pretraining data distribution\nLet’s break down each term:\nFirst term - rφ(x,y): Uses the reward model trained in Step 2 to score how good response y is for prompt x. This encourages generating responses that humans prefer.\nSecond term - KL divergence penalty: Measures how much the current model’s response distribution differs from the supervised model. KL divergence quantifies the “distance” between two probability distributions. When πφ^RL(y|x) becomes very different from πSFT(y|x), this penalty becomes large. β controls how much we penalize deviation.\nThird term - Pretraining preservation: Takes text sequences x from the original pretraining data and evaluates how well the current model predicts them token by token. The model computes log πφ^RL(x) = Σ log P(token_i | previous tokens) for each sequence. Since log probabilities are always negative, this term pulls the objective down. γ controls the strength of this regularization to keep language modeling abilities intact.\nThis is like training puppies with treats (rewards) while making sure they don’t forget their basic obedience training (KL penalty) and still remember how to be dogs (pretraining preservation).\n\n\n\nThe results were impressive. The 1.3B parameter InstructGPT model was preferred by human evaluators 85% of the time over the 175B GPT-3 model. This was surprising because the InstructGPT model was 100 times smaller.\nTo validate their approach, the researchers tested with “held-out” labelers - people who didn’t create any training data. These new evaluators still preferred InstructGPT at similar rates, showing the model wasn’t just overfitting to the preferences of the original labelers.\nThe model also showed improvements in truthfulness. On the TruthfulQA benchmark, InstructGPT generated truthful answers about twice as often as GPT-3. It also hallucinated less on closed-domain tasks, making up facts only 21% of the time compared to GPT-3’s 41%.\n\n\n\nDespite these improvements, InstructGPT still had several limitations. The model could still generate harmful content when explicitly instructed to do so. It also struggled with multiple constraints - for example, when asked to “list 10 movies made in the 1930s set in France,” it had difficulty satisfying all requirements simultaneously. This is like puppies that can follow simple commands but get confused when given multiple instructions at once.\nThe model also showed a tendency to “over-hedge” its answers. When asked simple questions, it would sometimes give unnecessarily cautious responses instead of direct answers. This happened because the labelers were trained to reward careful, humble responses. It’s like puppies that become overly hesitant because they’ve been rewarded for being cautious.\nAdditionally, the training data had limited diversity. The 40 labelers were mostly English-speaking people from the US and Southeast Asia, which may not represent all potential users of these models.\n\n\n\nThe InstructGPT paper opened up exciting possibilities for training language models. RLHF has become a standard technique for aligning AI systems with human preferences. We’ve seen this approach used in models like ChatGPT and Claude.\nWhat’s particularly interesting is how the method could scale. The researchers found that alignment was cost-effective - the human feedback training required much less compute than the original pretraining. This suggests we can make models more helpful without massive increases in computational resources.\nThe generalization to unseen domains like coding and non-English languages also hints at something important. These models might be learning general principles about following instructions rather than just memorizing specific patterns. As AI systems become more capable, understanding how to align them with human intentions becomes increasingly critical.\n\n\n\nThe InstructGPT paper showed that making AI systems better isn’t just about scaling up - it’s about aligning them with what humans actually want. By using human feedback to train models, researchers demonstrated that a much smaller model could outperform a giant one. Just like how we value puppies that are helpful, honest, and harmless over those that are simply good at hunting or looking cute, what matters most in AI systems is how well they align with human values rather than just raw capability. This work laid the foundation for the conversational AI systems we use today. As I continue learning about computational biology and AI, I find it fascinating how techniques like RLHF are helping us build AI systems that are not just more capable, but also more helpful and aligned with human values.\n Image generated with OpenAI.",
    "crumbs": [
      "Home",
      "Blog",
      "How InstructGPT is trained"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-03-instruct_gpt/index.html#how-large-language-models-are-trained",
    "href": "blog/posts/2025-06-03-instruct_gpt/index.html#how-large-language-models-are-trained",
    "title": "How InstructGPT is trained",
    "section": "",
    "text": "Training useful large language models like ChatGPT involves three main steps. It’s similar to training puppies.\n\n\nFirst, models are pretrained on massive amounts of text data from the internet. During this step, the model learns general knowledge by predicting the next token in a sentence. For example, it learns that ice is cold or that flowers smell nice. The model can complete sentences, but it doesn’t know how to follow instructions. This is like curious puppies learning about the world through their senses - they develop good knowledge about different objects, but they don’t understand human directions yet.\nStep 2: Supervised Fine-tuning Next, the model learns to follow instructions through supervised fine-tuning. Instead of random web text, it trains on datasets like FLAN or T0, which contain prompts paired with target responses. Now the model can follow basic instructions. However, it’s not very helpful or safe. If someone asks harmful questions, the model might provide dangerous answers. This is like puppies learning to fetch sticks from different trainers, but they also follow bad instructions like sneaking into stores for snacks.\nStep 3: Reinforcement Learning from Human Feedback (RLHF) Finally, the model trains with human feedback to learn preferred behaviors. It receives rewards for responses that humans prefer. This is like training puppies with treats - they get rewards when their actions align with the trainer’s intent. We will dig deeper into how this works in this article.\n A figure from the papre that describes the three steps to train.",
    "crumbs": [
      "Home",
      "Blog",
      "How InstructGPT is trained"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-03-instruct_gpt/index.html#reinforcement-learning-from-human-feedback",
    "href": "blog/posts/2025-06-03-instruct_gpt/index.html#reinforcement-learning-from-human-feedback",
    "title": "How InstructGPT is trained",
    "section": "",
    "text": "RLHF wasn’t invented for this paper. It was originally developed around 2017 for training robots in simulated environments and Atari games. For language models, researchers first applied it to specific tasks like text summarization in 2019-2020. What made the InstructGPT paper groundbreaking was applying RLHF to a broad range of instruction-following tasks at scale.\nThe researchers used a three-step process to train their models:\nStep 1: Collect demonstration data and train a supervised policy\nThe researchers hired about 40 labelers from diverse backgrounds to create high-quality training examples. These labelers wrote prompts and demonstrated good responses, creating a dataset of 13,000 training examples. The team then fine-tuned GPT-3 using supervised learning on these demonstrations.\nThis is like having experienced dog trainers show puppies exactly how to behave in different situations. Instead of the puppies figuring things out on their own, they learn from watching good examples.\nStep 2: Collect comparison data and train a reward model\nNext, the researchers collected 33,000 prompts and had labelers rank between 4-9 responses for each prompt from best to worst. They used this data to train a separate reward model that learned to predict human preferences. Interestingly, they found that 6 billion parameter reward models worked better than 175 billion parameter ones - the larger models were too unstable during training.\nThe reward model uses a specific loss function:\nloss(θ) = -1/(K choose 2) * E(x,yw,yl)~D[log(σ(rθ(x,yw) - rθ(x,yl)))]\nWhere σ is the sigmoid function, rθ(x,y) is the reward score for prompt x and response y, yw is the preferred response, and yl is the less preferred one. The 1/(K choose 2) term normalizes for the different number of comparison pairs each prompt generates. The log(σ(…)) term represents the log likelihood of observing the human preference.\nTo prevent overfitting, they processed all comparison pairs from the same prompt together as one batch element, rather than shuffling them randomly.\nThis is like teaching our puppies’ trainer to recognize good behavior by comparing examples and learning what makes one action better than another.\nStep 3: Optimize a policy against the reward model using reinforcement learning\nFinally, they used the reward model to train the language model using “PPO-ptx” (Proximal Policy Optimization with pretraining mix). The model generates responses to prompts, gets scored by the reward model, and learns to maximize those scores through reinforcement learning.\nThe objective function for PPO-ptx combines three terms:\nobjective(φ) = E(x,y)~Dπφ^RL [rφ(x,y) - β*log(πφ^RL(y|x)/πSFT(y|x))] + γ*E(x)~Dpretrain[log πφ^RL(x)]\nNotations: - φ = parameters of the current RL model - πφ^RL = the current RL policy (model being trained) - πSFT = the supervised fine-tuned model from Step 1 - x = input prompt, y = generated response - Dπφ^RL = distribution of (prompt, response) pairs from the current RL model - Dpretrain = original pretraining data distribution\nLet’s break down each term:\nFirst term - rφ(x,y): Uses the reward model trained in Step 2 to score how good response y is for prompt x. This encourages generating responses that humans prefer.\nSecond term - KL divergence penalty: Measures how much the current model’s response distribution differs from the supervised model. KL divergence quantifies the “distance” between two probability distributions. When πφ^RL(y|x) becomes very different from πSFT(y|x), this penalty becomes large. β controls how much we penalize deviation.\nThird term - Pretraining preservation: Takes text sequences x from the original pretraining data and evaluates how well the current model predicts them token by token. The model computes log πφ^RL(x) = Σ log P(token_i | previous tokens) for each sequence. Since log probabilities are always negative, this term pulls the objective down. γ controls the strength of this regularization to keep language modeling abilities intact.\nThis is like training puppies with treats (rewards) while making sure they don’t forget their basic obedience training (KL penalty) and still remember how to be dogs (pretraining preservation).",
    "crumbs": [
      "Home",
      "Blog",
      "How InstructGPT is trained"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-03-instruct_gpt/index.html#results-and-validation",
    "href": "blog/posts/2025-06-03-instruct_gpt/index.html#results-and-validation",
    "title": "How InstructGPT is trained",
    "section": "",
    "text": "The results were impressive. The 1.3B parameter InstructGPT model was preferred by human evaluators 85% of the time over the 175B GPT-3 model. This was surprising because the InstructGPT model was 100 times smaller.\nTo validate their approach, the researchers tested with “held-out” labelers - people who didn’t create any training data. These new evaluators still preferred InstructGPT at similar rates, showing the model wasn’t just overfitting to the preferences of the original labelers.\nThe model also showed improvements in truthfulness. On the TruthfulQA benchmark, InstructGPT generated truthful answers about twice as often as GPT-3. It also hallucinated less on closed-domain tasks, making up facts only 21% of the time compared to GPT-3’s 41%.",
    "crumbs": [
      "Home",
      "Blog",
      "How InstructGPT is trained"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-03-instruct_gpt/index.html#limitations",
    "href": "blog/posts/2025-06-03-instruct_gpt/index.html#limitations",
    "title": "How InstructGPT is trained",
    "section": "",
    "text": "Despite these improvements, InstructGPT still had several limitations. The model could still generate harmful content when explicitly instructed to do so. It also struggled with multiple constraints - for example, when asked to “list 10 movies made in the 1930s set in France,” it had difficulty satisfying all requirements simultaneously. This is like puppies that can follow simple commands but get confused when given multiple instructions at once.\nThe model also showed a tendency to “over-hedge” its answers. When asked simple questions, it would sometimes give unnecessarily cautious responses instead of direct answers. This happened because the labelers were trained to reward careful, humble responses. It’s like puppies that become overly hesitant because they’ve been rewarded for being cautious.\nAdditionally, the training data had limited diversity. The 40 labelers were mostly English-speaking people from the US and Southeast Asia, which may not represent all potential users of these models.",
    "crumbs": [
      "Home",
      "Blog",
      "How InstructGPT is trained"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-03-instruct_gpt/index.html#future-perspectives",
    "href": "blog/posts/2025-06-03-instruct_gpt/index.html#future-perspectives",
    "title": "How InstructGPT is trained",
    "section": "",
    "text": "The InstructGPT paper opened up exciting possibilities for training language models. RLHF has become a standard technique for aligning AI systems with human preferences. We’ve seen this approach used in models like ChatGPT and Claude.\nWhat’s particularly interesting is how the method could scale. The researchers found that alignment was cost-effective - the human feedback training required much less compute than the original pretraining. This suggests we can make models more helpful without massive increases in computational resources.\nThe generalization to unseen domains like coding and non-English languages also hints at something important. These models might be learning general principles about following instructions rather than just memorizing specific patterns. As AI systems become more capable, understanding how to align them with human intentions becomes increasingly critical.",
    "crumbs": [
      "Home",
      "Blog",
      "How InstructGPT is trained"
    ]
  },
  {
    "objectID": "blog/posts/2025-06-03-instruct_gpt/index.html#conclusion",
    "href": "blog/posts/2025-06-03-instruct_gpt/index.html#conclusion",
    "title": "How InstructGPT is trained",
    "section": "",
    "text": "The InstructGPT paper showed that making AI systems better isn’t just about scaling up - it’s about aligning them with what humans actually want. By using human feedback to train models, researchers demonstrated that a much smaller model could outperform a giant one. Just like how we value puppies that are helpful, honest, and harmless over those that are simply good at hunting or looking cute, what matters most in AI systems is how well they align with human values rather than just raw capability. This work laid the foundation for the conversational AI systems we use today. As I continue learning about computational biology and AI, I find it fascinating how techniques like RLHF are helping us build AI systems that are not just more capable, but also more helpful and aligned with human values.\n Image generated with OpenAI.",
    "crumbs": [
      "Home",
      "Blog",
      "How InstructGPT is trained"
    ]
  },
  {
    "objectID": "blog/posts/2025-01-17-how_to_solve_it/index.html",
    "href": "blog/posts/2025-01-17-how_to_solve_it/index.html",
    "title": "How to solve it",
    "section": "",
    "text": "Over the winter break, I learned about Georgy Polya’s “How to solve it” through a course. “How to solve it” is a book about problem solving. As a mathematician, Polya solved difficult math problems, and the book focuses on math. However, his problem solving techniques can apply into many other fields as well. Let’s briefly look at the technique.\n\n\n\nIn the course, we utilize four-step problem-solving frame on advent of code problems to familiarize ourselves with problem solving. In the book, there are five steps, which I found was a bit more intuitive for me. Here’s the steps from the book.\n\nGetting acquanted\nBefore we solve, we want to understand the problem clearly in a big picture. There is no need for little details now. It’s like looking at a painting in a distance. We can figure out what the painting is about.\nWorking for better understanding\nWe want to get a much deeper understanding of the problem. To the point where we can have everything related to the problem in our head without looking at the problem. We think about each component and consider how they are important for our problem.\nHunting for the helpful idea\nNow we look for ideas that could help us solve the problem. We can look for similar problems we have solved in the past. This can be very straight forward for some problems, but it may not for others. Sometimes problems are related, but hard to find the connection. For instance, some designs in engineering may be inspired from biology.\nCarrying out the plan\nWe take small steps and check for correctness. We think about it in different angles and as ourselves whether it makes sense. In the end, we have the correct solution.\nLooking back\nWe make sure the solution is correct by thinking about it in differnt ways. We try to conquer the problem by simplifying the solution and applying those helpful ideas into other problems. Like our immune system, we can solve this type of problem with ease in the future.\n\nAfter learning about the technique, I thought the steps were obvious and intuitive. I didn’t think there was anything new or surprising here. I thought to myself, “isn’t it what everyone does to solve any problem?” I was wrong. I learned that I wasn’t following the steps and got punished when I was solving advent of code problems.\n\n\n\nAdvent of code starts off with easy problems, and I believed in myself. But, I got one of them wrong because I didn’t fully understand the problem. Then, I realized how important it was to follow each step of the technique.\nSome of the advent of code problems were very difficult for me, and I got stuck. First of all, it was difficult to even understand the problem for a few of them. After going through different cases by hand, I understood them fully. Even after understanding, it was difficult to come up with a solution. Those were problems I had never seen before, and I had no idea how to solve them. After a couple hours, looking at the problem in different angles helped me get helpful ideas to solve them.\nTo really conquer the problem and prepare myself for future difficult problems, I looked back at the problem for a review. This step was probably the hardest because I already got the stars from the problems and I just wanted to move on and forget about them. But that doesn’t do anything for me in the long run. To get the most out of those problems, I looked at other people’s code. Many people had interesting dialogues that led to solutions, which helped me look at the problems from different angles.\nIn the end, I stuck with reading pytudes and try to figure out how Norvig came up with the code. After studying line by line and pretending to be in his shoes, I reproduced his code without looking after a day or two. There could probably be better ways to “look back,” but this was helpful for me to understand the problem better.\n\n\n\nOther than math and coding problems, it is interesting to solve problems in other domains. For instance, from Veritasium’s Youtube video on PCR, we can find out Kary Mullis may have used Polya’s technique to solve his problem and create polymerase chain reaction (PCR). His steps could’ve been: 1. He needed a lot of DNA for testing, and creating DNA from scratch was very tedious and boring. He needed a way to automate or simplify this. 2. As an experienced biochemist he had every component that’s required to build DNA in his head. 3. While driving, he simulated performing PCR in his head and wrote down the notes. 4. He worked with his team to make his idea reality. 5. Trying to automate the tedious work paid off. We got to be lazy.\nThere may be other fascinating examples in the history.\n\n\n\nOf course this technique applies to day to day problems as well with some flexibility and tweaks. I even apply to social problems if I feel like there is a problem with relationships and find ways to solve it. After practicing a lot on solving difficult problems, using the technique got easier. After all, I want to solve as many problems as possible fairly easily.",
    "crumbs": [
      "Home",
      "Blog",
      "How to solve it"
    ]
  },
  {
    "objectID": "blog/posts/2025-01-17-how_to_solve_it/index.html#intro",
    "href": "blog/posts/2025-01-17-how_to_solve_it/index.html#intro",
    "title": "How to solve it",
    "section": "",
    "text": "Over the winter break, I learned about Georgy Polya’s “How to solve it” through a course. “How to solve it” is a book about problem solving. As a mathematician, Polya solved difficult math problems, and the book focuses on math. However, his problem solving techniques can apply into many other fields as well. Let’s briefly look at the technique.",
    "crumbs": [
      "Home",
      "Blog",
      "How to solve it"
    ]
  },
  {
    "objectID": "blog/posts/2025-01-17-how_to_solve_it/index.html#technique-for-solving-problems",
    "href": "blog/posts/2025-01-17-how_to_solve_it/index.html#technique-for-solving-problems",
    "title": "How to solve it",
    "section": "",
    "text": "In the course, we utilize four-step problem-solving frame on advent of code problems to familiarize ourselves with problem solving. In the book, there are five steps, which I found was a bit more intuitive for me. Here’s the steps from the book.\n\nGetting acquanted\nBefore we solve, we want to understand the problem clearly in a big picture. There is no need for little details now. It’s like looking at a painting in a distance. We can figure out what the painting is about.\nWorking for better understanding\nWe want to get a much deeper understanding of the problem. To the point where we can have everything related to the problem in our head without looking at the problem. We think about each component and consider how they are important for our problem.\nHunting for the helpful idea\nNow we look for ideas that could help us solve the problem. We can look for similar problems we have solved in the past. This can be very straight forward for some problems, but it may not for others. Sometimes problems are related, but hard to find the connection. For instance, some designs in engineering may be inspired from biology.\nCarrying out the plan\nWe take small steps and check for correctness. We think about it in different angles and as ourselves whether it makes sense. In the end, we have the correct solution.\nLooking back\nWe make sure the solution is correct by thinking about it in differnt ways. We try to conquer the problem by simplifying the solution and applying those helpful ideas into other problems. Like our immune system, we can solve this type of problem with ease in the future.\n\nAfter learning about the technique, I thought the steps were obvious and intuitive. I didn’t think there was anything new or surprising here. I thought to myself, “isn’t it what everyone does to solve any problem?” I was wrong. I learned that I wasn’t following the steps and got punished when I was solving advent of code problems.",
    "crumbs": [
      "Home",
      "Blog",
      "How to solve it"
    ]
  },
  {
    "objectID": "blog/posts/2025-01-17-how_to_solve_it/index.html#advent-of-code",
    "href": "blog/posts/2025-01-17-how_to_solve_it/index.html#advent-of-code",
    "title": "How to solve it",
    "section": "",
    "text": "Advent of code starts off with easy problems, and I believed in myself. But, I got one of them wrong because I didn’t fully understand the problem. Then, I realized how important it was to follow each step of the technique.\nSome of the advent of code problems were very difficult for me, and I got stuck. First of all, it was difficult to even understand the problem for a few of them. After going through different cases by hand, I understood them fully. Even after understanding, it was difficult to come up with a solution. Those were problems I had never seen before, and I had no idea how to solve them. After a couple hours, looking at the problem in different angles helped me get helpful ideas to solve them.\nTo really conquer the problem and prepare myself for future difficult problems, I looked back at the problem for a review. This step was probably the hardest because I already got the stars from the problems and I just wanted to move on and forget about them. But that doesn’t do anything for me in the long run. To get the most out of those problems, I looked at other people’s code. Many people had interesting dialogues that led to solutions, which helped me look at the problems from different angles.\nIn the end, I stuck with reading pytudes and try to figure out how Norvig came up with the code. After studying line by line and pretending to be in his shoes, I reproduced his code without looking after a day or two. There could probably be better ways to “look back,” but this was helpful for me to understand the problem better.",
    "crumbs": [
      "Home",
      "Blog",
      "How to solve it"
    ]
  },
  {
    "objectID": "blog/posts/2025-01-17-how_to_solve_it/index.html#solving-other-problems",
    "href": "blog/posts/2025-01-17-how_to_solve_it/index.html#solving-other-problems",
    "title": "How to solve it",
    "section": "",
    "text": "Other than math and coding problems, it is interesting to solve problems in other domains. For instance, from Veritasium’s Youtube video on PCR, we can find out Kary Mullis may have used Polya’s technique to solve his problem and create polymerase chain reaction (PCR). His steps could’ve been: 1. He needed a lot of DNA for testing, and creating DNA from scratch was very tedious and boring. He needed a way to automate or simplify this. 2. As an experienced biochemist he had every component that’s required to build DNA in his head. 3. While driving, he simulated performing PCR in his head and wrote down the notes. 4. He worked with his team to make his idea reality. 5. Trying to automate the tedious work paid off. We got to be lazy.\nThere may be other fascinating examples in the history.",
    "crumbs": [
      "Home",
      "Blog",
      "How to solve it"
    ]
  },
  {
    "objectID": "blog/posts/2025-01-17-how_to_solve_it/index.html#conclusion",
    "href": "blog/posts/2025-01-17-how_to_solve_it/index.html#conclusion",
    "title": "How to solve it",
    "section": "",
    "text": "Of course this technique applies to day to day problems as well with some flexibility and tweaks. I even apply to social problems if I feel like there is a problem with relationships and find ways to solve it. After practicing a lot on solving difficult problems, using the technique got easier. After all, I want to solve as many problems as possible fairly easily.",
    "crumbs": [
      "Home",
      "Blog",
      "How to solve it"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html",
    "href": "blog/posts/2025-02-14-bllms_1/index.html",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "",
    "text": "I am reading “Build a Large Language Model from scratch” by Sebastian Raschka in a cluster of stars study group. In this blog, I am writing about the first chapter, which introduces large language models.\nImage generated from Dall-E",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html#introduction",
    "href": "blog/posts/2025-02-14-bllms_1/index.html#introduction",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "Introduction",
    "text": "Introduction\nI like to use LLMs for doing homework and coding. When chatGPT first came out, I was amazed at how I could ask homework questions and code. However, it could not solve all problems and code however I wanted. However, as time progressed, models improved so much. When working on homework together, it barely hallucinated. When coding, LLMs can code so much better with right context and tools now. Even without context, I can easily create web applications using Claude with a simple prompt like, “create a simple videogame” and I have it. I got curious about how LLMs came so far, and I wanted to learn about them more. So, I decided to read “Build a Large Language Model from scratch by Sebastian Raschka. What is a large language model? It is a deep learning model that can understand and generate words. And it is large because there are many layers and parameters in the model trained with a lot of data.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html#brief-history-of-llms.",
    "href": "blog/posts/2025-02-14-bllms_1/index.html#brief-history-of-llms.",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "Brief history of LLMs.",
    "text": "Brief history of LLMs.\nTraditional natural language processing relied heavily on rule-based systems and statistical methods. Scientists had to manually create rules for grammar and vocabulary, making it difficult for computers to understand context and meaning. These systems could perform basic tasks like identifying parts of speech or finding specific words, but they struggled with understanding human language’s complexity and nuance.\nThe introduction of transformers in 2017 revolutionized natural language processing. Unlike previous models that processed words one at a time, transformers could analyze entire sentences simultaneously through a mechanism called “attention.” This breakthrough allowed models to better understand context and relationships between words, leading to more accurate and natural language processing. The real game-changer came with models like BERT and GPT. These models, trained on massive amounts of text data, could understand and generate language in ways that seemed almost human-like.\nWhile these advances have transformed the field, LLMs still face important challenges. They require extensive computational resources and energy for training, raising concerns about environmental impact and accessibility. Technical hurdles include handling multiple languages equally well and adapting to new information without retraining. The field also grapples with broader issues like copyright concerns, as these models are trained on vast amounts of existing content, and the critical need for responsible AI development to prevent potential misuse.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html#why-are-llms-a-big-deal-now",
    "href": "blog/posts/2025-02-14-bllms_1/index.html#why-are-llms-a-big-deal-now",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "Why are LLMs a big deal now?",
    "text": "Why are LLMs a big deal now?\nI remember when I first learned about machine learning - it was all about classifying images or predicting numbers. But LLMs are different. They can write essays, debug code, and even create art descriptions. What amazes me most is how they understand context. When I ask about a coding error, they don’t just fix it - they explain why the error happened and how to prevent it.\nLLMs are transforming many fields. In education, they help students understand complex topics by breaking them down and providing examples. Scientists use them to summarize research papers and generate hypotheses. Developers use them as coding assistants - I’ve seen friends use cursor to write code faster than ever before. Even artists and writers use them for brainstorming ideas or editing their work.\nWhat excites me most about LLMs is their potential future impact. Imagine having a personal tutor that adapts perfectly to your learning style, or a research assistant that can help analyze complex scientific data. We’re already seeing LLMs being integrated into search engines and everyday applications. But there’s still so much more to explore - like using LLMs to help discover new medicines or solve complex engineering problems. Instead of working on small details, people will be able to plan further and effectively turn their ideas into reality with immense productivity.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html#how-to-build-an-llm",
    "href": "blog/posts/2025-02-14-bllms_1/index.html#how-to-build-an-llm",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "How to build an LLM?",
    "text": "How to build an LLM?\nWhen I first heard about building LLMs, I thought it would be incredibly complex - and it is! But breaking it down into steps makes it easier to understand. It’s like building with LEGO blocks: you start with basic pieces (pre-training), then customize them for specific tasks (fine-tuning). What fascinated me most was learning that these models aren’t just trained once and done - they go through multiple stages of learning, just like how we humans learn general knowledge first, then specialize in specific areas.\nThe transformer architecture is at the heart of modern LLMs, and there are two main approaches: BERT and GPT. When I first learned about them, I thought of BERT as a fill-in-the-blanks expert (like my English homework) and GPT as a story writer who keeps adding one word at a time. Let me explain why this matters.\n\nBERT vs GPT\nBERT (Bidirectional Encoder Representations from Transformers) looks at text from both directions - like how we humans can understand a sentence even if we start reading from the middle. For example, in the sentence “The cat ___ on the mat,” BERT can look at words before and after the blank to figure out the missing word is probably “sat.” This bidirectional understanding makes BERT really good at tasks like understanding the meaning of sentences or answering questions.\nGPT (Generative Pre-trained Transformer) works differently - it’s what we call an autoregressive model. Imagine you’re telling a story, and each word you say influences what word comes next. That’s how GPT works - it predicts the next word based on all the previous words. What amazes me is how this simple approach leads to what scientists call “emergent behavior” - the model suddenly showing abilities nobody explicitly programmed, like solving math problems or writing code.\n\n\nThe Power of Encoder Models\nWe are all familiar with decoder models, such as ChatGPT and deepseek, and what they are capable of. So, what are encoder models? Are they obsolete now? Think of encoder models as expert readers rather than writers - they’re really good at understanding text, even if they don’t generate long responses like their decoder cousins. Therefore, they were used for search engines and anomaly detection, such as spam filter.\nModernBERT takes this idea even further. What amazes me is how it can do multiple tasks with just its MLM (Masked Language Model) head - the same part it uses during pre-training. Instead of needing special heads for different tasks, it can handle things like classification and multiple-choice questions using the same mechanism. It’s like having a Swiss Army knife that gets better at everything it does!\nFor example, on knowledge-based questions (like MMLU tests), ModernBERT-Large-Instruct performs surprisingly well - even competing with larger decoder models while using fewer resources. What’s really cool is how it does this with a single forward pass, making it much faster than decoder models that generate tokens one by one.\nThe best part? Encoder models are usually more efficient than their decoder counterparts. While ChatGPT might take its time generating a response word by word, encoder models can process entire sentences at once. It’s like the difference between someone reading a whole page at a glance versus reading it word by word.\n\n\nTraining stages\nThe real magic happens in two stages:\n\nPre-training: This is like giving the model a massive library to read. During this stage, the model learns general patterns in language - grammar, facts, and even some basic reasoning. It’s similar to how we learn language by being exposed to lots of books and conversations.\nFine-tuning: This is where it gets interesting. After pre-training, we can teach the model specific skills. It’s like taking a generally educated person and training them for a specific job. For ChatGPT, this meant instruction fine-tuning - teaching it to be helpful and follow human instructions. For other models, we might fine-tune them to be better at coding or medical diagnosis.\n\n\nfrom graphviz import Digraph\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\ndot = Digraph(comment='LLM Building Process')\ndot.attr(rankdir='LR')\ndot.attr('node', shape='box', style='rounded,filled', fillcolor='lightblue')\n\ndot.node('A', 'Model Architecture\\n(Decoder)')\ndot.node('B', 'General Purpose Model\\n(GPT-3)')\ndot.node('C', 'Model with Specific Tasks\\n(ChatGPT)')\n\ndot.edge('A', 'B', 'Pre-training\\n(General Knowledge)\\n')\ndot.edge('B', 'C', 'Fine-tuning\\n(Specialized Learning)\\n')\n\ndot.render('llm_building_process', format='png', cleanup=True)\n\nimg = mpimg.imread('llm_building_process.png')\nplt.figure(figsize=(12, 6))\nplt.imshow(img)\nplt.axis('off')  \nplt.show()",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html#zero-shot-learning-and-emergent-behaviors",
    "href": "blog/posts/2025-02-14-bllms_1/index.html#zero-shot-learning-and-emergent-behaviors",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "Zero-shot Learning and Emergent Behaviors",
    "text": "Zero-shot Learning and Emergent Behaviors\nOne of the most fascinating things I learned about LLMs is their ability to perform zero-shot and few-shot learning. When I first heard these terms, they sounded like magic tricks, but they’re actually quite intuitive. Zero-shot learning is when a model can do something it was never explicitly trained to do. It’s like how we humans can guess what a “flying submarine” might look like even though we’ve never seen one - we combine our knowledge of flying and submarines to imagine it. Few-shot learning is similar, but with a few examples to guide the model - like showing someone a couple of examples of how to solve a math problem before asking them to solve a new one.\nAnother amazing property was learning about emergent behaviors in GPT models. As these models get larger and are trained on more data, they suddenly develop abilities that nobody programmed them to have. It’s like watching a child grow up - you teach them basic things like reading and writing, but suddenly they’re creating poetry or solving complex problems in ways you never taught them. For example, GPT-4 can solve complex math problems step by step, even though it was only trained to predict the next word in a sequence. How does this happen? Scientists are still trying to figure it out!\nChatGPT took this even further with instruction fine-tuning. Instead of just generating text, it learned to be helpful and follow instructions through a process called RLHF (Reinforcement Learning from Human Feedback). It’s like teaching a student not just knowledge, but also how to be a good teaching assistant - being helpful, admitting when they don’t know something, and explaining things clearly.",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html#conclusion",
    "href": "blog/posts/2025-02-14-bllms_1/index.html#conclusion",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nWe covered basics of LLMs. I hope you learned about what LLMs are. In the future blogs, we will be coding large language models from scratch as the book says, and it will be so much fun!",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2025-02-14-bllms_1/index.html#additional-resources",
    "href": "blog/posts/2025-02-14-bllms_1/index.html#additional-resources",
    "title": "Building LLM part1-Intro_to_LLMs",
    "section": "Additional resources",
    "text": "Additional resources\nHere are some additional resources: - AlphaFold video by Veritasium - Deep Dive into LLMs like ChatGPT by Andrej Karpathy - Visualizing transformers and attention by Grant Sanderson - “Build a Large Language Model from scratch” by Sebastian Raschka",
    "crumbs": [
      "Home",
      "Blog",
      "Building LLM part1-Intro_to_LLMs"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-17-Scheduler/index.html",
    "href": "blog/posts/2023-11-17-Scheduler/index.html",
    "title": "Scheduler",
    "section": "",
    "text": "In this blog, we will learn about Pytorch optimizers and learning rate schedulers. From all the learning rate schedulers, we will learn about CosineAnnealingLR and OneCycleLR by writing them from scratch.\nWe can use a learning rate scheduler to vary the learning rate while training. Why do we want to change the learning rate? Let’s take a look at an example.\nSo, let’s assume we are stranded in the desert while traveling because our vehicle broke down. We are thirsty, so we want to find some water. We get out of the car to find a well. Thankfully, we have a device that beeps when close to a body of water. The more water, the louder the beep. So, we walk in different directions to figure out where to go. After figuring out the direction, we can run for fast travel. When we are close to water, we want to slow down to find exactly where to dig to find the water. We don’t want to go past the water and come back.\nBy changing learning rate, we can change the speed of the training of the model. With the scheduler, we want to start with a small learning rate. In the beginning, our model is trying to figure out which direction to travel to reduce the loss. After it knows where to step, we can increase the learning rate to take bigger steps. In the end of the training, it’s so close to the ideal point, and we don’t want to go past. Therefore, we lower the learning rate to fine tune.\nSo, let’s see this in action.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n!pip -q install torcheval\n!pip -q install datasets\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 521.2/521.2 kB 7.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 13.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 14.8 MB/s eta 0:00:00\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.xtras import *\n\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\n\ndls = get_dls()\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))",
    "crumbs": [
      "Home",
      "Blog",
      "Scheduler"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-17-Scheduler/index.html#learning-rate-schedulers",
    "href": "blog/posts/2023-11-17-Scheduler/index.html#learning-rate-schedulers",
    "title": "Scheduler",
    "section": "",
    "text": "In this blog, we will learn about Pytorch optimizers and learning rate schedulers. From all the learning rate schedulers, we will learn about CosineAnnealingLR and OneCycleLR by writing them from scratch.\nWe can use a learning rate scheduler to vary the learning rate while training. Why do we want to change the learning rate? Let’s take a look at an example.\nSo, let’s assume we are stranded in the desert while traveling because our vehicle broke down. We are thirsty, so we want to find some water. We get out of the car to find a well. Thankfully, we have a device that beeps when close to a body of water. The more water, the louder the beep. So, we walk in different directions to figure out where to go. After figuring out the direction, we can run for fast travel. When we are close to water, we want to slow down to find exactly where to dig to find the water. We don’t want to go past the water and come back.\nBy changing learning rate, we can change the speed of the training of the model. With the scheduler, we want to start with a small learning rate. In the beginning, our model is trying to figure out which direction to travel to reduce the loss. After it knows where to step, we can increase the learning rate to take bigger steps. In the end of the training, it’s so close to the ideal point, and we don’t want to go past. Therefore, we lower the learning rate to fine tune.\nSo, let’s see this in action.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n!pip -q install torcheval\n!pip -q install datasets\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 521.2/521.2 kB 7.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 13.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 14.8 MB/s eta 0:00:00\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.xtras import *\n\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\n\ndls = get_dls()\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))",
    "crumbs": [
      "Home",
      "Blog",
      "Scheduler"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-17-Scheduler/index.html#pytorch-optimizers",
    "href": "blog/posts/2023-11-17-Scheduler/index.html#pytorch-optimizers",
    "title": "Scheduler",
    "section": "Pytorch optimizers",
    "text": "Pytorch optimizers\nBefore we look into schedulers, we need to look at Pytorch optimizers. We know how optimizers work in general from last blog, but Pytorch optimizers are a little bit different. To initialize a Pytorch optimizer, we need parameters and a learning rate.\n\nmodel = get_model()\nopt = optim.SGD(model.parameters(), lr=0.1)\nopt\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.1\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\nWe can also grab an optimizer from the learner. With SingleBatchCB, we can stop the training after one batch. This way, every initialization step is run, and we know there is no error when the batch ran.\n\nlearn = TrainLearner(get_model(), dls, F.cross_entropy, cbs=[DeviceCB(), SingleBatchCB()])\nlearn.fit()\n\n\nopt = learn.opt\nopt\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.1\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\nOptimizers store states for updating the weights. To access the states, we have to use parameters as keys because they are dictionaries.\n\nparams = next(iter(learn.model.parameters()))\nopt.state[params]\n\n{'momentum_buffer': None}\n\n\n\nlen(params)\n\n8\n\n\nParameters are also divided into param_groups. This will be useful later when we want to apply different learning rates for each parameter group.\n\nlen(opt.param_groups)\n\n1\n\n\n\nlist(opt.param_groups[0])\n\n['params',\n 'lr',\n 'momentum',\n 'dampening',\n 'weight_decay',\n 'nesterov',\n 'maximize',\n 'foreach',\n 'differentiable']\n\n\n\ntype(opt.param_groups[0])\n\ndict\n\n\nWe can also modify learning rates.\n\nopt.param_groups[0]['lr'] = 1\nopt.param_groups[0]['lr']\n\n1",
    "crumbs": [
      "Home",
      "Blog",
      "Scheduler"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-17-Scheduler/index.html#scheduler",
    "href": "blog/posts/2023-11-17-Scheduler/index.html#scheduler",
    "title": "Scheduler",
    "section": "Scheduler",
    "text": "Scheduler\n\nCosignAnnealingLR\nIt’s time to look at schedulers. Let’s look at CosineAnnealingLR and see how the learning rate changes by plotting it. To create a scheduler, we have to pass in an optimizer and the number of total steps. The optimizer stores states such as learning rate, and total steps are used to determine how to change the learning rate.\n\nsched = lr_scheduler.CosineAnnealingLR(opt, 100)\nsched\n\n&lt;torch.optim.lr_scheduler.CosineAnnealingLR&gt;\n\n\nWe can use step and get_last_lr to update and get the learning rate.\n\nsched.get_last_lr()\n\n[1]\n\n\nLet’s plot the function to see how the learning rate changes.\n\nlrs = []\nfor i in range(100):\n    lrs.append(sched.get_last_lr())\n    opt.step()\n    sched.step()\nplt.plot(lrs);\n\n\n\n\n\n\n\n\nWe can create a plotting function to explore more. If we step more than t_max, we can see the cosine curve coming up. We can also use eta_min to set the minimum of the learning rate.\n\ndef get_opt(lr=0.1): return optim.SGD(get_model().parameters(), lr=lr)\n\n\ndef plot_lrs(scheduler, opt, n_steps=None, **kwargs):\n    sched = scheduler(opt, **kwargs)\n    sched_lrs = []\n    for i in range(n_steps):\n        sched_lrs.append(sched.get_last_lr())\n        opt.step()\n        sched.step()\n\n    plt.plot(sched_lrs);\n\n\nplot_lrs(lr_scheduler.CosineAnnealingLR, get_opt(), T_max=100, n_steps=120, eta_min=.02)\n\n\n\n\n\n\n\n\nIt’s time to write it from scratch.\n\nclass Scheduler:\n    def __init__(self, opt, T_max):\n        self.opt = opt\n        self.T_max = T_max\n        self.cur_T = 1\n        self.max_lr = opt.param_groups[0]['lr']\n\n\nclass CosineAnnealing(Scheduler):\n    def __init__(self, opt, T_max):\n        super().__init__(opt, T_max)\n\n    def step(self):\n        new_lr = .5*(self.max_lr)*(1+torch.cos(tensor(self.cur_T/self.T_max*torch.pi)))\n        self.opt.param_groups[0]['lr'] = new_lr\n        self.cur_T += 1\n\n    def get_last_lr(self): return self.opt.param_groups[0]['lr']\n\n\nplot_lrs(CosineAnnealing, get_opt(), T_max=100, n_steps=120)\n\n\n\n\n\n\n\n\nNow, let’s see how it performs in the model. We will update the learning rate each batch. We will also record the learning rates.\n\nclass BaseSchedCB(Callback):\n    def __init__(self, sched): self.sched = sched\n    def before_fit(self, learn): self.schedo = self.sched(learn.opt)\n    def _step(self, learn):\n        if learn.training: self.schedo.step()\n\n\nclass BatchSchedCB(BaseSchedCB):\n    def after_batch(self, learn): self._step(learn)\n\n\nclass RecorderCB(Callback):\n    def __init__(self, **d): self.d = d\n    def before_fit(self, learn):\n        self.recs = {k:[] for k in self.d}\n        self.pg = learn.opt.param_groups[0]\n\n    def after_batch(self, learn):\n        if not learn.training: return\n        for k,v in self.d.items():\n            self.recs[k].append(v(self))\n\n    def plot(self):\n        for k,v in self.recs.items():\n            plt.plot(v, label=k)\n            plt.legend()\n            plt.show()\n\n\ndef _lr(cb): return cb.pg['lr']\n\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\nlrf_cbs = [DeviceCB(), LRFinderCB()]\n\n\nT_max = len(dls.train) * 3\nsched = partial(CosineAnnealing, T_max=T_max)\n\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\niw = partial(init_weights, leaky=0.1)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), astats]\n\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nrec = RecorderCB(lr=_lr)\nxtra = [BatchSchedCB(sched),rec]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=2e-2, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.796\n0.552\n0\nTrue\n00:23\n\n\n0.853\n0.404\n0\nFalse\n00:02\n\n\n0.876\n0.341\n1\nTrue\n00:21\n\n\n0.872\n0.354\n1\nFalse\n00:02\n\n\n0.891\n0.299\n2\nTrue\n00:25\n\n\n0.877\n0.338\n2\nFalse\n00:02\n\n\n\n\n\n\nrec.plot()\n\n\n\n\n\n\n\n\n\n\nOneCycleLR\nNow, let’s go over OneCycleLR. This technique came from Cyclical Learning Rates for Training Neural Networks by Leslie Smith. He experimented a lot with hyperparameters like learning rate, momentum, batch size, and weight decay. In the paper, he went over triangular learning rate policy. Let’s see what it looks like from Pytorch’s version.\n\nplot_lrs(lr_scheduler.OneCycleLR, get_opt(), n_steps=100, max_lr=1, total_steps=100, anneal_strategy='linear', three_phase=False, pct_start=.5)\n\n\n\n\n\n\n\n\nWhy does our model starts from the lowest to maximum and back down to the lowest? As menteioned before, it starts with the lowest to warm up. As it trains, it finds where to step, so we can take bigger steps. Then, we slow down to fine tune.\nIt’s very simple to implement. We can easily write the program.\n\nclass OneCycle(Scheduler):\n    def __init__(self, opt, T_max, min_lr=0.):\n        super().__init__(opt, T_max)\n        self.min_lr = min_lr\n        self.opt.param_groups[0]['lr'] = self.min_lr\n\n    def step(self):\n        cycle = math.floor(1 + self.cur_T/(2* self.T_max/2))\n        x = abs(self.cur_T / (self.T_max/2) - 2*cycle + 1)\n        new_lr = self.min_lr + (self.max_lr - self.min_lr) * max(0, 1-x)\n        self.opt.param_groups[0]['lr'] = new_lr\n        self.cur_T += 1\n\n    def get_last_lr(self): return self.opt.param_groups[0]['lr']\n\n\nplot_lrs(OneCycle, get_opt(), n_steps=100, T_max=100)\n\n\n\n\n\n\n\n\nThat looks good. Let’s try it out.\n\nT_max = len(dls.train) * 3\nsched = partial(OneCycle, T_max=T_max)\nrec = RecorderCB(lr=_lr)\n\nset_seed(42)\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\nrec = RecorderCB(lr=_lr)\nxtra = [BatchSchedCB(sched),rec]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=2e-2, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.711\n0.814\n0\nTrue\n00:25\n\n\n0.834\n0.462\n0\nFalse\n00:02\n\n\n0.862\n0.375\n1\nTrue\n00:22\n\n\n0.871\n0.359\n1\nFalse\n00:01\n\n\n0.892\n0.294\n2\nTrue\n00:23\n\n\n0.883\n0.322\n2\nFalse\n00:02\n\n\n\n\n\n\nrec.plot()\n\n\n\n\n\n\n\n\nWith OneCycle, we can increase the learning rate a bit more. Try it out and see how much you can increase.\nBy default, Pytorch’s OneCycleLR uses cosine annealing to modify both learning rate and momentum for the batch norm. In A disciplined approach to neural network hyper-parameters: Part 1, Leslie Smith talks about how there is a balance in regularization using learning rate, batch size, momentum, and weight decay. To summarize, - High batch size: high regularization - High weight decay: high regularization - High learning rate: high regularization - High momentum: high regularization\nLearning rate and momentum are more closely related. When using high regularization, learning rate has to be reduced, and vice versa. That’s what the Pytroch’s OneCycleLR takes advantage of.\n\ndef _beta1(cb): return cb.pg['betas'][0]\nrec = RecorderCB(lr=_lr, mom=_beta1)\n\n\nset_seed(42)\nlr,epochs = 6e-2,5\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched), rec]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.772\n0.646\n0\nTrue\n00:23\n\n\n0.844\n0.461\n0\nFalse\n00:01\n\n\n0.857\n0.395\n1\nTrue\n00:21\n\n\n0.866\n0.369\n1\nFalse\n00:02\n\n\n0.885\n0.311\n2\nTrue\n00:23\n\n\n0.879\n0.334\n2\nFalse\n00:01\n\n\n0.901\n0.266\n3\nTrue\n00:25\n\n\n0.891\n0.296\n3\nFalse\n00:02\n\n\n0.918\n0.224\n4\nTrue\n00:22\n\n\n0.897\n0.283\n4\nFalse\n00:01\n\n\n\n\n\n\nrec.plot()",
    "crumbs": [
      "Home",
      "Blog",
      "Scheduler"
    ]
  },
  {
    "objectID": "blog/posts/2023-11-17-Scheduler/index.html#conclusion",
    "href": "blog/posts/2023-11-17-Scheduler/index.html#conclusion",
    "title": "Scheduler",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog, we learned about Pytorch optimizers and schedulers. We were already familiar with optimizers, but we learned how Pytorch optimizers store states. Then, we went over CosignAnnealingLR and OneCycleLR schedulers and wrote them from scratch. We also went over Leslie Smith’s papers regarding balancing regularization and scheduling learning rates.",
    "crumbs": [
      "Home",
      "Blog",
      "Scheduler"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html",
    "title": "The Meselson-Stahl Experiment",
    "section": "",
    "text": "I first learned about the Meselson-Stahl experiment when I was reading my textbook for introductory biology class. It was the coolest experiment I learned about, and it has been my favorite since then. What is so cool about this experiment? Let’s dive in!\nNowadays, we know that DNA is the heritable material, but many scientists debated whether protein was the genetic material in the 1940s. Because DNA only had four kinds of nucleotides compared to twenty different amino acids in proteins, many believed DNA lacked the complexity necessary to store genetic information.1 After the Avery-MacLeod-McCarty experiment2 and Hershey-Chase experiment3, published in 1944 and 1952 respectively, scientists accepted DNA as the genetic material. Then in 1953, James Watson and Francis Crick proposed a model for the structure of DNA as a double helix based on Rosalind Franklin’s X-ray diffraction data.4 The next question scientists wanted to answer was: how does DNA replicate?\n\n\n\nBy the mid-1950s, scientists had three different ideas about how DNA might copy itself.5 It’s fascinating to think that even after Watson and Crick proposed the double helix structure, nobody actually knew how it worked! The three theories had pretty descriptive names that tell you exactly what each one predicted.\nThe semi-conservative model was Watson and Crick’s original idea. They thought the two DNA strands would separate, like unzipping a zipper, and each strand would serve as a template to build a new partner strand.6 So after replication, you’d have two DNA molecules, each containing one old strand and one new strand. It is “semi-conservative” because half of the original DNA is conserved in each copy.\nThe conservative model suggested something completely different. Scientists who supported this idea thought the original DNA double helix would stay completely intact, somehow producing an entirely separate copy.7 The original stays unchanged while you get a brand new copy like a photo copy.\nBut then there was the dispersive model, which was proposed by Max Delbrück. Delbrück was actually skeptical of Watson and Crick’s idea because he thought it was physically impossible to separate the tightly wound DNA strands.8 DNA is like coiled ropes that are wound together millions of times. So he proposed that the DNA would break into small pieces, replicate those pieces, and then reassemble them.9 The result would be two DNA molecules that were each a patchwork mix of old and new material scattered throughout.\nWhat made this debate so intense was that all three theories seemed plausible, but nobody had figured out how to test them experimentally.10 That’s where Meselson and Stahl came in with their amazing solution.\nHere is a diagram of the three models of DNA replications.\n\n\n\n\n\n\n\n\n\nBefore moving onto experimental details, let’s think about what each model would look like after multiple replications. In semi-conservative, DNA opens up and each strand of heavy DNA is used as a template and creates hybrid DNA with half heavy and half light on generation 1. After second replication, we have two hybrids and two lights.\n\n\n\n\n\n\n\n\n\nWhat would happen in conservative replication? Since the original DNA remains intact and copy is made like using a copy machine, we have a heavy one and a light one. We do not have a hybrid copy here. In the generation 2, we have 2 more light copies of the DNA.\n\n\n\n\n\n\n\n\n\nIn dispersive DNA replication, DNA is chopped up into little pieces but they still remain as double strand. In generation 1, random pieces of DNA are replaced with light ones. Therefore, we have 50% heavy and 50% light hybrids in generation 1. And in generation 2, we have 25% heavy with 75% light as hybrids.\n\n\n\n\n\n\n\n\n\n\n\n\nSo how do you actually test which theory is correct? This was the challenge that stumped scientists for years. Meselson and Stahl came up with an incredibly clever solution that’s honestly kind of beautiful in its simplicity.\nTheir big insight was to use isotopes to “label” DNA so they could track it. They chose nitrogen isotopes because DNA contains lots of nitrogen atoms.11 Most nitrogen in nature is the light version (¹⁴N), but there’s also a heavier version (¹⁵N) that works exactly the same way but weighs more. It’s like having two identical cars where one is made of regular steel and the other is made of slightly heavier steel. They both work the same, but you can tell them apart if you weigh them.\nWhy nitrogen instead of other elements? Well, other scientists had actually tried using phosphorus first. But phosphorus-32 was radioactive and decayed too quickly, so it didn’t work well for this kind of experiment.12 Nitrogen was perfect because it’s stable and DNA is packed with nitrogen atoms.\nNext, they needed to separate heavy DNA from light DNA. This is where Meselson’s genius really shows. He had attended a lecture by Jacques Monod about protein synthesis, and during that talk, he had this lightbulb moment about using density differences to separate molecules.13 The technique he developed is called density gradient centrifugation.\nHere’s how it works: you mix your DNA sample with a really dense salt solution made of cesium chloride (CsCl). When you spin this mixture in an ultracentrifuge at incredibly high speeds for hours, something amazing happens. The cesium chloride naturally forms a gradient from heavy at the bottom to light at the top.14 The DNA then settles at the exact spot in the tube where its density matches the cesium chloride around it. Heavy DNA sinks further down, light DNA floats higher up.\nWhy cesium chloride specifically? They actually tried other salts first! They looked at the periodic table for dense atoms that wouldn’t react with DNA. They tried rubidium chloride first, but even concentrated solutions weren’t dense enough to make DNA float. So they moved down the periodic table to cesium, and that worked perfectly.15\nFor the organisms, they initially tried bacteriophages (viruses that infect bacteria), but ran into problems. The viral DNA broke apart during centrifugation and replicated too fast to measure properly. So they switched to E. coli bacteria, whose DNA formed much clearer, more stable bands in the centrifuge.16\nThe experimental plan was elegant: grow bacteria for many generations in heavy nitrogen medium so all their DNA becomes heavy, then suddenly switch them to light nitrogen medium and watch what happens to the DNA after each round of replication.\n\n\n\nSo what did they actually see? The results were so clean and definitive that they immediately settled the debate that had been going on for years.\nFirst, let me set up what each theory predicted would happen. If the conservative model was right, after one generation in light nitrogen, you’d see two distinct bands: one heavy band (the original DNA staying intact) and one light band (the completely new copy). If the semi-conservative model was correct, you’d see just one band at an intermediate density (hybrid DNA with one heavy and one light strand). And if the dispersive model was right, you’d also see one intermediate band, but for a different reason (mixed heavy and light material throughout each molecule).\nHere’s what they actually observed:\nGeneration 0 (the control): All the DNA was heavy, forming a single band at the bottom of the cesium chloride gradient.17 This made sense because the bacteria had been growing in heavy nitrogen for many generations.\nGeneration 1 (after one replication in light medium): This was the moment of truth. They saw a single band, but it wasn’t at the heavy position or the light position. It was right in the middle at an intermediate density!18 This immediately ruled out the conservative model. If DNA replication were conservative, they would have seen two separate bands, not one intermediate band.\nBut here’s the thing: both the semi-conservative and dispersive models predicted this same result. So they needed to look at the second generation to figure out which one was correct.\nGeneration 2 (after two replications in light medium): This is where it got really exciting. Instead of one band, they now saw two distinct bands. One band was at the same intermediate position as generation 1, and the other was at the light position.19 Plus, the two bands were exactly equal in intensity, meaning there were equal amounts of each type of DNA.\nThis result was completely inconsistent with the dispersive model, which would have predicted just one band at a density between the intermediate and light positions.20 But it perfectly matched what the semi-conservative model predicted: half the DNA molecules would still be hybrids (intermediate density) and half would be completely light.\nDNA replication is semi-conservative, just like Watson and Crick had proposed.\nThe beauty of this experiment is how definitive the results were. There was no ambiguity, no need for statistical analysis or complex interpretation. The bands in the centrifuge tube told the whole story.\n\n\n\ndiagram of results from the experiment\n\n\nAn animated diagram of results from the experiment. Because 15 nitrogen isotope is heavier, it is on the lower side of the ultracentrifuge. The image is from https://commons.wikimedia.org/wiki/File:Meselson-stahl_experiment_diagram_en.svg\n\n\n\nThe real picture of result from the experiment\n\n\nThis is a picture of results from the experiment. The column a is from the ultracentrifuge, and the right side is the heavy side. The column b is densitometer traces, which tracks the intensity of the DNA bands. One peak means there is one DNA, and two peak means two kinds of DNA are present. The image is from https://evilutionarybiologist.blogspot.com/2007/10/most-beautiful-experiment-in-biology.html.\n\n\n\nThis experiment didn’t just answer one question about DNA replication. It completely transformed how we think about biology and opened up entire new fields of research that we’re still benefiting from today.\nFirst, the immediate impact was huge. The experiment provided the final proof that Watson and Crick’s double helix model was correct.21 Before Meselson-Stahl, many scientists were still skeptical about the double helix structure. But now they had concrete evidence that DNA really did work the way Watson and Crick proposed. The double helix went from being an interesting theory to accepted scientific fact.\nBut the bigger impact was methodological. Meselson’s density gradient centrifugation technique became one of the most important tools in molecular biology.22 Suddenly, scientists could separate different types of molecules based on their density. This opened up so many possibilities! Researchers started using it to purify viruses, separate different types of cells, isolate specific organelles from cells, and distinguish between different forms of DNA and RNA. It became a cornerstone technique that’s still used in labs around the world today.\nThe experiment also marked the transition from classical genetics to modern molecular biology.23 Before this, genetics was mostly about observing inheritance patterns and trying to figure out what was happening. After Meselson-Stahl, scientists could actually watch biological processes happening at the molecular level. It connected Mendel’s 19th century observations about inheritance to the actual physical machinery inside cells.24\nThis foundation made possible so many of the technologies we use today. CRISPR gene editing relies on understanding exactly how DNA replication works. PCR amplification, which is used in everything from COVID tests to crime scene analysis, is based on the semi-conservative replication mechanism they discovered. DNA fingerprinting for paternity tests and criminal investigations depends on knowing that DNA is inherited predictably from parents to children. Even personalized medicine is built on the understanding that genes are physical molecules that follow predictable rules.\nWhat really excites me is thinking about how one elegant experiment asking a simple question led to all these incredible applications that touch our lives every day. That’s the beauty of basic scientific research.\n\n\n\nFor this blog post, I wanted to experiment with deep research using solveit. I’ve used Gemini’s deep research feature before for a genomics project where I needed to learn about genes affected by breast cancer in mice, which is a topic I knew nothing about. Gemini provided a comprehensive report with research paper citations, but working with solveit felt more interactive and engaging.\nThe dialogue format made me more involved in the research process. Instead of receiving a finished report, I could think through questions and explore ideas as they came up. I started with only a vague plan to write about the Meselson-Stahl experiment, but through our conversation, I discovered what aspects I wanted to emphasize and developed a clearer focus.\nThis interactive approach helped me solidify my outline and gave me rich context for writing. The back-and-forth discussion became the foundation for the blog post itself. You can see the full dialogue here.",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html#introduction",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html#introduction",
    "title": "The Meselson-Stahl Experiment",
    "section": "",
    "text": "I first learned about the Meselson-Stahl experiment when I was reading my textbook for introductory biology class. It was the coolest experiment I learned about, and it has been my favorite since then. What is so cool about this experiment? Let’s dive in!\nNowadays, we know that DNA is the heritable material, but many scientists debated whether protein was the genetic material in the 1940s. Because DNA only had four kinds of nucleotides compared to twenty different amino acids in proteins, many believed DNA lacked the complexity necessary to store genetic information.1 After the Avery-MacLeod-McCarty experiment2 and Hershey-Chase experiment3, published in 1944 and 1952 respectively, scientists accepted DNA as the genetic material. Then in 1953, James Watson and Francis Crick proposed a model for the structure of DNA as a double helix based on Rosalind Franklin’s X-ray diffraction data.4 The next question scientists wanted to answer was: how does DNA replicate?",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html#the-three-competing-theories",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html#the-three-competing-theories",
    "title": "The Meselson-Stahl Experiment",
    "section": "",
    "text": "By the mid-1950s, scientists had three different ideas about how DNA might copy itself.5 It’s fascinating to think that even after Watson and Crick proposed the double helix structure, nobody actually knew how it worked! The three theories had pretty descriptive names that tell you exactly what each one predicted.\nThe semi-conservative model was Watson and Crick’s original idea. They thought the two DNA strands would separate, like unzipping a zipper, and each strand would serve as a template to build a new partner strand.6 So after replication, you’d have two DNA molecules, each containing one old strand and one new strand. It is “semi-conservative” because half of the original DNA is conserved in each copy.\nThe conservative model suggested something completely different. Scientists who supported this idea thought the original DNA double helix would stay completely intact, somehow producing an entirely separate copy.7 The original stays unchanged while you get a brand new copy like a photo copy.\nBut then there was the dispersive model, which was proposed by Max Delbrück. Delbrück was actually skeptical of Watson and Crick’s idea because he thought it was physically impossible to separate the tightly wound DNA strands.8 DNA is like coiled ropes that are wound together millions of times. So he proposed that the DNA would break into small pieces, replicate those pieces, and then reassemble them.9 The result would be two DNA molecules that were each a patchwork mix of old and new material scattered throughout.\nWhat made this debate so intense was that all three theories seemed plausible, but nobody had figured out how to test them experimentally.10 That’s where Meselson and Stahl came in with their amazing solution.\nHere is a diagram of the three models of DNA replications.\n\n\n\n\n\n\n\n\n\nBefore moving onto experimental details, let’s think about what each model would look like after multiple replications. In semi-conservative, DNA opens up and each strand of heavy DNA is used as a template and creates hybrid DNA with half heavy and half light on generation 1. After second replication, we have two hybrids and two lights.\n\n\n\n\n\n\n\n\n\nWhat would happen in conservative replication? Since the original DNA remains intact and copy is made like using a copy machine, we have a heavy one and a light one. We do not have a hybrid copy here. In the generation 2, we have 2 more light copies of the DNA.\n\n\n\n\n\n\n\n\n\nIn dispersive DNA replication, DNA is chopped up into little pieces but they still remain as double strand. In generation 1, random pieces of DNA are replaced with light ones. Therefore, we have 50% heavy and 50% light hybrids in generation 1. And in generation 2, we have 25% heavy with 75% light as hybrids.",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html#the-brilliant-experimental-design",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html#the-brilliant-experimental-design",
    "title": "The Meselson-Stahl Experiment",
    "section": "",
    "text": "So how do you actually test which theory is correct? This was the challenge that stumped scientists for years. Meselson and Stahl came up with an incredibly clever solution that’s honestly kind of beautiful in its simplicity.\nTheir big insight was to use isotopes to “label” DNA so they could track it. They chose nitrogen isotopes because DNA contains lots of nitrogen atoms.11 Most nitrogen in nature is the light version (¹⁴N), but there’s also a heavier version (¹⁵N) that works exactly the same way but weighs more. It’s like having two identical cars where one is made of regular steel and the other is made of slightly heavier steel. They both work the same, but you can tell them apart if you weigh them.\nWhy nitrogen instead of other elements? Well, other scientists had actually tried using phosphorus first. But phosphorus-32 was radioactive and decayed too quickly, so it didn’t work well for this kind of experiment.12 Nitrogen was perfect because it’s stable and DNA is packed with nitrogen atoms.\nNext, they needed to separate heavy DNA from light DNA. This is where Meselson’s genius really shows. He had attended a lecture by Jacques Monod about protein synthesis, and during that talk, he had this lightbulb moment about using density differences to separate molecules.13 The technique he developed is called density gradient centrifugation.\nHere’s how it works: you mix your DNA sample with a really dense salt solution made of cesium chloride (CsCl). When you spin this mixture in an ultracentrifuge at incredibly high speeds for hours, something amazing happens. The cesium chloride naturally forms a gradient from heavy at the bottom to light at the top.14 The DNA then settles at the exact spot in the tube where its density matches the cesium chloride around it. Heavy DNA sinks further down, light DNA floats higher up.\nWhy cesium chloride specifically? They actually tried other salts first! They looked at the periodic table for dense atoms that wouldn’t react with DNA. They tried rubidium chloride first, but even concentrated solutions weren’t dense enough to make DNA float. So they moved down the periodic table to cesium, and that worked perfectly.15\nFor the organisms, they initially tried bacteriophages (viruses that infect bacteria), but ran into problems. The viral DNA broke apart during centrifugation and replicated too fast to measure properly. So they switched to E. coli bacteria, whose DNA formed much clearer, more stable bands in the centrifuge.16\nThe experimental plan was elegant: grow bacteria for many generations in heavy nitrogen medium so all their DNA becomes heavy, then suddenly switch them to light nitrogen medium and watch what happens to the DNA after each round of replication.",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html#the-results-that-changed-everything",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html#the-results-that-changed-everything",
    "title": "The Meselson-Stahl Experiment",
    "section": "",
    "text": "So what did they actually see? The results were so clean and definitive that they immediately settled the debate that had been going on for years.\nFirst, let me set up what each theory predicted would happen. If the conservative model was right, after one generation in light nitrogen, you’d see two distinct bands: one heavy band (the original DNA staying intact) and one light band (the completely new copy). If the semi-conservative model was correct, you’d see just one band at an intermediate density (hybrid DNA with one heavy and one light strand). And if the dispersive model was right, you’d also see one intermediate band, but for a different reason (mixed heavy and light material throughout each molecule).\nHere’s what they actually observed:\nGeneration 0 (the control): All the DNA was heavy, forming a single band at the bottom of the cesium chloride gradient.17 This made sense because the bacteria had been growing in heavy nitrogen for many generations.\nGeneration 1 (after one replication in light medium): This was the moment of truth. They saw a single band, but it wasn’t at the heavy position or the light position. It was right in the middle at an intermediate density!18 This immediately ruled out the conservative model. If DNA replication were conservative, they would have seen two separate bands, not one intermediate band.\nBut here’s the thing: both the semi-conservative and dispersive models predicted this same result. So they needed to look at the second generation to figure out which one was correct.\nGeneration 2 (after two replications in light medium): This is where it got really exciting. Instead of one band, they now saw two distinct bands. One band was at the same intermediate position as generation 1, and the other was at the light position.19 Plus, the two bands were exactly equal in intensity, meaning there were equal amounts of each type of DNA.\nThis result was completely inconsistent with the dispersive model, which would have predicted just one band at a density between the intermediate and light positions.20 But it perfectly matched what the semi-conservative model predicted: half the DNA molecules would still be hybrids (intermediate density) and half would be completely light.\nDNA replication is semi-conservative, just like Watson and Crick had proposed.\nThe beauty of this experiment is how definitive the results were. There was no ambiguity, no need for statistical analysis or complex interpretation. The bands in the centrifuge tube told the whole story.\n\n\n\ndiagram of results from the experiment\n\n\nAn animated diagram of results from the experiment. Because 15 nitrogen isotope is heavier, it is on the lower side of the ultracentrifuge. The image is from https://commons.wikimedia.org/wiki/File:Meselson-stahl_experiment_diagram_en.svg\n\n\n\nThe real picture of result from the experiment\n\n\nThis is a picture of results from the experiment. The column a is from the ultracentrifuge, and the right side is the heavy side. The column b is densitometer traces, which tracks the intensity of the DNA bands. One peak means there is one DNA, and two peak means two kinds of DNA are present. The image is from https://evilutionarybiologist.blogspot.com/2007/10/most-beautiful-experiment-in-biology.html.",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html#the-impact-and-legacy",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html#the-impact-and-legacy",
    "title": "The Meselson-Stahl Experiment",
    "section": "",
    "text": "This experiment didn’t just answer one question about DNA replication. It completely transformed how we think about biology and opened up entire new fields of research that we’re still benefiting from today.\nFirst, the immediate impact was huge. The experiment provided the final proof that Watson and Crick’s double helix model was correct.21 Before Meselson-Stahl, many scientists were still skeptical about the double helix structure. But now they had concrete evidence that DNA really did work the way Watson and Crick proposed. The double helix went from being an interesting theory to accepted scientific fact.\nBut the bigger impact was methodological. Meselson’s density gradient centrifugation technique became one of the most important tools in molecular biology.22 Suddenly, scientists could separate different types of molecules based on their density. This opened up so many possibilities! Researchers started using it to purify viruses, separate different types of cells, isolate specific organelles from cells, and distinguish between different forms of DNA and RNA. It became a cornerstone technique that’s still used in labs around the world today.\nThe experiment also marked the transition from classical genetics to modern molecular biology.23 Before this, genetics was mostly about observing inheritance patterns and trying to figure out what was happening. After Meselson-Stahl, scientists could actually watch biological processes happening at the molecular level. It connected Mendel’s 19th century observations about inheritance to the actual physical machinery inside cells.24\nThis foundation made possible so many of the technologies we use today. CRISPR gene editing relies on understanding exactly how DNA replication works. PCR amplification, which is used in everything from COVID tests to crime scene analysis, is based on the semi-conservative replication mechanism they discovered. DNA fingerprinting for paternity tests and criminal investigations depends on knowing that DNA is inherited predictably from parents to children. Even personalized medicine is built on the understanding that genes are physical molecules that follow predictable rules.\nWhat really excites me is thinking about how one elegant experiment asking a simple question led to all these incredible applications that touch our lives every day. That’s the beauty of basic scientific research.",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html#how-i-wrote-this-blog-post",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html#how-i-wrote-this-blog-post",
    "title": "The Meselson-Stahl Experiment",
    "section": "",
    "text": "For this blog post, I wanted to experiment with deep research using solveit. I’ve used Gemini’s deep research feature before for a genomics project where I needed to learn about genes affected by breast cancer in mice, which is a topic I knew nothing about. Gemini provided a comprehensive report with research paper citations, but working with solveit felt more interactive and engaging.\nThe dialogue format made me more involved in the research process. Instead of receiving a finished report, I could think through questions and explore ideas as they came up. I started with only a vague plan to write about the Meselson-Stahl experiment, but through our conversation, I discovered what aspects I wanted to emphasize and developed a clearer focus.\nThis interactive approach helped me solidify my outline and gave me rich context for writing. The back-and-forth discussion became the foundation for the blog post itself. You can see the full dialogue here.",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2025-09-21-meselson-stahl/index.html#footnotes",
    "href": "blog/posts/2025-09-21-meselson-stahl/index.html#footnotes",
    "title": "The Meselson-Stahl Experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEmbryo Project Encyclopedia. “Hershey-Chase Experiments (1952): Alfred Hershey and Martha Chase.” Arizona State University. https://embryo.asu.edu/pages/hershey-chase-experiments-1952-alfred-hershey-and-martha-chase↩︎\nAvery, Oswald, Colin MacLeod, and Maclyn McCarty. “Studies on the Chemical Nature of the Substance Inducing Transformation of Pneumococcal Types: Induction of Transformation by a Desoxyribonucleic Acid Fraction Isolated from Pneumococcus Type III.” The Journal of Experimental Medicine 79 (1944): 137–58.↩︎\nHershey, Alfred D., and Martha Chase. “Independent Functions of Viral Protein and Nucleic Acid in Growth of Bacteriophage.” The Journal of General Physiology 36 (1952): 39–56.↩︎\nWatson, James D., and Francis H. C. Crick. “Molecular Structure of Nucleic Acids: A Structure for Deoxyribose Nucleic Acid.” Nature 171 (1953): 737–738.↩︎\nEmbryo Project Encyclopedia. “Categorization of Conservative, Semi-conservative, and Dispersive DNA Replication Theories (1953).” Arizona State University. https://embryo.asu.edu/pages/categorization-conservative-semi-conservative-and-dispersive-dna-replication-theories-1953↩︎\nNature Scitable. “Semi-Conservative DNA Replication: Meselson and Stahl.” Nature Education. https://www.nature.com/scitable/topicpage/semi-conservative-dna-replication-meselson-and-stahl-421/↩︎\nEmbryo Project Encyclopedia. “Meselson-Stahl Experiment (1957-1958): Matthew Meselson and Franklin Stahl.” Arizona State University. https://embryo.asu.edu/pages/meselson-stahl-experiment-1957-1958-matthew-meselson-and-franklin-stahl↩︎\nEmbryo Project Encyclopedia. “The Debate over DNA Replication: The Meselson-Stahl Experiment (1953-1957).” Arizona State University. https://embryo.asu.edu/pages/debate-over-dna-replication-meselson-stahl-experiment-1953-1957↩︎\nWikipedia. “Meselson–Stahl experiment.” https://en.wikipedia.org/wiki/Meselson–Stahl_experiment↩︎\nScience Direct. “Semiconservative Replication.” https://www.sciencedirect.com/topics/medicine-and-dentistry/semiconservative-replication↩︎\nNature Scitable. “Semi-Conservative DNA Replication: Meselson and Stahl.” Nature Education. https://www.nature.com/scitable/topicpage/semi-conservative-dna-replication-meselson-and-stahl-421/↩︎\nEmbryo Project Encyclopedia. “The Debate over DNA Replication: The Meselson-Stahl Experiment (1953-1957).” Arizona State University. https://embryo.asu.edu/pages/debate-over-dna-replication-meselson-stahl-experiment-1953-1957↩︎\nEmbryo Project Encyclopedia. “Matthew Stanley Meselson (1930- ).” Arizona State University. https://embryo.asu.edu/pages/matthew-stanley-meselson-1930↩︎\nExplore Biology. “How DNA Replicates.” https://explorebiology.org/collections/genetics/how-dna-replicates↩︎\nExplore Biology. “How DNA Replicates.” https://explorebiology.org/collections/genetics/how-dna-replicates↩︎\nEmbryo Project Encyclopedia. “Meselson-Stahl Experiment (1957-1958): Matthew Meselson and Franklin Stahl.” Arizona State University. https://embryo.asu.edu/pages/meselson-stahl-experiment-1957-1958-matthew-meselson-and-franklin-stahl↩︎\nEmbryo Project Encyclopedia. “Meselson-Stahl Experiment (1957-1958): Matthew Meselson and Franklin Stahl.” Arizona State University. https://embryo.asu.edu/pages/meselson-stahl-experiment-1957-1958-matthew-meselson-and-franklin-stahl↩︎\nExplore Biology. “How DNA Replicates.” https://explorebiology.org/collections/genetics/how-dna-replicates↩︎\nWikipedia. “Meselson–Stahl experiment.” https://en.wikipedia.org/wiki/Meselson–Stahl_experiment↩︎\nWikipedia. “Meselson–Stahl experiment.” https://en.wikipedia.org/wiki/Meselson–Stahl_experiment↩︎\nPNAS. “The double helix and the Nobel Prize.” https://pmc.ncbi.nlm.nih.gov/articles/PMC539798/↩︎\nVR Lab Academy. “Experiments That Changed the World: The Meselson-Stahl Experiment.” https://www.vrlabacademy.com/en/Blog/ExperimentsThatChanged↩︎\nExplore Biology. “How DNA Replicates.” https://explorebiology.org/collections/genetics/how-dna-replicates↩︎\nVR Lab Academy. “Experiments That Changed the World: The Meselson-Stahl Experiment.” https://www.vrlabacademy.com/en/Blog/ExperimentsThatChanged↩︎",
    "crumbs": [
      "Home",
      "Blog",
      "The Meselson-Stahl Experiment"
    ]
  },
  {
    "objectID": "blog/posts/2023-03-05-Fruit_Multi_pt1/index.html",
    "href": "blog/posts/2023-03-05-Fruit_Multi_pt1/index.html",
    "title": "Fruit Multi-Classifier pt.1 Training",
    "section": "",
    "text": "We will train a fruit classifier using multi-label classification. It is a part of further research part of fastbook chapter6. Instead of creating a bear classifier, we will classify three fruits: apple, orange, and pears.\n\n\n\nImage of an apple and a pear\n\n\nWhy fruits? While I was trying to find images on duckduckgo for teddy bear and grizzly bear together, I could not find them. On the other hand, I could find images with apple and pear easilly. I was also able to find many images with all three of them. Therefore, I chose fruits.\n\n\nWe start importing fastai library and other functions we will use to search and download images.\n\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\nfrom duckduckgo_search import ddg_images\n\ndef search_images(term, max_images=200): return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nWe can define a function, search_show_image, which searches the term, download, and shows the image. This way, we can check whether we’re getting the right images.\n\ndef search_show_image(term, dest=None):\n    \"\"\"Search, download, and show the image from the url.\"\"\"\n    url = search_images(term, max_images=1)[0]\n    if not dest: dest = term + '.jpg'\n    download_url(url, dest, show_progress=False)\n    im = Image.open(dest)\n    return im.to_thumb(256, 256)\n\n\nsearch_show_image('apple')\n\n\n\n\n\n\n\n\n\nsearch_show_image('orange')\n\n\n\n\n\n\n\n\n\nsearch_show_image('pear')\n\n\n\n\n\n\n\n\nWe can have multiple categories in an image. We can either have two categories or all three!\n\nsearch_show_image('apple and pear')\n\n\n\n\n\n\n\n\n\nsearch_show_image('apple and orange')\n\n\n\n\n\n\n\n\n\nsearch_show_image('pear and orange')\n\n\n\n\n\n\n\n\n\nsearch_show_image('apple and pear and orange')\n\n\n\n\n\n\n\n\nAnother challenge is predicting “no fruit.” We can search random images and use that for this purpose.\n\nsearch_show_image('random images')\n\n\n\n\n\n\n\n\nNow we start downloading images. We search all possible combinations of labels.\n\npath = Path('fruit_or_not')\n\n\nsearches = ('apple','pear', 'orange', \n            'apple and pear', 'apple and orange', 'pear and orange', \n            'apple and pear and orange',\n            'random images')\n\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=400, dest=path/o)\n\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nCheck and get rid of failed images.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n17\n\n\n\npath.ls()\n\n(#8) [Path('fruit_or_not/pear'),Path('fruit_or_not/apple and orange'),Path('fruit_or_not/apple and pear and orange'),Path('fruit_or_not/apple'),Path('fruit_or_not/orange'),Path('fruit_or_not/apple and pear'),Path('fruit_or_not/random images'),Path('fruit_or_not/pear and orange')]\n\n\n\n\n\nSince we have data, we start building a datablock. Since this is a multi-label classification, we use MultiCategoryBlock, instead of CategoryBlock. Another different thing is get_y. We can use parent_label for the labels. We will get apple, pear, and orange. But we also have problems. We will also get apple and pear and orange, which does not mean all three images for the model. So, we use a list of categories for the label. For instance, we will use ['apple'] for apple, ['apple, 'pear', 'orange'] for all three.\nSo, we define a function, get_y, which splits the names. We could also have used lambda function here. However, if we want to export our learner and load it later for inference, we have to define a function this way.\n\ndef get_y(path):\n    return parent_label(path).split(' and ')\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(seed=42),\n    get_y=get_y,\n    item_tfms=[Resize(192, method='squish')]\n)\n\n\npath.ls()\n\n(#8) [Path('fruit_or_not/pear'),Path('fruit_or_not/apple and orange'),Path('fruit_or_not/apple and pear and orange'),Path('fruit_or_not/apple'),Path('fruit_or_not/orange'),Path('fruit_or_not/apple and pear'),Path('fruit_or_not/random images'),Path('fruit_or_not/pear and orange')]\n\n\nBy using dblock.summary(path), we can see exactly what’s going on behind the scenes. For y, we got [random images], which was turned into TensorMultiCategory([3]), and also into TensorMultiCategory([0., 0., 0., 1.]) with OneHotEncode.\n\ndblock.summary(path)\n\nSetting-up type transforms pipelines\nCollecting items from fruit_or_not\nFound 1620 items\n2 datasets of sizes 1296,324\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      fruit_or_not/random images/a65bc8f7-8849-4207-ac8c-4a54bf119395.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=400x300\n  Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n    starting from\n      fruit_or_not/random images/a65bc8f7-8849-4207-ac8c-4a54bf119395.jpg\n    applying get_y gives\n      [random images]\n    applying MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorMultiCategory([3])\n    applying OneHotEncode -- {'c': None} gives\n      TensorMultiCategory([0., 0., 0., 1.])\n\nFinal sample: (PILImage mode=RGB size=400x300, TensorMultiCategory([0., 0., 0., 1.]))\n\n\nCollecting items from fruit_or_not\nFound 1620 items\n2 datasets of sizes 1296,324\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\nSetting up after_item: Pipeline: Resize -- {'size': (192, 192), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: Resize -- {'size': (192, 192), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=400x300, TensorMultiCategory([0., 0., 0., 1.]))\n    applying Resize -- {'size': (192, 192), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} gives\n      (PILImage mode=RGB size=192x192, TensorMultiCategory([0., 0., 0., 1.]))\n    applying ToTensor gives\n      (TensorImage of size 3x192x192, TensorMultiCategory([0., 0., 0., 1.]))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImage of size 4x3x192x192, TensorMultiCategory of size 4x4)\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x192x192, TensorMultiCategory of size 4x4)\n\n\nWith dblock, we create datasets. Before we move onto the next step, we take a look at x and y to actually check they look okay. We also check vocab.\n\ndsets = dblock.datasets(path)\n\n\nlen(dsets)\n\n1620\n\n\n\nx, y = dsets[550]\n\n\nx.to_thumb(256, 256)\n\n\n\n\n\n\n\n\n\ny\n\nTensorMultiCategory([1., 0., 0., 0.])\n\n\n\ndsets.vocab\n\n['apple', 'orange', 'pear', 'random images']\n\n\nEverything looks good, so we move on to dataloaders. We can use show_batch to take a look at images and labels respectively.\n\ndls = dblock.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n\n\n\nSome images may not be right since we downloaded them from a search engine. The best way to find out whether it will work or not is just start training and see how it performs!\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy_multi)\nlearn.lr_find()\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(5, 0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.661995\n0.422173\n0.854167\n00:07\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.313802\n0.438793\n0.861883\n00:08\n\n\n1\n0.226740\n0.426513\n0.868056\n00:08\n\n\n2\n0.166382\n0.364287\n0.891975\n00:08\n\n\n3\n0.119406\n0.320120\n0.905093\n00:08\n\n\n4\n0.086154\n0.311800\n0.903549\n00:08\n\n\n\n\n\nWe can optionally try to find the optimal value for thresh keyword, but it’s good where we are.\n\npreds, targs = learn.get_preds()\n\n\n\n\n\n\n\n\n\nxs = torch.linspace(0.1, .9, 30)\nys = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, ys);\n\n\n\n\n\n\n\n\nLet’s find out what our model had trouble with.\n\ninterp = ClassificationInterpretation.from_learner(learn)\n# interp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(15, nrows=3)\n\n\n\n\n\n\n\n\n\n\n\n\ntarget\npredicted\nprobabilities\nloss\n\n\n\n\n0\nrandom images\napple\nTensorBase([0.9999, 0.0204, 0.0098, 0.0011])\n3.993837356567383\n\n\n1\napple\nrandom images\nTensorBase([2.5680e-03, 1.7837e-03, 4.3446e-04, 9.9994e-01])\n3.9354634284973145\n\n\n2\napple\norange;pear\nTensorBase([1.1798e-03, 9.8064e-01, 9.0787e-01, 4.7235e-04])\n3.2679762840270996\n\n\n3\napple;orange\napple;orange;pear\nTensorBase([9.9813e-01, 9.9998e-01, 9.9999e-01, 7.8886e-05])\n2.7811737060546875\n\n\n4\napple;pear\norange\nTensorBase([2.5644e-01, 9.9927e-01, 8.3681e-02, 3.2027e-04])\n2.765456199645996\n\n\n5\napple;orange\napple;pear\nTensorBase([9.7966e-01, 2.2211e-02, 9.9570e-01, 2.1652e-04])\n2.319033622741699\n\n\n6\napple;orange\napple\nTensorBase([1.0000e+00, 1.2407e-04, 5.8573e-04, 9.8804e-05])\n2.248845338821411\n\n\n7\napple;orange\npear\nTensorBase([0.4713, 0.4008, 0.9990, 0.0017])\n2.1484780311584473\n\n\n8\napple;pear\napple;orange\nTensorBase([8.5449e-01, 5.0590e-01, 7.0017e-04, 2.9679e-03])\n2.032358407974243\n\n\n9\napple;pear\norange;pear\nTensorBase([3.6480e-03, 8.5574e-01, 9.9771e-01, 7.9937e-04])\n1.8881945610046387\n\n\n10\napple;orange\nrandom images\nTensorBase([0.2633, 0.0248, 0.0345, 0.9077])\n1.862053394317627\n\n\n11\norange;pear\napple;orange\nTensorBase([9.9609e-01, 9.9997e-01, 2.2600e-01, 3.7492e-04])\n1.7576959133148193\n\n\n12\norange;pear\napple;orange;pear\nTensorBase([0.9990, 0.9999, 1.0000, 0.0012])\n1.729599118232727\n\n\n13\napple\nrandom images\nTensorBase([0.0842, 0.0110, 0.0195, 0.9838])\n1.6565606594085693\n\n\n14\norange\norange;pear\nTensorBase([0.0480, 0.9988, 0.9984, 0.0012])\n1.628658652305603\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like our model is doing pretty good. The ones it missed are not correctly labelled or hard to figure out myself. We can test some images we downloaded in the beginning of this notebook\n\nis_fruit,_,probs = learn.predict(PILImage.create('apple.jpg'))\nis_fruit\n\n\n\n\n\n\n\n\n(#1) ['apple']\n\n\n\nis_fruit,_,probs = learn.predict(PILImage.create('random images.jpg'))\nis_fruit, probs\n\n\n\n\n\n\n\n\n((#1) ['random images'],\n TensorBase([2.0158e-03, 4.0127e-04, 5.0894e-04, 1.0000e+00]))\n\n\nLooks good! We are ready to deploy it!\n\nlearn.export()\n\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]",
    "crumbs": [
      "Home",
      "Blog",
      "Fruit Multi-Classifier pt.1 Training"
    ]
  },
  {
    "objectID": "blog/posts/2023-03-05-Fruit_Multi_pt1/index.html#import-libraries-and-download-data",
    "href": "blog/posts/2023-03-05-Fruit_Multi_pt1/index.html#import-libraries-and-download-data",
    "title": "Fruit Multi-Classifier pt.1 Training",
    "section": "",
    "text": "We start importing fastai library and other functions we will use to search and download images.\n\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\nfrom duckduckgo_search import ddg_images\n\ndef search_images(term, max_images=200): return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nWe can define a function, search_show_image, which searches the term, download, and shows the image. This way, we can check whether we’re getting the right images.\n\ndef search_show_image(term, dest=None):\n    \"\"\"Search, download, and show the image from the url.\"\"\"\n    url = search_images(term, max_images=1)[0]\n    if not dest: dest = term + '.jpg'\n    download_url(url, dest, show_progress=False)\n    im = Image.open(dest)\n    return im.to_thumb(256, 256)\n\n\nsearch_show_image('apple')\n\n\n\n\n\n\n\n\n\nsearch_show_image('orange')\n\n\n\n\n\n\n\n\n\nsearch_show_image('pear')\n\n\n\n\n\n\n\n\nWe can have multiple categories in an image. We can either have two categories or all three!\n\nsearch_show_image('apple and pear')\n\n\n\n\n\n\n\n\n\nsearch_show_image('apple and orange')\n\n\n\n\n\n\n\n\n\nsearch_show_image('pear and orange')\n\n\n\n\n\n\n\n\n\nsearch_show_image('apple and pear and orange')\n\n\n\n\n\n\n\n\nAnother challenge is predicting “no fruit.” We can search random images and use that for this purpose.\n\nsearch_show_image('random images')\n\n\n\n\n\n\n\n\nNow we start downloading images. We search all possible combinations of labels.\n\npath = Path('fruit_or_not')\n\n\nsearches = ('apple','pear', 'orange', \n            'apple and pear', 'apple and orange', 'pear and orange', \n            'apple and pear and orange',\n            'random images')\n\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=400, dest=path/o)\n\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nCheck and get rid of failed images.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n17\n\n\n\npath.ls()\n\n(#8) [Path('fruit_or_not/pear'),Path('fruit_or_not/apple and orange'),Path('fruit_or_not/apple and pear and orange'),Path('fruit_or_not/apple'),Path('fruit_or_not/orange'),Path('fruit_or_not/apple and pear'),Path('fruit_or_not/random images'),Path('fruit_or_not/pear and orange')]",
    "crumbs": [
      "Home",
      "Blog",
      "Fruit Multi-Classifier pt.1 Training"
    ]
  },
  {
    "objectID": "blog/posts/2023-03-05-Fruit_Multi_pt1/index.html#datablock",
    "href": "blog/posts/2023-03-05-Fruit_Multi_pt1/index.html#datablock",
    "title": "Fruit Multi-Classifier pt.1 Training",
    "section": "",
    "text": "Since we have data, we start building a datablock. Since this is a multi-label classification, we use MultiCategoryBlock, instead of CategoryBlock. Another different thing is get_y. We can use parent_label for the labels. We will get apple, pear, and orange. But we also have problems. We will also get apple and pear and orange, which does not mean all three images for the model. So, we use a list of categories for the label. For instance, we will use ['apple'] for apple, ['apple, 'pear', 'orange'] for all three.\nSo, we define a function, get_y, which splits the names. We could also have used lambda function here. However, if we want to export our learner and load it later for inference, we have to define a function this way.\n\ndef get_y(path):\n    return parent_label(path).split(' and ')\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(seed=42),\n    get_y=get_y,\n    item_tfms=[Resize(192, method='squish')]\n)\n\n\npath.ls()\n\n(#8) [Path('fruit_or_not/pear'),Path('fruit_or_not/apple and orange'),Path('fruit_or_not/apple and pear and orange'),Path('fruit_or_not/apple'),Path('fruit_or_not/orange'),Path('fruit_or_not/apple and pear'),Path('fruit_or_not/random images'),Path('fruit_or_not/pear and orange')]\n\n\nBy using dblock.summary(path), we can see exactly what’s going on behind the scenes. For y, we got [random images], which was turned into TensorMultiCategory([3]), and also into TensorMultiCategory([0., 0., 0., 1.]) with OneHotEncode.\n\ndblock.summary(path)\n\nSetting-up type transforms pipelines\nCollecting items from fruit_or_not\nFound 1620 items\n2 datasets of sizes 1296,324\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      fruit_or_not/random images/a65bc8f7-8849-4207-ac8c-4a54bf119395.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=400x300\n  Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n    starting from\n      fruit_or_not/random images/a65bc8f7-8849-4207-ac8c-4a54bf119395.jpg\n    applying get_y gives\n      [random images]\n    applying MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorMultiCategory([3])\n    applying OneHotEncode -- {'c': None} gives\n      TensorMultiCategory([0., 0., 0., 1.])\n\nFinal sample: (PILImage mode=RGB size=400x300, TensorMultiCategory([0., 0., 0., 1.]))\n\n\nCollecting items from fruit_or_not\nFound 1620 items\n2 datasets of sizes 1296,324\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\nSetting up after_item: Pipeline: Resize -- {'size': (192, 192), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: Resize -- {'size': (192, 192), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=400x300, TensorMultiCategory([0., 0., 0., 1.]))\n    applying Resize -- {'size': (192, 192), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} gives\n      (PILImage mode=RGB size=192x192, TensorMultiCategory([0., 0., 0., 1.]))\n    applying ToTensor gives\n      (TensorImage of size 3x192x192, TensorMultiCategory([0., 0., 0., 1.]))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImage of size 4x3x192x192, TensorMultiCategory of size 4x4)\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x192x192, TensorMultiCategory of size 4x4)\n\n\nWith dblock, we create datasets. Before we move onto the next step, we take a look at x and y to actually check they look okay. We also check vocab.\n\ndsets = dblock.datasets(path)\n\n\nlen(dsets)\n\n1620\n\n\n\nx, y = dsets[550]\n\n\nx.to_thumb(256, 256)\n\n\n\n\n\n\n\n\n\ny\n\nTensorMultiCategory([1., 0., 0., 0.])\n\n\n\ndsets.vocab\n\n['apple', 'orange', 'pear', 'random images']\n\n\nEverything looks good, so we move on to dataloaders. We can use show_batch to take a look at images and labels respectively.\n\ndls = dblock.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n\n\n\nSome images may not be right since we downloaded them from a search engine. The best way to find out whether it will work or not is just start training and see how it performs!\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy_multi)\nlearn.lr_find()\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(5, 0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.661995\n0.422173\n0.854167\n00:07\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.313802\n0.438793\n0.861883\n00:08\n\n\n1\n0.226740\n0.426513\n0.868056\n00:08\n\n\n2\n0.166382\n0.364287\n0.891975\n00:08\n\n\n3\n0.119406\n0.320120\n0.905093\n00:08\n\n\n4\n0.086154\n0.311800\n0.903549\n00:08\n\n\n\n\n\nWe can optionally try to find the optimal value for thresh keyword, but it’s good where we are.\n\npreds, targs = learn.get_preds()\n\n\n\n\n\n\n\n\n\nxs = torch.linspace(0.1, .9, 30)\nys = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, ys);\n\n\n\n\n\n\n\n\nLet’s find out what our model had trouble with.\n\ninterp = ClassificationInterpretation.from_learner(learn)\n# interp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(15, nrows=3)\n\n\n\n\n\n\n\n\n\n\n\n\ntarget\npredicted\nprobabilities\nloss\n\n\n\n\n0\nrandom images\napple\nTensorBase([0.9999, 0.0204, 0.0098, 0.0011])\n3.993837356567383\n\n\n1\napple\nrandom images\nTensorBase([2.5680e-03, 1.7837e-03, 4.3446e-04, 9.9994e-01])\n3.9354634284973145\n\n\n2\napple\norange;pear\nTensorBase([1.1798e-03, 9.8064e-01, 9.0787e-01, 4.7235e-04])\n3.2679762840270996\n\n\n3\napple;orange\napple;orange;pear\nTensorBase([9.9813e-01, 9.9998e-01, 9.9999e-01, 7.8886e-05])\n2.7811737060546875\n\n\n4\napple;pear\norange\nTensorBase([2.5644e-01, 9.9927e-01, 8.3681e-02, 3.2027e-04])\n2.765456199645996\n\n\n5\napple;orange\napple;pear\nTensorBase([9.7966e-01, 2.2211e-02, 9.9570e-01, 2.1652e-04])\n2.319033622741699\n\n\n6\napple;orange\napple\nTensorBase([1.0000e+00, 1.2407e-04, 5.8573e-04, 9.8804e-05])\n2.248845338821411\n\n\n7\napple;orange\npear\nTensorBase([0.4713, 0.4008, 0.9990, 0.0017])\n2.1484780311584473\n\n\n8\napple;pear\napple;orange\nTensorBase([8.5449e-01, 5.0590e-01, 7.0017e-04, 2.9679e-03])\n2.032358407974243\n\n\n9\napple;pear\norange;pear\nTensorBase([3.6480e-03, 8.5574e-01, 9.9771e-01, 7.9937e-04])\n1.8881945610046387\n\n\n10\napple;orange\nrandom images\nTensorBase([0.2633, 0.0248, 0.0345, 0.9077])\n1.862053394317627\n\n\n11\norange;pear\napple;orange\nTensorBase([9.9609e-01, 9.9997e-01, 2.2600e-01, 3.7492e-04])\n1.7576959133148193\n\n\n12\norange;pear\napple;orange;pear\nTensorBase([0.9990, 0.9999, 1.0000, 0.0012])\n1.729599118232727\n\n\n13\napple\nrandom images\nTensorBase([0.0842, 0.0110, 0.0195, 0.9838])\n1.6565606594085693\n\n\n14\norange\norange;pear\nTensorBase([0.0480, 0.9988, 0.9984, 0.0012])\n1.628658652305603\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like our model is doing pretty good. The ones it missed are not correctly labelled or hard to figure out myself. We can test some images we downloaded in the beginning of this notebook\n\nis_fruit,_,probs = learn.predict(PILImage.create('apple.jpg'))\nis_fruit\n\n\n\n\n\n\n\n\n(#1) ['apple']\n\n\n\nis_fruit,_,probs = learn.predict(PILImage.create('random images.jpg'))\nis_fruit, probs\n\n\n\n\n\n\n\n\n((#1) ['random images'],\n TensorBase([2.0158e-03, 4.0127e-04, 5.0894e-04, 1.0000e+00]))\n\n\nLooks good! We are ready to deploy it!\n\nlearn.export()\n\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]",
    "crumbs": [
      "Home",
      "Blog",
      "Fruit Multi-Classifier pt.1 Training"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-19-Initialization_part_2/index.html",
    "href": "blog/posts/2023-10-19-Initialization_part_2/index.html",
    "title": "Initialization part 2",
    "section": "",
    "text": "This blog will teach us about Layer-wise Sequential Unit-Variance (LSUV), layer normalization, and batch normalization. This blog is based on lesson 17 of the FastAI course.\nLSUV is a straightforward technique for initializing weights. Layer norm and batch norm do not initialize weights before the training, but they help the model train by modifying the statistics of the weights.\nFirst, we import libraries and do required setups.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n!pip -q install torcheval\n!pip -q install datasets\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport sys,gc,traceback\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.xtras import *\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 7.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 14.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 16.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 14.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 24.5 MB/s eta 0:00:00\n\n\nJust like the last time, we will use pytorch datasets. But this time, we will normalize the dataset before we create the dataloaders.\n\nxmean, xstd = (tensor(0.29), tensor(0.35))\n\n\ndef batch_tfm(img): return (ToTensor()(img) - xmean) / xstd\n\n\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ntrn_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=True,\n    download=True,\n    transform=batch_tfm\n)\n\nval_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=False,\n    download=True,\n    transform=batch_tfm\n)\n\n\nfrom torch.utils.data import DataLoader\nset_seed(42)\nbs = 1024\n\ntrn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=bs, shuffle=True)\ndls = DataLoaders(trn_dl, val_dl)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))\n\n\n\nxb.mean(), xb.std()\n\n(tensor(-0.01), tensor(1.01))\n\n\n\nclass GeneralRelu(nn.Module):\n    def __init__(self, leaky=None, sub=0., max_val=None):\n        super().__init__()\n        self.relu = nn.ReLU() if leaky is None else nn.LeakyReLU(leaky)\n        self.sub = sub\n        self.max_val = max_val\n\n    def forward(self, x):\n        x = self.relu(x)\n        x -= self.sub\n        if self.max_val is not None:\n            x.clamp_max_(self.max_val)\n        return x\n\n\ndef conv(ni, nf, ks=3, stride=2, act=None):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act is not None: res = nn.Sequential(res, act())\n    return res\n\n\ndef get_model(act=nn.ReLU, filters=None):\n    if filters is None: filters = [1, 8, 16, 32, 64]\n    layers = [conv(filters[i], filters[i+1], act=act) for i in range(len(filters)-1)]\n    return nn.Sequential(*layers, conv(filters[-1], 10, act=None), nn.Flatten()).to(def_device)",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 2"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-19-Initialization_part_2/index.html#initializing-weights-part-2",
    "href": "blog/posts/2023-10-19-Initialization_part_2/index.html#initializing-weights-part-2",
    "title": "Initialization part 2",
    "section": "",
    "text": "This blog will teach us about Layer-wise Sequential Unit-Variance (LSUV), layer normalization, and batch normalization. This blog is based on lesson 17 of the FastAI course.\nLSUV is a straightforward technique for initializing weights. Layer norm and batch norm do not initialize weights before the training, but they help the model train by modifying the statistics of the weights.\nFirst, we import libraries and do required setups.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n!pip -q install torcheval\n!pip -q install datasets\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport sys,gc,traceback\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.xtras import *\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 7.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 14.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 16.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 14.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 24.5 MB/s eta 0:00:00\n\n\nJust like the last time, we will use pytorch datasets. But this time, we will normalize the dataset before we create the dataloaders.\n\nxmean, xstd = (tensor(0.29), tensor(0.35))\n\n\ndef batch_tfm(img): return (ToTensor()(img) - xmean) / xstd\n\n\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ntrn_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=True,\n    download=True,\n    transform=batch_tfm\n)\n\nval_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=False,\n    download=True,\n    transform=batch_tfm\n)\n\n\nfrom torch.utils.data import DataLoader\nset_seed(42)\nbs = 1024\n\ntrn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=bs, shuffle=True)\ndls = DataLoaders(trn_dl, val_dl)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))\n\n\n\nxb.mean(), xb.std()\n\n(tensor(-0.01), tensor(1.01))\n\n\n\nclass GeneralRelu(nn.Module):\n    def __init__(self, leaky=None, sub=0., max_val=None):\n        super().__init__()\n        self.relu = nn.ReLU() if leaky is None else nn.LeakyReLU(leaky)\n        self.sub = sub\n        self.max_val = max_val\n\n    def forward(self, x):\n        x = self.relu(x)\n        x -= self.sub\n        if self.max_val is not None:\n            x.clamp_max_(self.max_val)\n        return x\n\n\ndef conv(ni, nf, ks=3, stride=2, act=None):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act is not None: res = nn.Sequential(res, act())\n    return res\n\n\ndef get_model(act=nn.ReLU, filters=None):\n    if filters is None: filters = [1, 8, 16, 32, 64]\n    layers = [conv(filters[i], filters[i+1], act=act) for i in range(len(filters)-1)]\n    return nn.Sequential(*layers, conv(filters[-1], 10, act=None), nn.Flatten()).to(def_device)",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 2"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-19-Initialization_part_2/index.html#lsuv-layer-sequential-unit-variance",
    "href": "blog/posts/2023-10-19-Initialization_part_2/index.html#lsuv-layer-sequential-unit-variance",
    "title": "Initialization part 2",
    "section": "LSUV (layer-sequential unit variance)",
    "text": "LSUV (layer-sequential unit variance)\nLSUV came from All you need is a good init. This has a very complicated name, but it is a straightforward idea. The good thing about this paper is that it does not have complicated math like other papers, so it is suitable for beginners to read. Simply put, regardless of what kind of activation layer we use, we can initialize weights correctly for training. Here, initializing correctly means making a mean of zero and a standard deviation of one, which allows us to train fast.\nFor different activation layers, we have to use variety of initialization techniques. For instance, when using relu, we use kaiming initialization, and if there is no activation layer, we can just use Xavier init. Instead of using different activation methods for them, we can just use LSUV to initialize the weights. Let’s look at the pseudo code from the paper and create it ourselves.\nHere is the pseudo code from the paper:\nPre-initialize network with orthonormal matrices as in Saxe et al. (2014)\nfor each layer L do\n    while |Var(BL) − 1.0| ≥ Tolvar and (Ti &lt; Tmax) do\n        do Forward pass with a mini-batch\n        calculate Var(BL)\n        WL = WL / sqrt(Var(BL))\n    end while\nend for\n\nL is convolution or full-connected layer\nVar(x) takes variance of input x.\nBL is output blob from L.\nTolvar is variance tolerance\nTi is current trial. Incremented every loop.\nTmax is max trial\nWL is weights\nsqrt(x) takes square root of input x.\n\nAccording to the paper, we call the model on a batch of data to find the standard deviation of the weights. Then, we divide the weight by the standard deviation to set them closer to 1. If it is not close enough, we repeat the process until it is within our tolerance, Tolvar. We also increment Ti in each loop, and if it reaches Tmax, we stop the loop. However, in practice, it never reached Tmax. That’s it. Let’s implement it ourselves.\n\nset_seed(1)\ngr = partial(GeneralRelu, .2, .4)\nmodel = get_model(gr)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy(device='cpu'))\nastats = ActivationStats(mod_filter=fc.risinstance(GeneralRelu), plot=True)\ncbs = [metrics, astats, DeviceCB(), ProgressCB()]\n\nTo get the statistics of the data, we use Pytorch hooks. For our initialization function, we take act for the activation function and conv for convolutional layers. So, we calculate statistics from act output and update conv weights. After initialization, we remove hooks to clean up.\n\ndef good_init_hook(hook, m, inp, outp):\n    x = to_cpu(outp)\n    hook.mean, hook.std = x.mean(), x.std()\n\n\ndef good_init_no_bias(model, act, conv, xb, tol=1e-2):\n    h = Hook(act, good_init_hook)\n    with torch.no_grad():\n        model(xb)\n        while abs(h.std - 1.0) &gt; tol:\n            model(xb)\n            conv.weight.data /= h.std\n    h.remove()\n\n\ngrs = [o for o in model.modules() if isinstance(o, GeneralRelu)]\nconvs = [o for o in model.modules() if isinstance(o, nn.Conv2d)]\n\n\nfor m, m_in in zip(grs, convs):\n    good_init_no_bias(model, m, m_in, xb, tol=0.001)\n\n\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.781\n0.612\n0\nTrue\n00:17\n\n\n0.844\n0.437\n0\nFalse\n00:02\n\n\n0.865\n0.369\n1\nTrue\n00:16\n\n\n0.863\n0.374\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, we can also set the mean of the weights to zero like lesson 17 of FastAI.\n\nset_seed(1)\nmodel = get_model(gr)\n\n\ndef good_init(model, act, conv, xb, tol=1e-3):\n    h = Hook(act, good_init_hook)\n    with torch.no_grad():\n        model(xb)\n        while abs(h.std - 1.0) &gt; tol and abs(h.mean) &gt; tol:\n            model(xb)\n            conv.weight.data /= h.std\n            conv.bias.data -= h.mean\n    h.remove()\n\n\ngrs = [o for o in model.modules() if isinstance(o, GeneralRelu)]\nconvs = [o for o in model.modules() if isinstance(o, nn.Conv2d)]\ngrs, convs\n\n([GeneralRelu(\n    (relu): LeakyReLU(negative_slope=0.2)\n  ),\n  GeneralRelu(\n    (relu): LeakyReLU(negative_slope=0.2)\n  ),\n  GeneralRelu(\n    (relu): LeakyReLU(negative_slope=0.2)\n  ),\n  GeneralRelu(\n    (relu): LeakyReLU(negative_slope=0.2)\n  )],\n [Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n  Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n  Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))])\n\n\n\nconvs[0].weight.std(), convs[0].bias\n\n(tensor(0.18, grad_fn=&lt;StdBackward0&gt;),\n Parameter containing:\n tensor([ 0.16,  0.02,  0.11,  0.07,  0.12,  0.17, -0.31,  0.17], requires_grad=True))\n\n\n\nfor m, m_in in zip(grs, convs):\n    good_init(model, m, m_in, xb, tol=0.001)\n\n\nconvs[0].weight.std(), convs[0].bias\n\n(tensor(0.65, grad_fn=&lt;StdBackward0&gt;),\n Parameter containing:\n tensor([ 0.03, -0.11, -0.02, -0.05, -0.01,  0.04, -0.44,  0.04], requires_grad=True))\n\n\n\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.783\n0.607\n0\nTrue\n00:17\n\n\n0.843\n0.438\n0\nFalse\n00:02\n\n\n0.865\n0.368\n1\nTrue\n00:18\n\n\n0.865\n0.371\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nThe initialization worked, and our model trained well. However, it requires many lines of code. So, let’s create a callback! Because we only want to fit once, we track whether the model’s been fitted. We also have an option to initialize bias or not. We do not need bias when we use batch normalization or layer normalization later.\n\nclass LSUVCB(Callback):\n    def __init__(self, bias=True):\n        self.fit_ = False\n        self.bias = bias\n\n    def before_fit(self, learn):\n        if not self.fit_:\n            self.fit_ = True\n            model = learn.model\n            grs = [o for o in model.modules() if isinstance(o, GeneralRelu)]\n            convs = [o for o in model.modules() if isinstance(o, nn.Conv2d)]\n            for m, m_in in zip(grs, convs):\n                init = good_init if self.bias else good_init_no_bias\n                init(model, m, m_in, xb, tol=0.001)\n\n\nset_seed(1)\nmodel = get_model(gr)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=.2, cbs=cbs+[LSUVCB()])\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.783\n0.607\n0\nTrue\n00:16\n\n\n0.843\n0.438\n0\nFalse\n00:01\n\n\n0.865\n0.368\n1\nTrue\n00:17\n\n\n0.865\n0.371\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nWith our LSUVCB, we can do the same thing very easily. When we look at the statistics of the activations, we can see the activations move to the mean of zero and the standard deviation of one.",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 2"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-19-Initialization_part_2/index.html#layer-normalization",
    "href": "blog/posts/2023-10-19-Initialization_part_2/index.html#layer-normalization",
    "title": "Initialization part 2",
    "section": "Layer Normalization",
    "text": "Layer Normalization\nBefore we start looking at batch normalization, we will look at layer norm, a simpler version of batch norm. Layer Normalization by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton is math-heavy and challenging to understand. However, the code is straightforward.\nAlthough batch norm performs very well, it has a couple of limitations. It was computationally expensive, cannot be used for recurrent neural networks, and cannot have models with small batch sizes. To fix these issues, layer norm uses fewer parameters to train with a more straightforward design.\nWhat does LayerNorm do? It has learnable parameters, self.mult and self.add. For each batch, it calculates means and standard deviations over the channels, heights, and weights for each batch size. Then, we normalize the weights to a mean of zero and a standard deviation of one. However, this might not be the ideal place for the the weights for training. That’s where self.mult and self.add come in to shift the weights to better places. To define new parameters in Pytorch, we use nn.Parameter.\nWe also take eps as a parameter. This prevents weights from getting too big when the standard deviation gets too low. dummy parameter does not do anything here, but it will be used for batch norm.\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dummy, eps = 1e-4):\n        super().__init__()\n        self.eps = eps\n        self.mult = nn.Parameter(tensor(1.))\n        self.add = nn.Parameter(tensor(0.))\n\n    def forward(self, x):\n        mean = x.mean((1,2,3), keepdim=True)  # NCHW\n        std  = x.std((1,2,3), keepdim=True)\n        x = (x - mean) / (std+self.eps).sqrt()\n        return (x * self.mult) + self.add\n\nTo use LayerNorm and BatchNorm, we must slightly modify the conv. It takes norm and bias parameters because these techniques do not need bias because we normalize each batch using self.mult and self.add. Basically, self.add acts as bias.\n\ndef conv(ni, nf, ks=3, stride=2, act=None, norm=None, bias=None):\n    if bias is None: bias = isinstance(norm, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))\n    res = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias)]\n    if norm is not None: res.append(norm(nf))\n    if act is not None: res.append(act())\n    return nn.Sequential(*res)\n\n\ndef get_model(act=nn.ReLU, norm=None, filters=None):\n    if filters is None: filters = [1, 8, 16, 32, 64]\n    layers = [conv(filters[i], filters[i+1], act=act, norm=norm) for i in range(len(filters)-1)]\n    return nn.Sequential(*layers, conv(filters[-1], 10, act=None), nn.Flatten()).to(def_device)\n\n\nset_seed(1)\nmodel = get_model(gr, norm=LayerNorm)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.711\n0.831\n0\nTrue\n00:21\n\n\n0.817\n0.499\n0\nFalse\n00:02\n\n\n0.841\n0.439\n1\nTrue\n00:20\n\n\n0.847\n0.428\n1\nFalse\n00:02\n\n\n0.864\n0.377\n2\nTrue\n00:22\n\n\n0.847\n0.411\n2\nFalse\n00:02\n\n\n0.876\n0.340\n3\nTrue\n00:21\n\n\n0.868\n0.358\n3\nFalse\n00:02\n\n\n0.882\n0.320\n4\nTrue\n00:20\n\n\n0.870\n0.353\n4\nFalse\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nI wondered what would happen if I take means and standard deviations like how we would do in batch norm, and it performed better.\n\nclass MyNorm(nn.Module):\n    def __init__(self, dummy, eps = 1e-4):\n        super().__init__()\n        self.eps = eps\n        self.mult = nn.Parameter(tensor(1.))\n        self.add = nn.Parameter(tensor(0.))\n\n    def forward(self, x):\n        mean = x.mean((0,2,3), keepdim=True)  # NCHW\n        std  = x.std((0,2,3), keepdim=True)\n        x = (x - mean) / (std+self.eps).sqrt()\n        return (x * self.mult) + self.add\n\n\nset_seed(1)\nmodel = get_model(gr, norm=MyNorm)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.747\n0.739\n0\nTrue\n00:22\n\n\n0.832\n0.459\n0\nFalse\n00:02\n\n\n0.860\n0.388\n1\nTrue\n00:21\n\n\n0.857\n0.392\n1\nFalse\n00:02\n\n\n0.875\n0.340\n2\nTrue\n00:23\n\n\n0.866\n0.358\n2\nFalse\n00:02\n\n\n0.888\n0.306\n3\nTrue\n00:23\n\n\n0.876\n0.335\n3\nFalse\n00:02\n\n\n0.893\n0.286\n4\nTrue\n00:23\n\n\n0.883\n0.318\n4\nFalse\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nWe did not initialize the weights before training. However, layer norm normalizes every batch, so it is optional. But let’s try initializing the weights before we train.\n\ndef init_weights(m, leaky=0.):\n    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)): init.kaiming_normal_(m.weights, a=leaky)\n\n\niw = partial(init_weights, .2)\n\n\nset_seed(1)\nmodel = get_model(gr, norm=LayerNorm)\nlearn = MomentumLearner(model.apply(iw), dls, F.cross_entropy, lr=.2, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.785\n0.595\n0\nTrue\n00:15\n\n\n0.848\n0.415\n0\nFalse\n00:01\n\n\n0.867\n0.361\n1\nTrue\n00:15\n\n\n0.863\n0.367\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nThere is little difference between initializing or not, at least in this case.",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 2"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-19-Initialization_part_2/index.html#batch-normalization",
    "href": "blog/posts/2023-10-19-Initialization_part_2/index.html#batch-normalization",
    "title": "Initialization part 2",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nBatch norm emerged in Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. This paper is more challenging to read than “All You Need Is Good Init” but can still be understood. Math can be confusing, but it has pseudo code for algorithms, which is easier to read. For an easier version of the paper with diagrams, take a look at a medium article.\nI think this paper is awesome how the authors thought covariate shift was the problem of training and found a way to manage it. They normalized each batch, which helped the model to train. It also had learnable parameters that can shift and scale the distribution to more easily trainable distribution.\nWe can think of the neural net as a chef and the transformation as a server. A chef can take orders, cook, and serve at the same time if the restaurant is not busy. However, as more customers come in, a server can help so that the chef can focus on cooking. Instead of the neural nets to find magic parameters for the minimum loss, we help them with finding the best distribution for the weights using those learnable parameters.\nHowever, batch norm makes training epochs slower. Also, this is quite a complicated algorithm with learnable parameters and has more complex inference as we will see later.\n\nMore in depth of the paper:\nThose who want to skip this part can still understand its importance and the code without learning the underlying math. However, we will dig into some math and equations for those who are curious.\nThe paper defines Internal Covariate Shift as the change in the distributions of internal nodes of a deep network during training. Why is this a problem? To train fast, the inputs of each layer have to have zero means and unit variances without correlation according to Efficient Backprop by LeCun et al. However, when we train, statistics of the inputs change due to updating parameters.\nFirst thing the authors tried to fix this issue is to update the statistics after the stochastic gradient descent step to train by normalization. However, the bias grew too much because the optimization did not know about the normalization. So, the normalization had to be part of the gradient step process.\nSo, they suggested using the whole data to normalize. However, this is too expensive, so they use mini-batch statistics assuming these represent statistics for the whole data.\nThe paper says, a layer with d-dimensional input \\(\\mathbf{x}=(x^{(1)}\\ldots x^{(d)})\\), we normalize each dimension\n\\[ \\hat{x}^{(k)} = \\frac{x^{(k)}-\\operatorname {E}[x^{(k)}]}{\\sqrt{\\operatorname {Var}[x^{(k)}]}} \\tag{1} \\]\nwhere \\(\\operatorname {E}[x^{(k)}]\\) is expected value and \\(\\operatorname {Var}[x^{(k)}]\\) is variance over the training data set. The expected value is an average, and there is more information on wikipedia.\nThen, we apply the transformation to each dimension:\n\\[y^{(k)} = \\gamma^{(k)}\\hat{x} + \\beta^{(k)} \\tag{2} \\]\nwhere \\(\\gamma^{(k)}\\) (gamma) and \\(\\beta^{(k)}\\) (beta) are learnable parameters.\nLet’s look at the normalizing part of Algorithm 1. This looks familiar:\n\\[ \\hat{x}_i \\leftarrow \\frac{x_i-\\operatorname \\mu_B}{\\sqrt{\\sigma^2_B+\\epsilon}} \\tag{3}\\]\nThis is almost the same thing as equation (1). For this one, we omit (k) for simplicity and readability. Instead of using \\(\\operatorname {E}\\) or \\(\\operatorname {Var}\\), we use Greek letters here for some reason. \\(\\mu_B\\) (mu) represents an average over the batch, \\(\\sigma^{2}_B\\) (sigma) is a variance over the batch. \\(\\epsilon\\) (epsilon) is used for numerical stability just in case our variance gets too small. After normalization, we scale and shift using the equation (2).\nNow, let’s move on to Algorithm 2 in section 3.1. This section is all about inference. For inference, we use the population (the whole thing) instead of using batches to calculate means and variances. We use moving averages to calculate these.\nIn Algorithm 2, the first part is training, and the second is inference. Although number 10 says, “process multiple training mini-batches \\(B\\), …” we calculate these during the training. The last step was a bit confusing to me when I first encountered this:\n\\[ y = \\frac{\\gamma}{\\sqrt{\\operatorname{Var}[x]+\\epsilon}}\\cdot x + (\\beta - \\frac{\\gamma\\operatorname{E}[x]}{\\sqrt{\\operatorname{Var}[x]+\\epsilon}}) \\tag{4}\\]\nThis is the same as the normalization step combined with scale and shift in Algorithm 1. We can rewrite equation (4) as the following:\n\\[ y=\\gamma(\\frac{x-\\operatorname{E}[x]}{\\sqrt{\\operatorname{Var}[x]+\\epsilon}}) + \\beta \\tag{5}\\]\nIt’s more clear this way.\n\n\nBatch norm code\nThat’s it. Now let’s take a look at the code from FastAI course:\n\nclass BatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1, eps=1e-5):\n        super().__init__()\n        # NB: pytorch bn mom is opposite of what you'd expect\n        self.mom,self.eps = mom,eps\n        self.mults = nn.Parameter(torch.ones (nf,1,1))             # Used for both training and inference (gamma)\n        self.adds  = nn.Parameter(torch.zeros(nf,1,1))             # Used for both training and inference (beta)\n        self.register_buffer('vars',  torch.ones(1,nf,1,1))        # Only used for inference (Var[x])\n        self.register_buffer('means', torch.zeros(1,nf,1,1))       # Only used for inference (E[x])\n\n    def update_stats(self, x):\n        m = x.mean((0,2,3), keepdim=True)  # NCHW\n        v = x.var ((0,2,3), keepdim=True)\n        self.means.lerp_(m, self.mom)                              # Weighted average\n        self.vars.lerp_ (v, self.mom)\n        return m,v\n\n    def forward(self, x):\n        if self.training:\n            with torch.no_grad(): m,v = self.update_stats(x)\n        else: m,v = self.means,self.vars\n        x = (x-m) / (v+self.eps).sqrt()\n        return x*self.mults + self.adds\n\nThe update_stats method calculates means and variances over channels/features of the convolutional layer. This is because we want the same feature map to be normalized similarly. This is also where the moving average takes place to save vars and means.\nTechnically, batch and layer norms do not need initialization because the layers are normalized in each batch/epoch. These also normalize layers, so they do not need as much normalization as others. We will learn about different normalization techniques later, but these are techniques that prevent the models from overfitting.\nBatch normalization also allows us to use a higher learning rate.\n\nset_seed(1)\n# model = get_model(gr, norm=BatchNorm).apply(iw)\nmodel = get_model(gr, norm=BatchNorm)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.762\n0.653\n0\nTrue\n00:22\n\n\n0.829\n0.480\n0\nFalse\n00:02\n\n\n0.850\n0.415\n1\nTrue\n00:21\n\n\n0.849\n0.425\n1\nFalse\n00:01\n\n\n0.866\n0.368\n2\nTrue\n00:22\n\n\n0.852\n0.404\n2\nFalse\n00:02\n\n\n0.877\n0.338\n3\nTrue\n00:20\n\n\n0.858\n0.384\n3\nFalse\n00:02\n\n\n0.884\n0.315\n4\nTrue\n00:20\n\n\n0.870\n0.355\n4\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nPros and cons of batch norm:\nPros: - Higher learning rate. - Regularization. - Less epochs for better results.\nCons: - Slower training. - Cannot be used for RNN. - Cannot use very small batch sizes.",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 2"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-19-Initialization_part_2/index.html#conclusion",
    "href": "blog/posts/2023-10-19-Initialization_part_2/index.html#conclusion",
    "title": "Initialization part 2",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog taught us about LSUV, layer norm, and batch norm. They are all similar to each other. LSUV normalizes before training, and others normalize each batch. Layer norm is a simpler version of batch norm. We also dug deeper into the papers. The batch norm is widely used, and reading the paper will benefit the deep learning journey.\nReading papers is difficult, but it is a helpful skill to learn new things. It may be challenging at first and perhaps impossible depending on which paper or what prerequisite readers have, but it will be easier to read.",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 2"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-09-Learner_part_2/index.html",
    "href": "blog/posts/2023-09-09-Learner_part_2/index.html",
    "title": "Learner Pt.2",
    "section": "",
    "text": "In part 1, we looked at a basic learner and a callback learner. The basic learner’s code was hand-written without any flexibility. We would have to modify the learner if we want to change a metric. With a callback learner, we could use callbacks to change a metric or some additional behaviors such as before_fit, after_batch, etc.\nIn part 2, we will look at a more flexible version of learner and some useful callbacks, such as ProgressCB and LRFinderCB. ProgressCB draws a graph with train and validation loss and LRFinderCB allows us to find a learning rate very fast. This blog is based on lesson 16 of the FastAI course.\n\n\nLet’s install and import libraries we need.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n\n\n\n!pip install -q datasets\n!pip install -q torcheval\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 6.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 11.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 11.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 12.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 2.5 MB/s eta 0:00:00\n\n\n\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom miniai.datasets import *\nfrom miniai.conv import *\nimport logging\nfrom fastcore.test import test_close\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\nlogging.disable(logging.WARNING)\n\n\nfrom datasets import Array2D\n\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\nbuilder = load_dataset_builder(name)\ndsd_features = builder.info.features.copy()\ndsd_features['image'] = Array2D(shape=[1, 28*28], dtype='float32')\ndsd = load_dataset(name, features=dsd_features)\ndsd.set_format(type=\"torch\")\n\n@inplace\ndef sq(b): b[x] = [o.squeeze().div(255) for o in b[x]]\n\ntds = dsd.map(sq, batched=True)\nbs = 1024\ndls = DataLoaders.from_dd(tds, bs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgressCB uses fastprogress to generate a graph for losses. It is beneficial to visualize how our model trains throughout each batch. This callback uses _log to overwrite learn.metrics._log. So, instead of printing losses, it creates the graph.\nIt is interesting to understand what’s going on in this code, but it is okay to skip.\n\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n        self.val_losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            if self.val_losses:\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],\n                                        [fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\n    def after_epoch(self, learn):\n        if not learn.training:\n            if self.plot and hasattr(learn, 'metrics'):\n                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],\n                                        [fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\n\n\n\nWe will build a flexible learner with decorators. There are two ways of defining it: with a class and wtih a function. They are both 11 lines of code. However, the function version is a bit weird because it has a wrapper and decorator inside of with_cbs. So, the class version might be easier to read.\nHere is a simple decorator built with a class. It saves a name and prints a greeting with the name each time the function is called. Then we have a do_chores function that tells the person to finish any chore. It is a very friendly way of getting the job done.\n\nclass Greeting_decorator:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, f):\n        print(f'Creating a decorator with f: {f}, name: {self.name}')\n        def _f(*args, **kwargs):\n            print(f'Hello, {self.name}')\n            f(*args, **kwargs)\n        return _f\n\n\n@Greeting_decorator('Bobby')\ndef do_chores(chore): print(f'Please finish {chore}')\n\nCreating a decorator with f: &lt;function do_chores&gt;, name: Bobby\n\n\n\ndo_chores('the dishes')\n\nHello, Bobby\nPlease finish the dishes\n\n\n\nclass with_cbs:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.name}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.name}')\n            except globals()[f'Cancel{self.name.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.name}')\n        return _f\n\nSo, here is our final version of the learner. It is longer than before, but it is not hard to understand. fit can take additional parameters, such as train, valid, cbs, and lr. train and valid tells whether to use train data or not and to use valid data or not. cbs can optionally add callbacks for fitting and removes them after fitting. lr can be added for fitting like callbacks.\nAdditionally, there are callbacks for one_batch, such as after_predict, after_loss, after_backward, and after_step.\n\ndef get_model(): return nn.Sequential(nn.Linear(784, 100), nn.ReLU(), nn.Linear(100, 10))\n\n\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n\n\nclass Learner:\n    def __init__(self, model, dls, loss_func=F.mse_loss, lr=0.1, opt_func=optim.SGD, cbs = None): fc.store_attr()\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=[], lr=None):\n        try:\n            for c in cbs: self.cbs.append(c)\n            if lr is None: lr = self.lr\n            self.epochs = range(n_epochs)\n            self.opt = self.opt_func(self.model.parameters(), lr=lr)\n            self._fit(train, valid)\n        finally:\n            for c in cbs: self.cbs.remove(c)\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.batch in self.dl: self._one_batch()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            with torch.no_grad():\n                self.backward()\n                self.callback('after_backward')\n                self.step()\n                self.callback('after_step')\n                self.zero_grad()\n\n    def callback(self, name): run_cbs(self.cbs, name, self)\n\n    def __getattr__(self, o):\n        if o in ['predict', 'get_loss', 'backward', 'step', 'zero_grad']:\n            return partial(self.callback, o)\n        raise AttributeError(o)\n\n    @property\n    def training(self): return self.model.training\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [metric, ProgressCB(plot=True), TrainCB(), DeviceCB()]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.623\n1.160\n0\ntrain\n\n\n0.702\n0.839\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\nYay, it trains and draws a graph as well! If you feel adventurous, you can rewrite this as much as possible without peeking. After that, you can get a good sense of how it works. Then, look back and check the answer. I was having a weird error because I was writing self.model.training = training instead of self.model.train(training). My model did not change into validation mode and tried to do a backward step! Then, it gave me an error. I was able to fix the error by looking at Jeremy’s code.\n\n\n\nBy inheriting our learner, we can create TrainLearner by defining the training process, such as predict, get_loss, backward, etc. This way, we don’t have to type learn.preds because it belongs to self. And methods don’t take learn either.\nAlso, by inheriting from TrainLearner, we can create MomentumLearner. Instead of setting gradients to zero on zero_grad step, we keep some of the gradients by multiplying them by momentum. This way, gradients move smoothly like water flowing down the drain, and it is faster to train.\n\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n\n\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func=F.mse_loss, lr=0.1, opt_func=optim.SGD, cbs=None, mom=.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, opt_func, cbs)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p *= self.mom\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [metric, ProgressCB(plot=True), TrainCB(), DeviceCB()]\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.663\n0.963\n0\ntrain\n\n\n0.800\n0.553\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\nThe accuracy is better than the pervious one.\nOf course we can do the same thing with callbacks.\n\nclass MomentumCB(TrainCB):\n    def __init__(self, n_inp=1, mom=.85):\n        self.mom = mom\n        super().__init__(n_inp)\n\n    def zero_grad(self, learn):\n        with torch.no_grad():\n            for p in learn.model.parameters():\n                p *= self.mom\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [metric, ProgressCB(plot=True), MomentumCB(), DeviceCB()]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.665\n0.979\n0\ntrain\n\n\n0.800\n0.555\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s time to look at LRFinderCB, which allows us to find the learning rate very quickly. It only uses the training part. In every batch, the callback increases the learning rate by a certain amount. It also keeps the minimum loss. This value gets updated if a smaller loss is found. If the current loss is bigger than three times the minimum loss, training stops. Then it plots the loss and learning rate.\n\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): self.lr_mult = lr_mult\n    def before_fit(self, learn):\n        self.losses, self.lrs = [], []\n        self.min_loss = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        loss = learn.loss.detach().cpu()\n        self.losses.append(loss)\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        if loss &lt; self.min_loss: self.min_loss = loss\n        if loss &gt; 3 * self.min_loss: raise CancelFitException()\n        for p in learn.opt.param_groups:\n            p['lr'] *= self.lr_mult\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\nlr_find = LRFinderCB()\ncbs = [metric, ProgressCB(plot=True), MomentumCB(), DeviceCB(), lr_find]\nlearn = Learner(model, dls, F.cross_entropy, lr=1e-4, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      55.93% [33/59 00:01&lt;00:01 2.564]\n    \n    \n\n\n\nplt.plot(lr_find.lrs, lr_find.losses)\nplt.xscale('log');\n\n\n\n\n\n\n\n\nOkay, we got it. The ideal learning rate is a bit before it hits the absolute minimum loss, so we could use 0.1 or something smaller in this case. Why not choose the point when the loss is at the minimum?\nLet’s say we are playing mini golf. We are on the top of the ramp and the hole is at the bottom. The surface of the ramp has many little obstacles, which could change the course of the trajectory of the golf ball. The golf ball could get stuck on the obstacle if it went too slow and could pass the hole if it went too fast. So there has to be an ideal speed the golf ball has to tavel. By speeding up the speed of the golf ball each second from the top, we could figure out the speed we need. By this analogy, it was too fast when the ball hit the hole, so we want to use the speed before it hit the bottom.\n\n\n\nIn this blog, we covered the final version of the learner which had flexibility and rigidity. Instead of changing the learner to modify how the model trains, we could add or remove callbacks. We could also inherit learner if we wanted to. We also looked at some interesting callbacks, such as LRFinderCB and ProgressCB.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.2"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-09-Learner_part_2/index.html#environment-setup",
    "href": "blog/posts/2023-09-09-Learner_part_2/index.html#environment-setup",
    "title": "Learner Pt.2",
    "section": "",
    "text": "Let’s install and import libraries we need.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n\n\n\n!pip install -q datasets\n!pip install -q torcheval\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 6.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 11.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 11.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 12.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 2.5 MB/s eta 0:00:00\n\n\n\nimport matplotlib as mpl\nimport torchvision.transforms.functional as TF\nfrom contextlib import contextmanager\nfrom torch import nn,tensor\nfrom datasets import load_dataset,load_dataset_builder\nfrom miniai.datasets import *\nfrom miniai.conv import *\nimport logging\nfrom fastcore.test import test_close\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n\nlogging.disable(logging.WARNING)\n\n\nfrom datasets import Array2D\n\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\nbuilder = load_dataset_builder(name)\ndsd_features = builder.info.features.copy()\ndsd_features['image'] = Array2D(shape=[1, 28*28], dtype='float32')\ndsd = load_dataset(name, features=dsd_features)\ndsd.set_format(type=\"torch\")\n\n@inplace\ndef sq(b): b[x] = [o.squeeze().div(255) for o in b[x]]\n\ntds = dsd.map(sq, batched=True)\nbs = 1024\ndls = DataLoaders.from_dd(tds, bs)",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.2"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-09-Learner_part_2/index.html#progress-callback",
    "href": "blog/posts/2023-09-09-Learner_part_2/index.html#progress-callback",
    "title": "Learner Pt.2",
    "section": "",
    "text": "ProgressCB uses fastprogress to generate a graph for losses. It is beneficial to visualize how our model trains throughout each batch. This callback uses _log to overwrite learn.metrics._log. So, instead of printing losses, it creates the graph.\nIt is interesting to understand what’s going on in this code, but it is okay to skip.\n\nclass ProgressCB(Callback):\n    order = MetricsCB.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n        self.losses = []\n        self.val_losses = []\n\n    def _log(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.training:\n            self.losses.append(learn.loss.item())\n            if self.val_losses:\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],\n                                        [fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\n    def after_epoch(self, learn):\n        if not learn.training:\n            if self.plot and hasattr(learn, 'metrics'):\n                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],\n                                        [fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.2"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-09-Learner_part_2/index.html#flexible-learner",
    "href": "blog/posts/2023-09-09-Learner_part_2/index.html#flexible-learner",
    "title": "Learner Pt.2",
    "section": "",
    "text": "We will build a flexible learner with decorators. There are two ways of defining it: with a class and wtih a function. They are both 11 lines of code. However, the function version is a bit weird because it has a wrapper and decorator inside of with_cbs. So, the class version might be easier to read.\nHere is a simple decorator built with a class. It saves a name and prints a greeting with the name each time the function is called. Then we have a do_chores function that tells the person to finish any chore. It is a very friendly way of getting the job done.\n\nclass Greeting_decorator:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, f):\n        print(f'Creating a decorator with f: {f}, name: {self.name}')\n        def _f(*args, **kwargs):\n            print(f'Hello, {self.name}')\n            f(*args, **kwargs)\n        return _f\n\n\n@Greeting_decorator('Bobby')\ndef do_chores(chore): print(f'Please finish {chore}')\n\nCreating a decorator with f: &lt;function do_chores&gt;, name: Bobby\n\n\n\ndo_chores('the dishes')\n\nHello, Bobby\nPlease finish the dishes\n\n\n\nclass with_cbs:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.name}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.name}')\n            except globals()[f'Cancel{self.name.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.name}')\n        return _f\n\nSo, here is our final version of the learner. It is longer than before, but it is not hard to understand. fit can take additional parameters, such as train, valid, cbs, and lr. train and valid tells whether to use train data or not and to use valid data or not. cbs can optionally add callbacks for fitting and removes them after fitting. lr can be added for fitting like callbacks.\nAdditionally, there are callbacks for one_batch, such as after_predict, after_loss, after_backward, and after_step.\n\ndef get_model(): return nn.Sequential(nn.Linear(784, 100), nn.ReLU(), nn.Linear(100, 10))\n\n\nfrom torcheval.metrics import MulticlassAccuracy,Mean\n\n\nclass Learner:\n    def __init__(self, model, dls, loss_func=F.mse_loss, lr=0.1, opt_func=optim.SGD, cbs = None): fc.store_attr()\n\n    def fit(self, n_epochs=1, train=True, valid=True, cbs=[], lr=None):\n        try:\n            for c in cbs: self.cbs.append(c)\n            if lr is None: lr = self.lr\n            self.epochs = range(n_epochs)\n            self.opt = self.opt_func(self.model.parameters(), lr=lr)\n            self._fit(train, valid)\n        finally:\n            for c in cbs: self.cbs.remove(c)\n\n    @with_cbs('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: self.one_epoch(True)\n            if valid: torch.no_grad()(self.one_epoch)(False)\n\n    def one_epoch(self, training):\n        self.model.train(training)\n        self.dl = self.dls.train if training else self.dls.valid\n        self._one_epoch()\n\n    @with_cbs('epoch')\n    def _one_epoch(self):\n        for self.batch in self.dl: self._one_batch()\n\n    @with_cbs('batch')\n    def _one_batch(self):\n        self.predict()\n        self.callback('after_predict')\n        self.get_loss()\n        self.callback('after_loss')\n        if self.training:\n            with torch.no_grad():\n                self.backward()\n                self.callback('after_backward')\n                self.step()\n                self.callback('after_step')\n                self.zero_grad()\n\n    def callback(self, name): run_cbs(self.cbs, name, self)\n\n    def __getattr__(self, o):\n        if o in ['predict', 'get_loss', 'backward', 'step', 'zero_grad']:\n            return partial(self.callback, o)\n        raise AttributeError(o)\n\n    @property\n    def training(self): return self.model.training\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [metric, ProgressCB(plot=True), TrainCB(), DeviceCB()]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.623\n1.160\n0\ntrain\n\n\n0.702\n0.839\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\nYay, it trains and draws a graph as well! If you feel adventurous, you can rewrite this as much as possible without peeking. After that, you can get a good sense of how it works. Then, look back and check the answer. I was having a weird error because I was writing self.model.training = training instead of self.model.train(training). My model did not change into validation mode and tried to do a backward step! Then, it gave me an error. I was able to fix the error by looking at Jeremy’s code.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.2"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-09-Learner_part_2/index.html#trainlearner-and-momentumlearner",
    "href": "blog/posts/2023-09-09-Learner_part_2/index.html#trainlearner-and-momentumlearner",
    "title": "Learner Pt.2",
    "section": "",
    "text": "By inheriting our learner, we can create TrainLearner by defining the training process, such as predict, get_loss, backward, etc. This way, we don’t have to type learn.preds because it belongs to self. And methods don’t take learn either.\nAlso, by inheriting from TrainLearner, we can create MomentumLearner. Instead of setting gradients to zero on zero_grad step, we keep some of the gradients by multiplying them by momentum. This way, gradients move smoothly like water flowing down the drain, and it is faster to train.\n\nclass TrainLearner(Learner):\n    def predict(self): self.preds = self.model(self.batch[0])\n    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n\n\nclass MomentumLearner(TrainLearner):\n    def __init__(self, model, dls, loss_func=F.mse_loss, lr=0.1, opt_func=optim.SGD, cbs=None, mom=.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_func, lr, opt_func, cbs)\n\n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p *= self.mom\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [metric, ProgressCB(plot=True), TrainCB(), DeviceCB()]\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.663\n0.963\n0\ntrain\n\n\n0.800\n0.553\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\nThe accuracy is better than the pervious one.\nOf course we can do the same thing with callbacks.\n\nclass MomentumCB(TrainCB):\n    def __init__(self, n_inp=1, mom=.85):\n        self.mom = mom\n        super().__init__(n_inp)\n\n    def zero_grad(self, learn):\n        with torch.no_grad():\n            for p in learn.model.parameters():\n                p *= self.mom\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [metric, ProgressCB(plot=True), MomentumCB(), DeviceCB()]\nlearn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.665\n0.979\n0\ntrain\n\n\n0.800\n0.555\n0\neval",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.2"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-09-Learner_part_2/index.html#lrfindercb",
    "href": "blog/posts/2023-09-09-Learner_part_2/index.html#lrfindercb",
    "title": "Learner Pt.2",
    "section": "",
    "text": "It’s time to look at LRFinderCB, which allows us to find the learning rate very quickly. It only uses the training part. In every batch, the callback increases the learning rate by a certain amount. It also keeps the minimum loss. This value gets updated if a smaller loss is found. If the current loss is bigger than three times the minimum loss, training stops. Then it plots the loss and learning rate.\n\nclass LRFinderCB(Callback):\n    def __init__(self, lr_mult=1.3): self.lr_mult = lr_mult\n    def before_fit(self, learn):\n        self.losses, self.lrs = [], []\n        self.min_loss = math.inf\n\n    def after_batch(self, learn):\n        if not learn.training: raise CancelEpochException()\n        loss = learn.loss.detach().cpu()\n        self.losses.append(loss)\n        self.lrs.append(learn.opt.param_groups[0]['lr'])\n        if loss &lt; self.min_loss: self.min_loss = loss\n        if loss &gt; 3 * self.min_loss: raise CancelFitException()\n        for p in learn.opt.param_groups:\n            p['lr'] *= self.lr_mult\n\n\nmodel = get_model()\n\nmetric = MetricsCB(accuracy=MulticlassAccuracy())\nlr_find = LRFinderCB()\ncbs = [metric, ProgressCB(plot=True), MomentumCB(), DeviceCB(), lr_find]\nlearn = Learner(model, dls, F.cross_entropy, lr=1e-4, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      55.93% [33/59 00:01&lt;00:01 2.564]\n    \n    \n\n\n\nplt.plot(lr_find.lrs, lr_find.losses)\nplt.xscale('log');\n\n\n\n\n\n\n\n\nOkay, we got it. The ideal learning rate is a bit before it hits the absolute minimum loss, so we could use 0.1 or something smaller in this case. Why not choose the point when the loss is at the minimum?\nLet’s say we are playing mini golf. We are on the top of the ramp and the hole is at the bottom. The surface of the ramp has many little obstacles, which could change the course of the trajectory of the golf ball. The golf ball could get stuck on the obstacle if it went too slow and could pass the hole if it went too fast. So there has to be an ideal speed the golf ball has to tavel. By speeding up the speed of the golf ball each second from the top, we could figure out the speed we need. By this analogy, it was too fast when the ball hit the hole, so we want to use the speed before it hit the bottom.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.2"
    ]
  },
  {
    "objectID": "blog/posts/2023-09-09-Learner_part_2/index.html#conclusion",
    "href": "blog/posts/2023-09-09-Learner_part_2/index.html#conclusion",
    "title": "Learner Pt.2",
    "section": "",
    "text": "In this blog, we covered the final version of the learner which had flexibility and rigidity. Instead of changing the learner to modify how the model trains, we could add or remove callbacks. We could also inherit learner if we wanted to. We also looked at some interesting callbacks, such as LRFinderCB and ProgressCB.",
    "crumbs": [
      "Home",
      "Blog",
      "Learner Pt.2"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-10-Initialization_part_1/index.html",
    "href": "blog/posts/2023-10-10-Initialization_part_1/index.html",
    "title": "Initialization part 1",
    "section": "",
    "text": "In this blog, let’s explore some initializing methods on weights to train better. Our model can train well with a good initialization because close to all the activations become zeroes. Thanks to the Pytorch hooks we learned from the last blog, we can look into activation statistics as we try different techniques. This blog is based on lesson 17 of the FastAI course.\nFirst, we import libraries and do required setups.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n!pip -q install torcheval\n!pip -q install datasets\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport sys,gc,traceback\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.xtras import *\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 7.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 10.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 16.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 11.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 23.2 MB/s eta 0:00:00\n\n\nWe’ve been using Hugging Face Datasets, but let’s try Pytorch’s datasets this time. This way, we do not have to convert the Pyarrow format into tensors every time we use the data. So, it is faster for slow CPUs like Kaggle or Google Colab.\n\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\ntrn_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\nval_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\n\nfrom torch.utils.data import DataLoader\nset_seed(42)\nbs = 1024\n\ntrn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=bs, shuffle=True)\ndls = DataLoaders(trn_dl, val_dl)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))\n\n\n\ndef get_model():\n    return nn.Sequential(conv(1 ,8), conv(8 ,16), conv(16,32), conv(32,64),\n                         conv(64,10, act=False), nn.Flatten()).to(def_device)\n\nBefore we apply any initialization method, let’s try to train and see what happens.\n\nset_seed(1)\nmodel = get_model()\nMomentumLearner(model, dls, F.cross_entropy, cbs=[DeviceCB()]).lr_find(gamma=1.1, start_lr=1e-2)\n\n\n\n\n\n\n\n\nAfter training, we want to look at the statistics of the weights all the time, so we make a callback for it. This way, we do not have to call stat.plot_stats after training.\n\nclass ShowStatsCB(Callback):\n    def after_fit(self, learn):\n        astats.plot_stats(figsize=(8, 3))\n\n\nset_seed(1)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy(device='cpu'))\nastats = ActivationStats(fc.risinstance(nn.ReLU))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), astats, ShowStatsCB()]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.01, cbs=cbs)\n\n\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.100\n2.303\n0\nTrue\n00:16\n\n\n0.100\n2.302\n0\nFalse\n00:01\n\n\n0.124\n2.301\n1\nTrue\n00:16\n\n\n0.154\n2.300\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\n\n\n\n\nThis isn’t good. Reducing the learning rate does not do anything because it is not training at all. Looking at activation statistics, we can see that they are very close to zeros. We can also check the dead chart; almost all the activations are zeroes. So what’s going on here?",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 1"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-10-Initialization_part_1/index.html#initializing-weights-part-1",
    "href": "blog/posts/2023-10-10-Initialization_part_1/index.html#initializing-weights-part-1",
    "title": "Initialization part 1",
    "section": "",
    "text": "In this blog, let’s explore some initializing methods on weights to train better. Our model can train well with a good initialization because close to all the activations become zeroes. Thanks to the Pytorch hooks we learned from the last blog, we can look into activation statistics as we try different techniques. This blog is based on lesson 17 of the FastAI course.\nFirst, we import libraries and do required setups.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n!pip -q install torcheval\n!pip -q install datasets\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nimport sys,gc,traceback\nimport fastcore.all as fc\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.xtras import *\n\nfrom fastcore.test import test_close\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\n\nimport logging\nlogging.disable(logging.WARNING)\n\nMounted at /content/drive\n/content/drive/MyDrive/Colab Notebooks\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.2/179.2 kB 3.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 7.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 10.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 16.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 11.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 23.2 MB/s eta 0:00:00\n\n\nWe’ve been using Hugging Face Datasets, but let’s try Pytorch’s datasets this time. This way, we do not have to convert the Pyarrow format into tensors every time we use the data. So, it is faster for slow CPUs like Kaggle or Google Colab.\n\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\ntrn_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\nval_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\n\nfrom torch.utils.data import DataLoader\nset_seed(42)\nbs = 1024\n\ntrn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=bs, shuffle=True)\ndls = DataLoaders(trn_dl, val_dl)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))\n\n\n\ndef get_model():\n    return nn.Sequential(conv(1 ,8), conv(8 ,16), conv(16,32), conv(32,64),\n                         conv(64,10, act=False), nn.Flatten()).to(def_device)\n\nBefore we apply any initialization method, let’s try to train and see what happens.\n\nset_seed(1)\nmodel = get_model()\nMomentumLearner(model, dls, F.cross_entropy, cbs=[DeviceCB()]).lr_find(gamma=1.1, start_lr=1e-2)\n\n\n\n\n\n\n\n\nAfter training, we want to look at the statistics of the weights all the time, so we make a callback for it. This way, we do not have to call stat.plot_stats after training.\n\nclass ShowStatsCB(Callback):\n    def after_fit(self, learn):\n        astats.plot_stats(figsize=(8, 3))\n\n\nset_seed(1)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy(device='cpu'))\nastats = ActivationStats(fc.risinstance(nn.ReLU))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), astats, ShowStatsCB()]\nlearn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.01, cbs=cbs)\n\n\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.100\n2.303\n0\nTrue\n00:16\n\n\n0.100\n2.302\n0\nFalse\n00:01\n\n\n0.124\n2.301\n1\nTrue\n00:16\n\n\n0.154\n2.300\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\n\n\n\n\nThis isn’t good. Reducing the learning rate does not do anything because it is not training at all. Looking at activation statistics, we can see that they are very close to zeros. We can also check the dead chart; almost all the activations are zeroes. So what’s going on here?",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 1"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-10-Initialization_part_1/index.html#matrix-multiplications",
    "href": "blog/posts/2023-10-10-Initialization_part_1/index.html#matrix-multiplications",
    "title": "Initialization part 1",
    "section": "Matrix multiplications",
    "text": "Matrix multiplications\nSo, let’s see what happens if we repeatedly perform matrix multiplication on a given matrix. If we execute this fifty times, all the values become nans, which means they are too large for the computers. Therefore, if we have a model with fifty layers, we would get a model full of nans. Do you know how we can fix this issue? We can perform matrix multiplications with smaller values to prevent values from exploding.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\n\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])\n\n\nSo, if we try to multiply by .01 to a new matrix, we get zeroes.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100) * .01\nx[0:5,0:5]\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nHowever, if we multiply by .1, we get numbers. So, .1 is a magic number for us.\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100) * .1\nx[0:5,0:5]\n\ntensor([[-1.22, -0.63,  1.26, -0.47,  0.27],\n        [-0.92, -0.66,  0.18,  0.05, -0.04],\n        [-0.40, -0.52,  0.63, -0.09, -0.17],\n        [-0.17, -0.20, -0.31, -0.41,  0.08],\n        [ 0.47,  0.53, -0.67, -0.10,  0.06]])",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 1"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-10-Initialization_part_1/index.html#glorotxavier-init",
    "href": "blog/posts/2023-10-10-Initialization_part_1/index.html#glorotxavier-init",
    "title": "Initialization part 1",
    "section": "Glorot/Xavier init",
    "text": "Glorot/Xavier init\nAccording to Understanding the difficulty of training deep feedforward neural networks written by Xavier Glorot and Yoshua Bengio, we can initialize weights by multiplying them by the square root of the number of inputs to stabilize calculations. The paper used uniform distribution, but we can use normal distribution, which still works. This trick also appears in Efficient Backprop by Yann Lecun et al. section 4.6 initializing the weights. In both papers, the goal is to make the mean of zero and the standard deviation of 1.\n\nx.mean(), x.std()\n\n(tensor(0.00), tensor(0.61))\n\n\nFirst, we initialize weights, w, with normal distribution with the mean of 0 and the standard deviation of 1. Then, we multiply w by \\(1/\\sqrt{n_{in}}\\) where \\(n_{in}\\) is a number of inputs. \\[w = w * \\frac{1}{\\sqrt{n_{in}}}\\]\nIn other words, w is normally distributed with the mean of 0 and the standard deviation of \\(1/\\sqrt{n_{in}}\\) or the variance of \\(1/n_{in}\\).\n\\[w \\sim \\mathcal{N}(0, \\frac{1}{n_{in}})\\]\nHow did they come up with this number? We can try to find mean and standard deviations ourselves from matrix multiplications. When we change the size, sz, of the input matrix, x, the standard deviation is pretty close to the sz. We can try different sizes.\n\ndef get_stats(epochs = 100, sz = 100, init = 1., act = None, seed = None):\n    if seed is not None: set_seed(seed)\n    mean, sqr = 0., 0.\n    for _ in range(epochs):\n        x = torch.randn(64, sz) * init\n        a = torch.randn(sz)\n        y = x @ a\n        if act is not None: y = act(y)\n        mean += y.mean().item()\n        sqr  += y.pow(2).mean().item()\n    return mean / epochs, sqr / epochs\n\n\nget_stats()\n\n(-0.04464139200747013, 99.57821556091308)\n\n\n\nget_stats(sz = 200)\n\n(-0.005759633108973503, 197.81238624572754)\n\n\n\nget_stats(sz = 1000)\n\n(0.7149963945150375, 1005.3396026611329)\n\n\nThat is very cool. We can keep the mean of 0 and the standard deviation of 1 and train deep neural networks! However, we cannot use this initialization because we will use the relu activation function, which messes up the statistics! What do you think we should do?",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 1"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-10-Initialization_part_1/index.html#kaiminghe-init",
    "href": "blog/posts/2023-10-10-Initialization_part_1/index.html#kaiminghe-init",
    "title": "Initialization part 1",
    "section": "Kaiming/He init",
    "text": "Kaiming/He init\nWe can use Kaiming/He initialization with relu! This method came from Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification by Kaiming He et al. We initialize this way:\n\\[w \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})\\]\nWhere w is the weights are distributed as a normal distribution with the mean of 0 and the standard deviation of \\(\\sqrt{2/n_{in}}\\) or the variance of \\(2/n_{in}\\) where \\(n_{in}\\) is the number of inputs.\n\nget_stats(act=nn.ReLU())\n\n(4.105671746730804, 52.16908494949341)\n\n\n\nget_stats(sz = 200, act=nn.ReLU())\n\n(5.568780157566071, 97.80967510223388)\n\n\n\nget_stats(sz = 1000, act=nn.ReLU())\n\n(12.306093001365662, 485.7778285217285)\n\n\nCompared to Glorot init, standard deviations are halved. We can also see that the mean is not zero anymore. What’s going on? We are left with only positive numbers because relu clips all the negative numbers. Therefore, we cannot have the mean of zero anymore. The only way to get a zero mean is to have all the zeroes as activations, but the standard deviation will also be zero.\nNow that we can use an initialization method, let’s try to improve our previous model with Kaiming init.\n\nmodel = get_model()\nmodel.apply(lambda m: print(type(m).__name__));\n\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nReLU\nSequential\nConv2d\nFlatten\nSequential\n\n\n\ndef _kaiming_init_(w):\n    # For convolutions, inputs start with index 1\n    st = (2 / np.prod(w.shape[1:])) ** .5\n    with torch.no_grad():\n        return w.normal_(0, st)\n\n\ndef relu_(x): x.clamp_(0)\n\n\ndef kaiming_init_(m):\n    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)): _kaiming_init_(m.weight)\n\n\nset_seed(1)\nmodel = get_model()\nmodel[0][0].weight.mean(), model[0][0].weight.std()\n\n(tensor(0.01, grad_fn=&lt;MeanBackward0&gt;), tensor(0.18, grad_fn=&lt;StdBackward0&gt;))\n\n\nWe can apply Kaiming init with apply method. We can see that it worked by looking at the mean and the standard deviation of the activations before and after initialization.\n\nmodel.apply(kaiming_init_);\n\n\nmodel[0][0].weight.mean(), model[0][0].weight.std()\n\n(tensor(0.02, grad_fn=&lt;MeanBackward0&gt;), tensor(0.45, grad_fn=&lt;StdBackward0&gt;))\n\n\n\nset_seed(1)\nmodel = get_model().apply(kaiming_init_)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.504\n1.476\n0\nTrue\n00:16\n\n\n0.764\n0.646\n0\nFalse\n00:02\n\n\n0.802\n0.538\n1\nTrue\n00:15\n\n\n0.808\n0.530\n1\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s still not training very well. Although weights are normalized, images are not. So, we can normalize the data with callbacks.\n\nclass BatchTfmCB(Callback):\n    def before_batch(self, learn):\n        xb, yb = learn.batch\n        xb = (xb - xb.mean()) / xb.std()\n        learn.batch = xb, yb\n\n\nset_seed(1)\nmodel = get_model().apply(kaiming_init_)\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs + [BatchTfmCB()])\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.634\n1.089\n0\nTrue\n00:17\n\n\n0.823\n0.487\n0\nFalse\n00:01\n\n\n0.848\n0.416\n1\nTrue\n00:16\n\n\n0.846\n0.416\n1\nFalse\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s an improvement. We almost achieved 85% accuracy with two epochs.\nWe can also transform the inputs as transform before creating dataloaders. Using this method, we do not have to normalize data every batch.\n\ndef batch_tfm(img): return (ToTensor()(img) - xmean) / xstd\n\n\nxmean, xstd = xb.mean(), xb.std()\nxmean, xstd\n\n(tensor(0.29), tensor(0.35))\n\n\n\ntrn_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=True,\n    download=True,\n    transform=batch_tfm\n)\n\nval_ds = datasets.FashionMNIST(\n    root=\".\",\n    train=False,\n    download=True,\n    transform=batch_tfm\n)\n\n\ntrn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=bs, shuffle=True)\ndls = DataLoaders(trn_dl, val_dl)\ndt = dls.train\nit = iter(dt)\nnext(it)\nxb,yb = next(it)\nxb.shape,yb[:10]\n\n(torch.Size([1024, 1, 28, 28]), tensor([3, 3, 3, 4, 6, 7, 6, 8, 6, 1]))\n\n\n\nxb.mean(), xb.std()\n\n(tensor(-0.02), tensor(0.99))\n\n\n\nset_seed(1)\nmodel = get_model().apply(kaiming_init_)\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), astats]\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(2)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.626\n1.112\n0\nTrue\n00:18\n\n\n0.814\n0.525\n0\nFalse\n00:01\n\n\n0.844\n0.425\n1\nTrue\n00:17\n\n\n0.850\n0.417\n1\nFalse\n00:02",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 1"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-10-Initialization_part_1/index.html#general-relu",
    "href": "blog/posts/2023-10-10-Initialization_part_1/index.html#general-relu",
    "title": "Initialization part 1",
    "section": "General ReLU",
    "text": "General ReLU\nWe now create flexible GeneralRelu because the mean was not zero with Kaiming init. This version of relu allows us to subtract a number and do a leaky relu. Instead of clamping negative numbers, leaky relu uses a negative slope.\n\nclass GeneralRelu(nn.Module):\n    def __init__(self, leaky=None, sub=None, max_val=None):\n        super().__init__()\n        self.relu = nn.ReLU() if leaky is None else nn.LeakyReLU(leaky)\n        self.sub = 0. if sub is None else sub\n        self.max_val = max_val\n\n    def forward(self, x):\n        x = self.relu(x)\n        x -= self.sub\n        if self.max_val is not None:\n            x.clamp_max_(self.max_val)\n        return x\n\nLet’s look at what it looks like by plotting this function.\n\ndef plot_func(f, min=-4., max=4.):\n    x = torch.linspace(min, max, 100)\n    y = f(x)\n    plt.plot(x, y)\n    plt.grid(True, which='both', ls='-')\n    plt.axhline(y=0, color='k', linewidth=0.7)\n    plt.axvline(x=0, color='k', linewidth=0.7)\n\n\ngr = GeneralRelu(.1, .4)\nplot_func(gr)\n\n\n\n\n\n\n\n\nWe can also look at this version’s mean and standard deviation.\n\nget_stats(act=GeneralRelu(.1, .4))\n\n(3.269775085449219, 49.65711633682251)\n\n\nBefore we use GeneralRelu, we have to modify our conv and get_model functions to use different activation functions.\n\ndef conv(ni, nf, ks=3, stride=2, act=None):\n    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n    if act is not None: res = nn.Sequential(res, act())\n    return res\n\n\ndef get_model(act=nn.ReLU, filters=None):\n    if filters is None: filters = [1, 8, 16, 32, 64]\n    layers = [conv(filters[i], filters[i+1], act=act) for i in range(len(filters)-1)]\n    return nn.Sequential(*layers, conv(filters[-1], 10, act=None), nn.Flatten()).to(def_device)\n\nNow, we are ready to see how it performs. Let’s also try some other values, too.\n\nget_stats(act=GeneralRelu(.1, .4), seed=1)\n\n(3.2273051607608796, 50.619369144439695)\n\n\n\nset_seed(1)\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), astats, ShowStatsCB()]\nmodel = get_model(act=partial(GeneralRelu, .1, .4))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.518\n1.320\n0\nTrue\n00:18\n\n\n0.796\n0.541\n0\nFalse\n00:01\n\n\n0.834\n0.449\n1\nTrue\n00:17\n\n\n0.845\n0.421\n1\nFalse\n00:02\n\n\n0.867\n0.366\n2\nTrue\n00:18\n\n\n0.858\n0.397\n2\nFalse\n00:01\n\n\n0.875\n0.339\n3\nTrue\n00:17\n\n\n0.869\n0.358\n3\nFalse\n00:01\n\n\n0.886\n0.312\n4\nTrue\n00:19\n\n\n0.876\n0.353\n4\nFalse\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_stats(act=GeneralRelu(.05, .5), seed=1)\n\n(3.3271772360801695, 49.40698293685913)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .05, .5))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.168\nnan\n0\nTrue\n00:17\n\n\n0.100\nnan\n0\nFalse\n00:01\n\n\n0.100\nnan\n1\nTrue\n00:16\n\n\n0.100\nnan\n1\nFalse\n00:02\n\n\n0.100\nnan\n2\nTrue\n00:18\n\n\n0.100\nnan\n2\nFalse\n00:01\n\n\n0.100\nnan\n3\nTrue\n00:16\n\n\n0.100\nnan\n3\nFalse\n00:01\n\n\n0.100\nnan\n4\nTrue\n00:16\n\n\n0.100\nnan\n4\nFalse\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_stats(act=GeneralRelu(.05, .7), seed=1)\n\n(3.127177233695984, 48.11611066818237)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .05, .7))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.506\n1.379\n0\nTrue\n00:17\n\n\n0.793\n0.560\n0\nFalse\n00:01\n\n\n0.836\n0.446\n1\nTrue\n00:17\n\n\n0.850\n0.412\n1\nFalse\n00:01\n\n\n0.866\n0.367\n2\nTrue\n00:18\n\n\n0.854\n0.408\n2\nFalse\n00:01\n\n\n0.876\n0.338\n3\nTrue\n00:19\n\n\n0.868\n0.370\n3\nFalse\n00:01\n\n\n0.889\n0.304\n4\nTrue\n00:18\n\n\n0.865\n0.369\n4\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we use GeneralRelu(.05, .9), the mean and the std are similar to others, but the activations collapse. So, do the mean and the standard deviations not matter at all? Let’s keep trying other values.\n\nget_stats(act=GeneralRelu(.05, .9), seed=1)\n\n(2.927177308797836, 46.90524224281311)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .05, .9))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.100\nnan\n0\nTrue\n00:17\n\n\n0.100\nnan\n0\nFalse\n00:01\n\n\n0.100\nnan\n1\nTrue\n00:16\n\n\n0.100\nnan\n1\nFalse\n00:02\n\n\n0.100\nnan\n2\nTrue\n00:17\n\n\n0.100\nnan\n2\nFalse\n00:01\n\n\n0.100\nnan\n3\nTrue\n00:16\n\n\n0.100\nnan\n3\nFalse\n00:01\n\n\n0.100\nnan\n4\nTrue\n00:16\n\n\n0.100\nnan\n4\nFalse\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_stats(act=GeneralRelu(.2, .5), seed=1)\n\n(2.727560875415802, 51.89185791015625)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .2, .5))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.516\n1.351\n0\nTrue\n00:18\n\n\n0.781\n0.585\n0\nFalse\n00:01\n\n\n0.833\n0.453\n1\nTrue\n00:17\n\n\n0.848\n0.411\n1\nFalse\n00:02\n\n\n0.867\n0.366\n2\nTrue\n00:20\n\n\n0.866\n0.376\n2\nFalse\n00:01\n\n\n0.874\n0.343\n3\nTrue\n00:17\n\n\n0.859\n0.387\n3\nFalse\n00:02\n\n\n0.885\n0.314\n4\nTrue\n00:18\n\n\n0.867\n0.359\n4\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_stats(act=GeneralRelu(.2, .46), seed=1)\n\n(2.767560924291611, 52.11166482925415)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .2, .46))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.553\n1.237\n0\nTrue\n00:17\n\n\n0.786\n0.547\n0\nFalse\n00:01\n\n\n0.837\n0.446\n1\nTrue\n00:19\n\n\n0.847\n0.419\n1\nFalse\n00:01\n\n\n0.867\n0.367\n2\nTrue\n00:18\n\n\n0.843\n0.424\n2\nFalse\n00:01\n\n\n0.876\n0.344\n3\nTrue\n00:18\n\n\n0.870\n0.365\n3\nFalse\n00:01\n\n\n0.885\n0.315\n4\nTrue\n00:17\n\n\n0.864\n0.370\n4\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_stats(act=GeneralRelu(.2, .4), seed=1)\n\n(2.827560887336731, 52.44737192153931)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .2, .4))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.259\nnan\n0\nTrue\n00:19\n\n\n0.100\nnan\n0\nFalse\n00:03\n\n\n0.100\nnan\n1\nTrue\n00:17\n\n\n0.100\nnan\n1\nFalse\n00:01\n\n\n0.100\nnan\n2\nTrue\n00:16\n\n\n0.100\nnan\n2\nFalse\n00:01\n\n\n0.100\nnan\n3\nTrue\n00:17\n\n\n0.100\nnan\n3\nFalse\n00:02\n\n\n0.100\nnan\n4\nTrue\n00:16\n\n\n0.100\nnan\n4\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_stats(act=GeneralRelu(.3, .35), seed=1)\n\n(2.4778165701031685, 55.52612455368042)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .3, .35))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.285\nnan\n0\nTrue\n00:20\n\n\n0.100\nnan\n0\nFalse\n00:02\n\n\n0.100\nnan\n1\nTrue\n00:17\n\n\n0.100\nnan\n1\nFalse\n00:01\n\n\n0.100\nnan\n2\nTrue\n00:17\n\n\n0.100\nnan\n2\nFalse\n00:01\n\n\n0.100\nnan\n3\nTrue\n00:17\n\n\n0.100\nnan\n3\nFalse\n00:02\n\n\n0.100\nnan\n4\nTrue\n00:18\n\n\n0.100\nnan\n4\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_stats(act=GeneralRelu(.3, .5), seed=1)\n\n(2.3278166237473488, 54.80528106689453)\n\n\n\nset_seed(1)\nmodel = get_model(act=partial(GeneralRelu, .3, .5))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.588\n1.117\n0\nTrue\n00:17\n\n\n0.787\n0.543\n0\nFalse\n00:02\n\n\n0.838\n0.443\n1\nTrue\n00:19\n\n\n0.854\n0.410\n1\nFalse\n00:01\n\n\n0.864\n0.377\n2\nTrue\n00:18\n\n\n0.860\n0.400\n2\nFalse\n00:01\n\n\n0.871\n0.356\n3\nTrue\n00:19\n\n\n0.861\n0.384\n3\nFalse\n00:01\n\n\n0.879\n0.332\n4\nTrue\n00:17\n\n\n0.858\n0.388\n4\nFalse\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nReducing the mean to closer to zero did not always work out. Also, using different seed values sometimes made the model untrainable. For instance, using .2 and .4 with seed one does not work, but it works well without seed or using other seed values. Also, depending on the seed, the accuracy fluctuates a lot.\nWe can hypothesize that if we use an immense value for the leaky relu, the function becomes too linear, and the model cannot calculate helpful predictions. If we pay close attention to the statistics, we can see that all the mean and the standard deviation shifted early in the training to find a good spot.\nSome good values to use are (.1, .4), (.5, .7), and (.2, .4), but there can be other values.\n\nget_stats(act=GeneralRelu(.2, .4), seed=42)\n\n(2.706548626422882, 47.42090078353882)\n\n\n\nset_seed(42)\nmodel = get_model(act=partial(GeneralRelu, .2, .4))\nlearn = MomentumLearner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\nlearn.fit(5)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.592\n1.111\n0\nTrue\n00:18\n\n\n0.805\n0.539\n0\nFalse\n00:02\n\n\n0.833\n0.452\n1\nTrue\n00:17\n\n\n0.840\n0.442\n1\nFalse\n00:01\n\n\n0.861\n0.381\n2\nTrue\n00:20\n\n\n0.857\n0.393\n2\nFalse\n00:02\n\n\n0.870\n0.355\n3\nTrue\n00:17\n\n\n0.869\n0.363\n3\nFalse\n00:01\n\n\n0.884\n0.321\n4\nTrue\n00:17\n\n\n0.866\n0.364\n4\nFalse\n00:02",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 1"
    ]
  },
  {
    "objectID": "blog/posts/2023-10-10-Initialization_part_1/index.html#conclusion",
    "href": "blog/posts/2023-10-10-Initialization_part_1/index.html#conclusion",
    "title": "Initialization part 1",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog, we learned about different initialization techniques, such as Glorot/Xavier init, Kaiming/He init, and trying GeneralRelu with different arguments. We also learned how important it is to have the mean of zero and the standard deviation of one for our activations. Now that we have learned Kaiming init, we can train deeper networks without vanishing or crashing activations.\nNext time, we will learn about Layer-wise Sequential Unit-Variance (LSUV), layer normalization, and batch normalization.",
    "crumbs": [
      "Home",
      "Blog",
      "Initialization part 1"
    ]
  }
]