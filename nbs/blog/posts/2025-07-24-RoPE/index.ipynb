{
 "cells": [
  {
   "cell_type": "raw",
   "id": "992c081a",
   "metadata": {
    "input_tokens": 88
   },
   "source": [
    "---\n",
    "title: \"RoPE\"\n",
    "author: \"galopy\"\n",
    "date: \"August 14, 2025\"\n",
    "toc: true\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: galopyz/delicious-nbdev\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011c5a8",
   "metadata": {
    "input_tokens": 4
   },
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb66c1",
   "metadata": {
    "input_tokens": 126
   },
   "source": [
    "In order to let large language models (LLMs) know the order of the tokens, we use positional embeddings. There are absolute positional embedding and relative positional embeding. Today, we will talk about relative positional embedding with Rotary Position Embedding (RoPE) from [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Su et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9f606",
   "metadata": {
    "input_tokens": 3
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736e699",
   "metadata": {
    "input_tokens": 145
   },
   "source": [
    "There are two ways to implement RoPE. One from [Llama](https://github.com/meta-llama/llama/blob/main/llama/model.py#L80) and another from [Hugging Face](https://github.com/huggingface/transformers/blob/e42587f596181396e1c4b63660abf0c736b10dae/src/transformers/models/llama/modeling_llama.py#L92)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166215b5",
   "metadata": {
    "input_tokens": 22
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea70e91",
   "metadata": {
    "input_tokens": 34
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "context_len = 3\n",
    "num_heads = 4\n",
    "head_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e5bb8",
   "metadata": {
    "input_tokens": 64,
    "output_tokens": 60
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "queries = torch.randn(batch_size, context_len, num_heads, head_dim)\n",
    "keys = torch.randn(batch_size, context_len, num_heads, head_dim)\n",
    "queries.shape, keys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40342561",
   "metadata": {
    "input_tokens": 4
   },
   "source": [
    "### Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ed01d",
   "metadata": {
    "input_tokens": 60
   },
   "source": [
    "Below is the code from [Llama 2 implementation](https://github.com/meta-llama/llama/blob/main/llama/model.py#L80). Let's try to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533945f",
   "metadata": {
    "input_tokens": 390
   },
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "\n",
    "    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "    and the end index 'end'. The 'theta' parameter scales the frequencies.\n",
    "    The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the frequency tensor.\n",
    "        end (int): End index for precomputing frequencies.\n",
    "        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c6bd6",
   "metadata": {
    "input_tokens": 25
   },
   "source": [
    "Let's go through code step by step. First, we need the arguments to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e8851",
   "metadata": {
    "input_tokens": 24
   },
   "outputs": [],
   "source": [
    "dim = head_dim\n",
    "end = context_len\n",
    "theta = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05033fb0",
   "metadata": {
    "input_tokens": 63
   },
   "source": [
    "We want to create a frequency tensor. We are creating a `dim // 2` tensor because we get cos and sin from each frequency. And these are applied to each column as we will see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200d050",
   "metadata": {
    "input_tokens": 28,
    "output_tokens": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, dim, 2)[: (dim // 2)].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501deb70",
   "metadata": {
    "input_tokens": 10
   },
   "source": [
    "Then we normalize by `dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cda4be",
   "metadata": {
    "input_tokens": 33,
    "output_tokens": 91
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1250, 0.2500, 0.3750, 0.5000, 0.6250, 0.7500, 0.8750])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62872f",
   "metadata": {
    "input_tokens": 37,
    "output_tokens": 129
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.1623e+00, 1.0000e+01, 3.1623e+01, 1.0000e+02, 3.1623e+02,\n",
       "        1.0000e+03, 3.1623e+03])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc7029",
   "metadata": {
    "input_tokens": 45,
    "output_tokens": 129
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n",
       "        1.0000e-03, 3.1623e-04])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a83d4",
   "metadata": {
    "input_tokens": 54,
    "output_tokens": 129
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n",
       "        1.0000e-03, 3.1623e-04])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a7918",
   "metadata": {
    "input_tokens": 195
   },
   "source": [
    "- **High frequencies** (like 1.0) rotate quickly → capture short-range positional relationships\n",
    "- **Low frequencies** (like 3.1623e-04 = 0.00031623) rotate slowly → capture long-range positional relationships\n",
    "\n",
    "Think of it like a clock:\n",
    "- A second hand (high frequency) completes many rotations quickly - good for distinguishing nearby moments\n",
    "- An hour hand (low frequency) rotates slowly - good for distinguishing longer time periods\n",
    "\n",
    "In RoPE, the high frequency dimensions help the model distinguish between adjacent tokens, while the low frequency dimensions help distinguish between tokens that are far apart in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63982c",
   "metadata": {
    "input_tokens": 19,
    "output_tokens": 33
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(end, device=freqs.device)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c175e",
   "metadata": {
    "input_tokens": 27,
    "output_tokens": 364
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 8]),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n",
       "          1.0000e-03, 3.1623e-04],\n",
       "         [2.0000e+00, 6.3246e-01, 2.0000e-01, 6.3246e-02, 2.0000e-02, 6.3246e-03,\n",
       "          2.0000e-03, 6.3246e-04]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = torch.outer(t, freqs).float()\n",
    "freqs.shape, freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de10a6",
   "metadata": {
    "input_tokens": 37
   },
   "source": [
    "- **Lower dimension indices** → higher frequencies → faster rotation\n",
    "- **Higher dimension indices** → lower frequencies → slower rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d79c7a",
   "metadata": {
    "input_tokens": 40,
    "output_tokens": 610
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 8]),\n",
       " tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
       "           1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
       "           1.0000+0.0000e+00j,  1.0000+0.0000e+00j],\n",
       "         [ 0.5403+8.4147e-01j,  0.9504+3.1098e-01j,  0.9950+9.9833e-02j,\n",
       "           0.9995+3.1618e-02j,  0.9999+9.9998e-03j,  1.0000+3.1623e-03j,\n",
       "           1.0000+1.0000e-03j,  1.0000+3.1623e-04j],\n",
       "         [-0.4161+9.0930e-01j,  0.8066+5.9113e-01j,  0.9801+1.9867e-01j,\n",
       "           0.9980+6.3203e-02j,  0.9998+1.9999e-02j,  1.0000+6.3245e-03j,\n",
       "           1.0000+2.0000e-03j,  1.0000+6.3246e-04j]]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "freqs_cis.shape, freqs_cis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a01b5",
   "metadata": {
    "input_tokens": 39
   },
   "source": [
    "![Figure 2: Long-term decay of RoPE](fig2_from_paper.png)\n",
    "\n",
    "Image from the RoPE paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5eaac",
   "metadata": {
    "input_tokens": 502
   },
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n",
    "    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n",
    "    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n",
    "    returned as real tensors.\n",
    "\n",
    "    Args:\n",
    "        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n",
    "        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n",
    "        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n",
    "    \"\"\"\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cf9d6",
   "metadata": {
    "input_tokens": 28
   },
   "source": [
    "queries and keys have shape `(batch_size, context_len, num_heads, head_dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206063c",
   "metadata": {
    "input_tokens": 34,
    "output_tokens": 72
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]), torch.Size([3, 8]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq = queries\n",
    "xk = keys\n",
    "\n",
    "xq.shape, xk.shape, freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fc029",
   "metadata": {
    "input_tokens": 9,
    "output_tokens": 34
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.shape[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7a1b7",
   "metadata": {
    "input_tokens": 37
   },
   "source": [
    "Expand last dimension of query (head_dim), 16, into 2. One for cos, the other for sin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd936a",
   "metadata": {
    "input_tokens": 28,
    "output_tokens": 43
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 8, 2])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.float().reshape(*xq.shape[:-1], -1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14d733",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 177
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "         0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fda46e",
   "metadata": {
    "input_tokens": 45,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 8])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "xq_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05532ae",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 172
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3374-0.1778j, -0.3035-0.5880j,  0.3486+0.6603j, -0.2196-0.3792j,\n",
       "         0.7671-1.1925j,  0.6984-1.4097j,  0.1794+1.8951j,  0.4954+0.2692j])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq_[0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12cd5e",
   "metadata": {
    "input_tokens": 16
   },
   "source": [
    "Then, `reshape_for_broadcast` happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebc822",
   "metadata": {
    "input_tokens": 334
   },
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Reshape frequency tensor for broadcasting it with another tensor.\n",
    "\n",
    "    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "    for the purpose of broadcasting the frequency tensor during element-wise operations.\n",
    "\n",
    "    Args:\n",
    "        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n",
    "        x (torch.Tensor): Target tensor for broadcasting compatibility.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reshaped frequency tensor.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the frequency tensor doesn't match the expected shape.\n",
    "        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960225c3",
   "metadata": {
    "input_tokens": 12
   },
   "source": [
    "Let's remind ourselves of the shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f526d",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 51
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 8]), torch.Size([2, 3, 4, 16]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis.shape, queries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be5e92",
   "metadata": {
    "input_tokens": 126
   },
   "source": [
    "Rembmer `queries` and `keys` have shape: `(batch_size, context_len, num_heads, head_dim)`. `reshape_for_broadcast` is used after queries and keys are transformed into complex space according to `apply_rotary_emb`. We want `freqs_cis` to be `(1, context_len, 1, head_dim)`. Let's use `queries` as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882b610",
   "metadata": {
    "input_tokens": 10,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 8])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = xq_\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea484d2",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 19
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndim = x.ndim\n",
    "ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9a8e0",
   "metadata": {
    "input_tokens": 40
   },
   "outputs": [],
   "source": [
    "assert 0 <= 1 < ndim\n",
    "assert freqs_cis.shape == (x.shape[1], x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a1070",
   "metadata": {
    "input_tokens": 43,
    "output_tokens": 34
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 1, 8]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac389370",
   "metadata": {
    "input_tokens": 13
   },
   "source": [
    "Going back to `apply_rotary_emb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150fafaa",
   "metadata": {
    "input_tokens": 33,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 8])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4d4dc",
   "metadata": {
    "input_tokens": 15,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 8])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xq_ * freqs_cis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d303263",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 168
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5146+0.9938j, -0.2587-1.0826j, -0.0444+1.6236j, -2.3229+1.0878j,\n",
       "         0.6716+0.6933j, -0.9487-0.0765j, -0.1526+0.1167j,  0.4403-1.4465j])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq_[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b584e057",
   "metadata": {
    "input_tokens": 16,
    "output_tokens": 202
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5403+8.4147e-01j, 0.9504+3.1098e-01j, 0.9950+9.9833e-02j,\n",
       "        0.9995+3.1618e-02j, 0.9999+9.9998e-03j, 1.0000+3.1623e-03j,\n",
       "        1.0000+1.0000e-03j, 1.0000+3.1623e-04j])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae20b8c",
   "metadata": {
    "input_tokens": 22,
    "output_tokens": 168
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5582+0.9700j,  0.0908-1.1093j, -0.2062+1.6110j, -2.3561+1.0138j,\n",
       "         0.6646+0.7000j, -0.9485-0.0795j, -0.1528+0.1166j,  0.4407-1.4464j])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xq_ * freqs_cis)[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce9b48",
   "metadata": {
    "input_tokens": 76,
    "output_tokens": 42
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5582145060000001, 0.969970602)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Under the hood\n",
    "0.5146*0.5403-0.9938*8.4147e-01, 0.5146*8.4147e-01+0.9938*0.5403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081226c9",
   "metadata": {
    "input_tokens": 202
   },
   "source": [
    "The multiplication `xq_ * freqs_cis` is complex number multiplication, which performs the rotation.\n",
    "\n",
    "When you multiply two complex numbers:\n",
    "```\n",
    "(a + bi) * (cos(θ) + i*sin(θ)) = (a*cos(θ) - b*sin(θ)) + i*(a*sin(θ) + b*cos(θ))\n",
    "```\n",
    "\n",
    "This is exactly the 2D rotation formula! So each complex number in `xq_` gets rotated by the corresponding angle in `freqs_cis`.\n",
    "\n",
    "Each position gets its own rotation angles, and each dimension pair gets its own frequency, but all heads share the same positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99339659",
   "metadata": {
    "input_tokens": 12
   },
   "source": [
    "This is what the transformation looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83313da3",
   "metadata": {
    "input_tokens": 19,
    "output_tokens": 627
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374-0.1778j, -0.3035-0.5880j,  0.3486+0.6603j, -0.2196-0.3792j,\n",
       "          0.7671-1.1925j,  0.6984-1.4097j,  0.1794+1.8951j,  0.4954+0.2692j],\n",
       "        [-0.0770-1.0205j, -0.1690+0.9178j,  1.5810+1.3010j,  1.2753-0.2010j,\n",
       "          0.4965-1.5723j,  0.9666-1.1481j, -1.1589+0.3255j, -0.6315-2.8400j],\n",
       "        [-1.3250+0.1784j, -2.1338+1.0524j, -0.3885-0.9343j, -0.4991-1.0867j,\n",
       "          0.8805+1.5542j,  0.6266-0.1755j,  0.0983-0.0935j,  0.2662-0.5850j],\n",
       "        [ 0.8768+1.6221j, -1.4779+1.1331j, -1.2203+1.3139j,  1.0533+0.1388j,\n",
       "          2.2473-0.8036j, -0.2808+0.7697j, -0.6596-0.7979j,  0.1838+0.2293j]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xq_ * freqs_cis)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46de3a4",
   "metadata": {
    "input_tokens": 31
   },
   "source": [
    "![Figure 1 from Paper](fig1_from_paper.png)\n",
    "\n",
    "Image from the RoPE paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c891ae2",
   "metadata": {
    "input_tokens": 15
   },
   "source": [
    "When we use `torch.view_as_real`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee24c0",
   "metadata": {
    "input_tokens": 25,
    "output_tokens": 702
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3374, -0.1778],\n",
       "         [-0.3035, -0.5880],\n",
       "         [ 0.3486,  0.6603],\n",
       "         [-0.2196, -0.3792],\n",
       "         [ 0.7671, -1.1925],\n",
       "         [ 0.6984, -1.4097],\n",
       "         [ 0.1794,  1.8951],\n",
       "         [ 0.4954,  0.2692]],\n",
       "\n",
       "        [[-0.0770, -1.0205],\n",
       "         [-0.1690,  0.9178],\n",
       "         [ 1.5810,  1.3010],\n",
       "         [ 1.2753, -0.2010],\n",
       "         [ 0.4965, -1.5723],\n",
       "         [ 0.9666, -1.1481],\n",
       "         [-1.1589,  0.3255],\n",
       "         [-0.6315, -2.8400]],\n",
       "\n",
       "        [[-1.3250,  0.1784],\n",
       "         [-2.1338,  1.0524],\n",
       "         [-0.3885, -0.9343],\n",
       "         [-0.4991, -1.0867],\n",
       "         [ 0.8805,  1.5542],\n",
       "         [ 0.6266, -0.1755],\n",
       "         [ 0.0983, -0.0935],\n",
       "         [ 0.2662, -0.5850]],\n",
       "\n",
       "        [[ 0.8768,  1.6221],\n",
       "         [-1.4779,  1.1331],\n",
       "         [-1.2203,  1.3139],\n",
       "         [ 1.0533,  0.1388],\n",
       "         [ 2.2473, -0.8036],\n",
       "         [-0.2808,  0.7697],\n",
       "         [-0.6596, -0.7979],\n",
       "         [ 0.1838,  0.2293]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.view_as_real(xq_ * freqs_cis)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34043075",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 649
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
       "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
       "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
       "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
       "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
       "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
       "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq_out[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97619ade",
   "metadata": {
    "input_tokens": 37,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "xq_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797ffef",
   "metadata": {
    "input_tokens": 7
   },
   "source": [
    "### torchtune implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75f5eb",
   "metadata": {
    "input_tokens": 66
   },
   "source": [
    "torchtune's [RotaryPositionalEmbeddings](https://docs.pytorch.org/torchtune/stable/generated/torchtune.modules.RotaryPositionalEmbeddings.html) follows llama's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ae227",
   "metadata": {
    "input_tokens": 16,
    "output_tokens": 63
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacce645",
   "metadata": {
    "input_tokens": 42,
    "output_tokens": 27
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RotaryPositionalEmbeddings()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_rope_emb = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=context_len, base=theta)\n",
    "tt_rope_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e518272",
   "metadata": {
    "input_tokens": 22,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 16])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_queries_rot = tt_rope_emb(queries)\n",
    "tt_queries_rot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267da72",
   "metadata": {
    "input_tokens": 15
   },
   "source": [
    "This gives us the same result as llama implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79153faa",
   "metadata": {
    "input_tokens": 18
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(tt_queries_rot, xq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0b8f2",
   "metadata": {
    "input_tokens": 6
   },
   "source": [
    "## LitGPT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe0886",
   "metadata": {
    "input_tokens": 804
   },
   "outputs": [],
   "source": [
    "# LitGPT code function `litgpt_build_rope_cache` from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py\n",
    "# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE\n",
    "def litgpt_build_rope_cache(\n",
    "    seq_len: int,\n",
    "    n_elem: int,\n",
    "    device: Optional[torch.device] = None,\n",
    "    base: int = 10000,\n",
    "    condense_ratio: int = 1,\n",
    "    extra_config: Optional[dict] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): Sequence length.\n",
    "        n_elem (int): Number of elements (head dimension).\n",
    "        device (torch.device, optional): Device for tensor allocations.\n",
    "        base (int, optional): Base for computing inverse frequencies.\n",
    "        condense_ratio (int, optional): Ratio to condense the position indices.\n",
    "        extra_config (dict, optional): Configuration parameters for frequency adjustments (used by Llama 3.1 and 3.2)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Cosine and sine caches for RoPE.\n",
    "    \"\"\"\n",
    "    # Compute the inverse frequencies theta\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, device=device).float() / n_elem))\n",
    "\n",
    "    if extra_config is not None:\n",
    "        orig_context_len = extra_config[\"original_max_seq_len\"]\n",
    "        factor = extra_config[\"factor\"]\n",
    "        low_freq_factor = extra_config[\"low_freq_factor\"]\n",
    "        high_freq_factor = extra_config[\"high_freq_factor\"]\n",
    "\n",
    "        wavelen = 2 * torch.pi / theta\n",
    "        ratio = orig_context_len / wavelen\n",
    "        smooth_factor = (ratio - low_freq_factor) / (high_freq_factor - low_freq_factor)\n",
    "        smooth_factor = torch.clamp(smooth_factor, min=0.0, max=1.0)\n",
    "\n",
    "        # Compute adjusted_theta without masked indexing\n",
    "        adjusted_theta = (1 - smooth_factor) * (theta / factor) + smooth_factor * theta\n",
    "        theta = adjusted_theta\n",
    "\n",
    "    # Create position indices `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, device=device) / condense_ratio\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af638d",
   "metadata": {
    "input_tokens": 63,
    "output_tokens": 42
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 16]), torch.Size([3, 16]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litgpt_cos, litgpt_sin = litgpt_build_rope_cache(context_len, n_elem=head_dim, base=theta)\n",
    "litgpt_cos.shape, litgpt_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabdbbb",
   "metadata": {
    "input_tokens": 42,
    "output_tokens": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis = precompute_freqs_cis(dim=head_dim, end=context_len, theta=theta)\n",
    "freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46014dc1",
   "metadata": {
    "input_tokens": 22,
    "output_tokens": 385
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000, 0.5403,\n",
       "         0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000]),\n",
       " tensor([8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n",
       "         1.0000e-03, 3.1623e-04, 8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02,\n",
       "         9.9998e-03, 3.1623e-03, 1.0000e-03, 3.1623e-04]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litgpt_cos[1], litgpt_sin[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a357fbc",
   "metadata": {
    "input_tokens": 10,
    "output_tokens": 202
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5403+8.4147e-01j, 0.9504+3.1098e-01j, 0.9950+9.9833e-02j,\n",
       "        0.9995+3.1618e-02j, 0.9999+9.9998e-03j, 1.0000+3.1623e-03j,\n",
       "        1.0000+1.0000e-03j, 1.0000+3.1623e-04j])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2e8c0",
   "metadata": {
    "input_tokens": 10,
    "output_tokens": 91
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(freqs)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b2927",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 202
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1795-4.8577e-01j, 0.6097-2.5713e-01j, 0.5472-8.3876e-02j,\n",
       "        0.5410-2.6601e-02j, 0.5404-8.4144e-03j, 0.5403-2.6610e-03j,\n",
       "        0.5403-8.4147e-04j, 0.5403-2.6610e-04j])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(freqs_cis)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408632d",
   "metadata": {
    "input_tokens": 28,
    "output_tokens": 34
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 2])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([litgpt_cos, litgpt_sin], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d41960",
   "metadata": {
    "input_tokens": 33,
    "output_tokens": 264
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 8, 2]),\n",
       " tensor([[5.4030e-01, 8.4147e-01],\n",
       "         [9.5042e-01, 3.1098e-01],\n",
       "         [9.9500e-01, 9.9833e-02],\n",
       "         [9.9950e-01, 3.1618e-02],\n",
       "         [9.9995e-01, 9.9998e-03],\n",
       "         [9.9999e-01, 3.1623e-03],\n",
       "         [1.0000e+00, 1.0000e-03],\n",
       "         [1.0000e+00, 3.1623e-04]]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.view_as_real(freqs_cis).shape, torch.view_as_real(freqs_cis)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf0b55",
   "metadata": {
    "input_tokens": 22,
    "output_tokens": 91
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5403, 0.9504, 0.9950, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.view_as_real(freqs_cis)[..., 0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a0504",
   "metadata": {
    "input_tokens": 42
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(litgpt_cos, torch.view_as_real(freqs_cis)[..., 0].repeat(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fe4cc",
   "metadata": {
    "input_tokens": 379
   },
   "outputs": [],
   "source": [
    "# LitGPT code from https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py\n",
    "# LitGPT is licensed under Apache v2: https://github.com/Lightning-AI/litgpt/blob/main/LICENSE\n",
    "def litgpt_apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    head_size = x.size(-1)\n",
    "    a = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n",
    "    b = x[..., head_size // 2:]  # (B, nh, T, hs/2)\n",
    "    rotated = torch.cat((-b, a), dim=-1)  # (B, nh, T, hs)\n",
    "    if cos.dim() > 1:\n",
    "        # batch dimensions must align\n",
    "        # sin/cos are (B, T, hs) so we unsqeeze -3 for nh\n",
    "        # we count from back because all of apply_rope does\n",
    "        cos = cos.unsqueeze(-3)\n",
    "        sin = sin.unsqueeze(-3)\n",
    "\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416ff5a",
   "metadata": {
    "input_tokens": 139
   },
   "source": [
    "`(a + bi) * (cos(θ) + i*sin(θ)) = (acos(θ) - bsin(θ)) + i(asin(θ) + bcos(θ))`\n",
    "\n",
    "`(acos(θ) - bsin(θ)) + i(asin(θ) + bcos(θ)) = (acos(θ)+ibcos(θ)) + (-bsin(θ)+iasin(θ))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12fff3",
   "metadata": {
    "input_tokens": 3,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 16])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49002f",
   "metadata": {
    "input_tokens": 19
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(tt_queries_rot, freqs_cis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948f558",
   "metadata": {
    "input_tokens": 7
   },
   "source": [
    "## Hugging Face implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcc49a",
   "metadata": {
    "input_tokens": 55
   },
   "source": [
    "Hugging Face [modeling_llama](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9b05b",
   "metadata": {
    "input_tokens": 43,
    "output_tokens": 60
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 16]), torch.Size([2, 4, 3, 16]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queires_t = queries.transpose(1,2)\n",
    "keys_t = keys.transpose(1,2)\n",
    "queires_t.shape, keys_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435995e0",
   "metadata": {
    "input_tokens": 1,
    "output_tokens": 5569
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.3737e-01, -1.7778e-01, -3.0353e-01, -5.8801e-01,  3.4861e-01,\n",
       "            6.6034e-01, -2.1964e-01, -3.7917e-01,  7.6711e-01, -1.1925e+00,\n",
       "            6.9835e-01, -1.4097e+00,  1.7938e-01,  1.8951e+00,  4.9545e-01,\n",
       "            2.6920e-01],\n",
       "          [-7.7020e-02, -1.0205e+00, -1.6896e-01,  9.1776e-01,  1.5810e+00,\n",
       "            1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n",
       "            9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n",
       "           -2.8400e+00],\n",
       "          [-1.3250e+00,  1.7843e-01, -2.1338e+00,  1.0524e+00, -3.8848e-01,\n",
       "           -9.3435e-01, -4.9914e-01, -1.0867e+00,  8.8054e-01,  1.5542e+00,\n",
       "            6.2662e-01, -1.7549e-01,  9.8284e-02, -9.3507e-02,  2.6621e-01,\n",
       "           -5.8504e-01],\n",
       "          [ 8.7684e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n",
       "            1.3139e+00,  1.0533e+00,  1.3881e-01,  2.2473e+00, -8.0364e-01,\n",
       "           -2.8084e-01,  7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n",
       "            2.2935e-01]],\n",
       "\n",
       "         [[ 5.1463e-01,  9.9376e-01, -2.5873e-01, -1.0826e+00, -4.4382e-02,\n",
       "            1.6236e+00, -2.3229e+00,  1.0878e+00,  6.7155e-01,  6.9330e-01,\n",
       "           -9.4872e-01, -7.6507e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n",
       "           -1.4465e+00],\n",
       "          [ 2.5529e-01, -5.4963e-01,  1.0042e+00,  8.2723e-01, -3.9481e-01,\n",
       "            4.8923e-01, -2.1681e-01, -1.7472e+00, -1.6025e+00, -1.0764e+00,\n",
       "            9.0315e-01, -7.2184e-01, -5.9508e-01, -7.1122e-01,  6.2296e-01,\n",
       "           -1.3729e+00],\n",
       "          [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1861e-02,\n",
       "           -4.7896e-01,  7.6681e-01,  2.7468e-02,  1.9929e+00,  1.3708e+00,\n",
       "           -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3745e-03, -9.8955e-01,\n",
       "            7.0161e-01],\n",
       "          [-9.4053e-01, -4.6806e-01,  1.0322e+00, -2.8300e-01,  4.9275e-01,\n",
       "           -1.4078e-02, -2.7466e-01, -7.6409e-01,  1.3966e+00, -9.9491e-01,\n",
       "           -1.5822e-03,  1.2471e+00, -7.7105e-02,  1.2774e+00, -1.4596e+00,\n",
       "           -2.1595e+00]],\n",
       "\n",
       "         [[-2.5822e-01, -2.0407e+00, -8.0156e-01, -8.1830e-01, -1.1820e+00,\n",
       "           -2.8774e-01, -6.0430e-01,  6.0024e-01, -1.4053e+00, -5.9217e-01,\n",
       "           -2.5479e-01,  1.1517e+00, -1.7858e-02,  4.2640e-01, -7.6574e-01,\n",
       "           -5.4514e-02],\n",
       "          [-1.2743e+00,  4.5128e-01, -2.2801e-01,  9.2238e-01,  2.0561e-01,\n",
       "           -4.9696e-01,  5.8206e-01,  2.0532e-01, -3.0177e-01, -6.7030e-01,\n",
       "           -6.1710e-01, -8.3339e-01,  4.8387e-01, -1.3493e-01,  2.1187e-01,\n",
       "           -8.7140e-01],\n",
       "          [ 6.8508e-01,  2.0024e+00, -5.4688e-01,  1.6014e+00, -2.2577e+00,\n",
       "           -1.8009e+00,  7.0147e-01,  5.7028e-01, -1.1766e+00, -2.0524e+00,\n",
       "            1.1318e-01,  1.4353e+00,  8.8307e-02, -1.2037e+00,  1.0964e+00,\n",
       "            2.4210e+00],\n",
       "          [ 1.5382e-01, -4.4516e-01,  5.5035e-01,  6.5788e-02,  6.8050e-01,\n",
       "            1.2064e+00,  1.6250e+00,  3.4595e-01,  1.3425e-01,  7.6623e-01,\n",
       "            2.2760e+00, -1.3255e+00, -8.9702e-01,  1.1318e-01,  8.3647e-01,\n",
       "            2.8520e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.7969e-01, -2.1126e+00, -2.7214e-01, -3.5100e-01,  1.1152e+00,\n",
       "           -6.1722e-01, -2.2708e+00, -1.3819e+00,  1.1721e+00, -4.3716e-01,\n",
       "           -4.0527e-01,  7.0864e-01,  9.5331e-01, -1.3035e-02, -1.3009e-01,\n",
       "           -8.7660e-02],\n",
       "          [-6.7349e-02,  2.4674e-01, -9.3917e-01, -1.0448e+00,  1.2783e+00,\n",
       "            4.1903e-01, -5.0727e-01, -6.0623e-01, -1.0532e+00,  1.8386e+00,\n",
       "           -1.0954e-01, -3.3161e-01,  9.0084e-01,  4.8398e-01, -1.3237e+00,\n",
       "            7.8692e-01],\n",
       "          [ 1.3818e+00, -6.9367e-02, -7.6117e-01,  2.4163e-01, -5.8781e-01,\n",
       "           -1.1506e+00,  1.0164e+00,  1.2343e-01,  1.1311e+00, -8.5805e-02,\n",
       "           -5.9727e-02,  3.5527e-01, -1.4355e+00,  7.2748e-02,  1.0528e-01,\n",
       "           -1.0311e+00],\n",
       "          [ 1.3113e+00, -3.5963e-02,  2.1181e-01, -8.6248e-03,  1.8576e+00,\n",
       "            2.1321e+00, -5.0561e-01, -7.9884e-01, -1.0944e+00, -1.0197e+00,\n",
       "           -5.3986e-01,  1.2117e+00, -8.6321e-01,  1.3337e+00,  7.7101e-02,\n",
       "           -5.2181e-02]],\n",
       "\n",
       "         [[ 2.3862e-01,  1.4106e-01, -1.3354e+00, -2.9340e+00,  1.1411e-01,\n",
       "           -1.2072e+00, -3.0083e-01,  1.4274e-01, -1.3027e+00, -4.9187e-01,\n",
       "           -2.1429e+00,  9.4881e-01, -5.6842e-01, -6.4643e-02,  6.6467e-01,\n",
       "           -2.7836e+00],\n",
       "          [ 1.1366e+00,  9.0886e-01,  9.4943e-01,  2.6565e-02, -9.2207e-01,\n",
       "            7.0338e-01, -3.6590e-01, -1.9654e-01, -9.2071e-01,  3.1535e-01,\n",
       "           -2.1734e-02,  3.4414e-01,  2.2710e-01, -4.5969e-01, -6.1831e-01,\n",
       "            2.4612e-01],\n",
       "          [-4.0549e-01, -8.3681e-01,  1.2277e+00, -4.2971e-01, -2.2121e+00,\n",
       "           -3.7802e-01,  9.8382e-01, -1.0895e+00,  2.0171e-01,  2.2145e-02,\n",
       "           -1.7753e+00, -7.4896e-01,  2.7808e-01, -9.6208e-01, -4.2228e-01,\n",
       "           -1.1036e+00],\n",
       "          [ 2.4727e-01,  1.4549e+00, -2.8351e-01, -3.7675e-01, -3.0577e-02,\n",
       "           -8.9448e-02, -1.9652e-01, -9.7133e-01,  9.0046e-01, -2.5233e-01,\n",
       "            1.0669e+00, -2.9846e-01,  8.5576e-01,  1.6098e+00, -1.1893e+00,\n",
       "            1.1677e+00]],\n",
       "\n",
       "         [[ 3.2765e-01, -8.3307e-01, -1.6179e+00,  2.2651e-01, -4.3815e-01,\n",
       "            3.2652e-01, -1.5786e+00, -1.3995e+00,  5.4460e-01, -8.3004e-02,\n",
       "           -1.1753e+00,  1.7825e+00,  1.7524e+00, -2.1347e-01,  4.0949e-01,\n",
       "            4.6454e-02],\n",
       "          [ 6.3669e-01, -1.9433e-01, -8.6139e-01,  5.3384e-01,  9.3758e-01,\n",
       "           -9.2248e-01,  7.0466e-01, -2.7221e-01,  1.4419e-02, -6.4115e-01,\n",
       "            2.3902e+00, -1.4256e+00, -4.6192e-01, -1.5539e+00, -3.3382e-01,\n",
       "            2.4049e-01],\n",
       "          [ 2.1065e+00,  5.5087e-01, -2.9364e-01, -1.8027e+00, -6.9333e-01,\n",
       "            1.7409e+00,  2.6979e-01,  9.5949e-01, -1.0253e+00, -5.5049e-01,\n",
       "            1.0264e+00, -5.6696e-01, -2.6584e-01, -1.1116e+00, -1.3696e+00,\n",
       "           -6.5336e-01],\n",
       "          [-1.6125e+00, -2.2840e-01,  1.8388e+00, -9.4727e-01,  1.4192e-01,\n",
       "            3.6959e-01, -1.7425e-02, -9.5746e-01, -8.1691e-01, -2.8655e-01,\n",
       "            4.3434e-01, -1.3402e-01, -2.1467e+00, -1.7984e+00, -6.8222e-01,\n",
       "           -5.1905e-01]]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7cd29",
   "metadata": {
    "input_tokens": 81
   },
   "outputs": [],
   "source": [
    "def permute(w, n_heads, dim1=dim, dim2=dim):\n",
    "    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad210d",
   "metadata": {
    "input_tokens": 3,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 16])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b330447",
   "metadata": {
    "input_tokens": 9,
    "output_tokens": 649
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
       "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
       "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
       "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
       "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
       "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
       "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac94e7b",
   "metadata": {
    "input_tokens": 12,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 16, 4])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.transpose(2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92f6ba",
   "metadata": {
    "input_tokens": 16,
    "output_tokens": 667
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.0770, -1.3250,  0.8768],\n",
       "        [-0.1778, -1.0205,  0.1784,  1.6221],\n",
       "        [-0.3035, -0.1690, -2.1338, -1.4779],\n",
       "        [-0.5880,  0.9178,  1.0524,  1.1331],\n",
       "        [ 0.3486,  1.5810, -0.3885, -1.2203],\n",
       "        [ 0.6603,  1.3010, -0.9343,  1.3139],\n",
       "        [-0.2196,  1.2753, -0.4991,  1.0533],\n",
       "        [-0.3792, -0.2010, -1.0867,  0.1388],\n",
       "        [ 0.7671,  0.4965,  0.8805,  2.2473],\n",
       "        [-1.1925, -1.5723,  1.5542, -0.8036],\n",
       "        [ 0.6984,  0.9666,  0.6266, -0.2808],\n",
       "        [-1.4097, -1.1481, -0.1755,  0.7697],\n",
       "        [ 0.1794, -1.1589,  0.0983, -0.6596],\n",
       "        [ 1.8951,  0.3255, -0.0935, -0.7979],\n",
       "        [ 0.4954, -0.6315,  0.2662,  0.1838],\n",
       "        [ 0.2692, -2.8400, -0.5850,  0.2293]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.transpose(2,3)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73d51a",
   "metadata": {
    "input_tokens": 6,
    "output_tokens": 19
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10277323",
   "metadata": {
    "input_tokens": 49,
    "output_tokens": 48
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 2, 2, 4])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.transpose(2,3).view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fe3a4",
   "metadata": {
    "input_tokens": 60,
    "output_tokens": 48
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 2, 2, 4])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.transpose(2,3).view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4).transpose(-2, -3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa305142",
   "metadata": {
    "input_tokens": 84,
    "output_tokens": 39
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 16, 4])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(queries.transpose(2,3)\n",
    " .view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4)\n",
    " .transpose(-2, -3)\n",
    " .reshape(batch_size, context_len,16, 4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d0c06",
   "metadata": {
    "input_tokens": 97,
    "output_tokens": 649
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.3035, -0.1778, -0.5880,  0.3486, -0.2196,  0.6603, -0.3792,\n",
       "          0.7671,  0.6984, -1.1925, -1.4097,  0.1794,  0.4954,  1.8951,  0.2692],\n",
       "        [-0.0770, -0.1690, -1.0205,  0.9178,  1.5810,  1.2753,  1.3010, -0.2010,\n",
       "          0.4965,  0.9666, -1.5723, -1.1481, -1.1589, -0.6315,  0.3255, -2.8400],\n",
       "        [-1.3250, -2.1338,  0.1784,  1.0524, -0.3885, -0.4991, -0.9343, -1.0867,\n",
       "          0.8805,  0.6266,  1.5542, -0.1755,  0.0983,  0.2662, -0.0935, -0.5850],\n",
       "        [ 0.8768, -1.4779,  1.6221,  1.1331, -1.2203,  1.0533,  1.3139,  0.1388,\n",
       "          2.2473, -0.2808, -0.8036,  0.7697, -0.6596,  0.1838, -0.7979,  0.2293]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(queries.transpose(2,3)\n",
    " .view(batch_size, context_len, num_heads, 16 // num_heads // 2, 2, 4)\n",
    " .transpose(-2, -3)\n",
    " .reshape(batch_size, context_len,16, 4)).transpose(2,3)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2070941",
   "metadata": {
    "input_tokens": 30,
    "output_tokens": 273
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 2, 2, 4]' is invalid for input of size 384",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mpermute\u001b[39m\u001b[34m(w, n_heads, dim1, dim2)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpermute\u001b[39m(w, n_heads, dim1=dim, dim2=dim):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim1\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim2\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).reshape(dim1, dim2)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[4, 2, 2, 4]' is invalid for input of size 384"
     ]
    }
   ],
   "source": [
    "permute(queries.transpose(2,3), num_heads, 16, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc534bc7",
   "metadata": {
    "input_tokens": 36
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, apply_rotary_pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac7e9d",
   "metadata": {
    "input_tokens": 16,
    "output_tokens": 36
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.50.1'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers_version = transformers.__version__\n",
    "transformers_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd960fd5",
   "metadata": {
    "input_tokens": 97
   },
   "outputs": [],
   "source": [
    "class RoPEConfig:\n",
    "    dim: int = head_dim\n",
    "    rope_theta = theta\n",
    "    max_position_embeddings: int = context_len\n",
    "    hidden_size = head_dim * num_heads\n",
    "    num_attention_heads = num_heads\n",
    "\n",
    "config = RoPEConfig()\n",
    "hf_rot_emb = LlamaRotaryEmbedding(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61100e05",
   "metadata": {
    "input_tokens": 28,
    "output_tokens": 33
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(context_len, dtype=torch.long).unsqueeze(0)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a56f32",
   "metadata": {
    "input_tokens": 85,
    "output_tokens": 60
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 16]), torch.Size([2, 4, 3, 16]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_cos, hf_sin = hf_rot_emb(queries, position_ids)\n",
    "hf_queries_t_rot, hf_keys_t_rot = apply_rotary_pos_emb(queires_t, keys_t, hf_cos, hf_sin)\n",
    "hf_queries_t_rot.shape, hf_keys_t_rot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6689ed0",
   "metadata": {
    "input_tokens": 55,
    "output_tokens": 60
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4, 16]), torch.Size([2, 3, 4, 16]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_queries_rot = hf_queries_t_rot.transpose(1,2)\n",
    "hf_keys_rot = hf_keys_t_rot.transpose(1,2)\n",
    "hf_queries_rot.shape, hf_keys_rot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed07bf",
   "metadata": {
    "input_tokens": 30,
    "output_tokens": 271
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 2, 2, 16]' is invalid for input of size 384",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.allclose(tt_queries_rot, \u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_queries_rot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mpermute\u001b[39m\u001b[34m(w, n_heads, dim1, dim2)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpermute\u001b[39m(w, n_heads, dim1=dim, dim2=dim):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim1\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim2\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).reshape(dim1, dim2)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[4, 2, 2, 16]' is invalid for input of size 384"
     ]
    }
   ],
   "source": [
    "assert torch.allclose(tt_queries_rot, permute(hf_queries_rot, n_heads=num_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56750506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39bf083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e2f00",
   "metadata": {
    "input_tokens": 90
   },
   "outputs": [],
   "source": [
    "torch.testing.assert_close(sin.repeat(1,2), ref_sin.squeeze(0))\n",
    "torch.testing.assert_close(cos.repeat(1,2), ref_cos.squeeze(0))\n",
    "torch.testing.assert_close(keys_rot, ref_keys_rot)\n",
    "torch.testing.assert_close(queries_rot, ref_queries_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e264f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5671b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d0898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f90c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90810cd1",
   "metadata": {
    "input_tokens": 81
   },
   "outputs": [],
   "source": [
    "def permute(w, n_heads, dim1=dim, dim2=dim):\n",
    "    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0840b",
   "metadata": {
    "input_tokens": 25
   },
   "source": [
    "For HF implementation to match with torchtune, we have to permute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf494f",
   "metadata": {
    "input_tokens": 28,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [16, 17, 18, 19, 20, 21, 22, 23],\n",
       "        [24, 25, 26, 27, 28, 29, 30, 31],\n",
       "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
       "        [40, 41, 42, 43, 44, 45, 46, 47],\n",
       "        [48, 49, 50, 51, 52, 53, 54, 55],\n",
       "        [56, 57, 58, 59, 60, 61, 62, 63]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=8\n",
    "t = torch.arange(8*8).reshape(dim,dim)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e1c03",
   "metadata": {
    "input_tokens": 46,
    "output_tokens": 366
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 2, 8]),\n",
       " tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "           [ 8,  9, 10, 11, 12, 13, 14, 15]],\n",
       " \n",
       "          [[16, 17, 18, 19, 20, 21, 22, 23],\n",
       "           [24, 25, 26, 27, 28, 29, 30, 31]]],\n",
       " \n",
       " \n",
       "         [[[32, 33, 34, 35, 36, 37, 38, 39],\n",
       "           [40, 41, 42, 43, 44, 45, 46, 47]],\n",
       " \n",
       "          [[48, 49, 50, 51, 52, 53, 54, 55],\n",
       "           [56, 57, 58, 59, 60, 61, 62, 63]]]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_heads=2\n",
    "t_ = t.view(n_heads, dim//n_heads//2, 2, dim)\n",
    "t_.shape, t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f3da1",
   "metadata": {
    "input_tokens": 34,
    "output_tokens": 366
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 2, 8]),\n",
       " tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "           [16, 17, 18, 19, 20, 21, 22, 23]],\n",
       " \n",
       "          [[ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "           [24, 25, 26, 27, 28, 29, 30, 31]]],\n",
       " \n",
       " \n",
       "         [[[32, 33, 34, 35, 36, 37, 38, 39],\n",
       "           [48, 49, 50, 51, 52, 53, 54, 55]],\n",
       " \n",
       "          [[40, 41, 42, 43, 44, 45, 46, 47],\n",
       "           [56, 57, 58, 59, 60, 61, 62, 63]]]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_t = t_.transpose(1,2)\n",
    "t_t.transpose(1,2).shape, t_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c76c5",
   "metadata": {
    "input_tokens": 10,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [16, 17, 18, 19, 20, 21, 22, 23],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [24, 25, 26, 27, 28, 29, 30, 31],\n",
       "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
       "        [48, 49, 50, 51, 52, 53, 54, 55],\n",
       "        [40, 41, 42, 43, 44, 45, 46, 47],\n",
       "        [56, 57, 58, 59, 60, 61, 62, 63]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_t.reshape(dim,dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2329d1e",
   "metadata": {
    "input_tokens": 7
   },
   "source": [
    "Permuting on columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395ac11",
   "metadata": {
    "input_tokens": 3,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n",
       "        [ 1,  9, 17, 25, 33, 41, 49, 57],\n",
       "        [ 2, 10, 18, 26, 34, 42, 50, 58],\n",
       "        [ 3, 11, 19, 27, 35, 43, 51, 59],\n",
       "        [ 4, 12, 20, 28, 36, 44, 52, 60],\n",
       "        [ 5, 13, 21, 29, 37, 45, 53, 61],\n",
       "        [ 6, 14, 22, 30, 38, 46, 54, 62],\n",
       "        [ 7, 15, 23, 31, 39, 47, 55, 63]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79798fe",
   "metadata": {
    "input_tokens": 18,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n",
       "        [ 2, 10, 18, 26, 34, 42, 50, 58],\n",
       "        [ 1,  9, 17, 25, 33, 41, 49, 57],\n",
       "        [ 3, 11, 19, 27, 35, 43, 51, 59],\n",
       "        [ 4, 12, 20, 28, 36, 44, 52, 60],\n",
       "        [ 6, 14, 22, 30, 38, 46, 54, 62],\n",
       "        [ 5, 13, 21, 29, 37, 45, 53, 61],\n",
       "        [ 7, 15, 23, 31, 39, 47, 55, 63]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute(t.T, n_heads, dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccb8cc",
   "metadata": {
    "input_tokens": 19,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  2,  1,  3,  4,  6,  5,  7],\n",
       "        [ 8, 10,  9, 11, 12, 14, 13, 15],\n",
       "        [16, 18, 17, 19, 20, 22, 21, 23],\n",
       "        [24, 26, 25, 27, 28, 30, 29, 31],\n",
       "        [32, 34, 33, 35, 36, 38, 37, 39],\n",
       "        [40, 42, 41, 43, 44, 46, 45, 47],\n",
       "        [48, 50, 49, 51, 52, 54, 53, 55],\n",
       "        [56, 58, 57, 59, 60, 62, 61, 63]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute(t.T, n_heads, dim, dim).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29120b4e",
   "metadata": {
    "input_tokens": 9
   },
   "source": [
    "Let's add a batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834da1b8",
   "metadata": {
    "input_tokens": 39,
    "output_tokens": 775
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5,   6,   7],\n",
       "         [  8,   9,  10,  11,  12,  13,  14,  15],\n",
       "         [ 16,  17,  18,  19,  20,  21,  22,  23],\n",
       "         [ 24,  25,  26,  27,  28,  29,  30,  31],\n",
       "         [ 32,  33,  34,  35,  36,  37,  38,  39],\n",
       "         [ 40,  41,  42,  43,  44,  45,  46,  47],\n",
       "         [ 48,  49,  50,  51,  52,  53,  54,  55],\n",
       "         [ 56,  57,  58,  59,  60,  61,  62,  63]],\n",
       "\n",
       "        [[ 64,  65,  66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77,  78,  79],\n",
       "         [ 80,  81,  82,  83,  84,  85,  86,  87],\n",
       "         [ 88,  89,  90,  91,  92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99, 100, 101, 102, 103],\n",
       "         [104, 105, 106, 107, 108, 109, 110, 111],\n",
       "         [112, 113, 114, 115, 116, 117, 118, 119],\n",
       "         [120, 121, 122, 123, 124, 125, 126, 127]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=8\n",
    "bs=2\n",
    "t = torch.arange(dim*dim*bs).reshape(bs,dim,dim)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f77ef",
   "metadata": {
    "input_tokens": 49,
    "output_tokens": 820
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 2, 2, 8]),\n",
       " tensor([[[[[  0,   1,   2,   3,   4,   5,   6,   7],\n",
       "            [  8,   9,  10,  11,  12,  13,  14,  15]],\n",
       " \n",
       "           [[ 16,  17,  18,  19,  20,  21,  22,  23],\n",
       "            [ 24,  25,  26,  27,  28,  29,  30,  31]]],\n",
       " \n",
       " \n",
       "          [[[ 32,  33,  34,  35,  36,  37,  38,  39],\n",
       "            [ 40,  41,  42,  43,  44,  45,  46,  47]],\n",
       " \n",
       "           [[ 48,  49,  50,  51,  52,  53,  54,  55],\n",
       "            [ 56,  57,  58,  59,  60,  61,  62,  63]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 64,  65,  66,  67,  68,  69,  70,  71],\n",
       "            [ 72,  73,  74,  75,  76,  77,  78,  79]],\n",
       " \n",
       "           [[ 80,  81,  82,  83,  84,  85,  86,  87],\n",
       "            [ 88,  89,  90,  91,  92,  93,  94,  95]]],\n",
       " \n",
       " \n",
       "          [[[ 96,  97,  98,  99, 100, 101, 102, 103],\n",
       "            [104, 105, 106, 107, 108, 109, 110, 111]],\n",
       " \n",
       "           [[112, 113, 114, 115, 116, 117, 118, 119],\n",
       "            [120, 121, 122, 123, 124, 125, 126, 127]]]]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_heads=2\n",
    "t_ = t.view(bs, n_heads, dim//n_heads//2, 2, dim)\n",
    "t_.shape, t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b3c80",
   "metadata": {
    "input_tokens": 34,
    "output_tokens": 820
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 2, 2, 8]),\n",
       " tensor([[[[[  0,   1,   2,   3,   4,   5,   6,   7],\n",
       "            [  8,   9,  10,  11,  12,  13,  14,  15]],\n",
       " \n",
       "           [[ 32,  33,  34,  35,  36,  37,  38,  39],\n",
       "            [ 40,  41,  42,  43,  44,  45,  46,  47]]],\n",
       " \n",
       " \n",
       "          [[[ 16,  17,  18,  19,  20,  21,  22,  23],\n",
       "            [ 24,  25,  26,  27,  28,  29,  30,  31]],\n",
       " \n",
       "           [[ 48,  49,  50,  51,  52,  53,  54,  55],\n",
       "            [ 56,  57,  58,  59,  60,  61,  62,  63]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 64,  65,  66,  67,  68,  69,  70,  71],\n",
       "            [ 72,  73,  74,  75,  76,  77,  78,  79]],\n",
       " \n",
       "           [[ 96,  97,  98,  99, 100, 101, 102, 103],\n",
       "            [104, 105, 106, 107, 108, 109, 110, 111]]],\n",
       " \n",
       " \n",
       "          [[[ 80,  81,  82,  83,  84,  85,  86,  87],\n",
       "            [ 88,  89,  90,  91,  92,  93,  94,  95]],\n",
       " \n",
       "           [[112, 113, 114, 115, 116, 117, 118, 119],\n",
       "            [120, 121, 122, 123, 124, 125, 126, 127]]]]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_t = t_.transpose(1,2)\n",
    "t_t.transpose(1,2).shape, t_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06d438",
   "metadata": {
    "input_tokens": 13,
    "output_tokens": 775
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   1,   2,   3,   4,   5,   6,   7],\n",
       "         [  8,   9,  10,  11,  12,  13,  14,  15],\n",
       "         [ 32,  33,  34,  35,  36,  37,  38,  39],\n",
       "         [ 40,  41,  42,  43,  44,  45,  46,  47],\n",
       "         [ 16,  17,  18,  19,  20,  21,  22,  23],\n",
       "         [ 24,  25,  26,  27,  28,  29,  30,  31],\n",
       "         [ 48,  49,  50,  51,  52,  53,  54,  55],\n",
       "         [ 56,  57,  58,  59,  60,  61,  62,  63]],\n",
       "\n",
       "        [[ 64,  65,  66,  67,  68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75,  76,  77,  78,  79],\n",
       "         [ 96,  97,  98,  99, 100, 101, 102, 103],\n",
       "         [104, 105, 106, 107, 108, 109, 110, 111],\n",
       "         [ 80,  81,  82,  83,  84,  85,  86,  87],\n",
       "         [ 88,  89,  90,  91,  92,  93,  94,  95],\n",
       "         [112, 113, 114, 115, 116, 117, 118, 119],\n",
       "         [120, 121, 122, 123, 124, 125, 126, 127]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_t.reshape(bs,dim,dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af988c4",
   "metadata": {
    "input_tokens": 7
   },
   "source": [
    "Permuting on columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1396135",
   "metadata": {
    "input_tokens": 10,
    "output_tokens": 784
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   8,  16,  24,  32,  40,  48,  56],\n",
       "         [  1,   9,  17,  25,  33,  41,  49,  57],\n",
       "         [  2,  10,  18,  26,  34,  42,  50,  58],\n",
       "         [  3,  11,  19,  27,  35,  43,  51,  59],\n",
       "         [  4,  12,  20,  28,  36,  44,  52,  60],\n",
       "         [  5,  13,  21,  29,  37,  45,  53,  61],\n",
       "         [  6,  14,  22,  30,  38,  46,  54,  62],\n",
       "         [  7,  15,  23,  31,  39,  47,  55,  63]],\n",
       "\n",
       "        [[ 64,  72,  80,  88,  96, 104, 112, 120],\n",
       "         [ 65,  73,  81,  89,  97, 105, 113, 121],\n",
       "         [ 66,  74,  82,  90,  98, 106, 114, 122],\n",
       "         [ 67,  75,  83,  91,  99, 107, 115, 123],\n",
       "         [ 68,  76,  84,  92, 100, 108, 116, 124],\n",
       "         [ 69,  77,  85,  93, 101, 109, 117, 125],\n",
       "         [ 70,  78,  86,  94, 102, 110, 118, 126],\n",
       "         [ 71,  79,  87,  95, 103, 111, 119, 127]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e021dc0",
   "metadata": {
    "input_tokens": 18,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  8, 16, 24, 32, 40, 48, 56],\n",
       "        [ 2, 10, 18, 26, 34, 42, 50, 58],\n",
       "        [ 1,  9, 17, 25, 33, 41, 49, 57],\n",
       "        [ 3, 11, 19, 27, 35, 43, 51, 59],\n",
       "        [ 4, 12, 20, 28, 36, 44, 52, 60],\n",
       "        [ 6, 14, 22, 30, 38, 46, 54, 62],\n",
       "        [ 5, 13, 21, 29, 37, 45, 53, 61],\n",
       "        [ 7, 15, 23, 31, 39, 47, 55, 63]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute(t.T, n_heads, dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4534d2",
   "metadata": {
    "input_tokens": 19,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  2,  1,  3,  4,  6,  5,  7],\n",
       "        [ 8, 10,  9, 11, 12, 14, 13, 15],\n",
       "        [16, 18, 17, 19, 20, 22, 21, 23],\n",
       "        [24, 26, 25, 27, 28, 30, 29, 31],\n",
       "        [32, 34, 33, 35, 36, 38, 37, 39],\n",
       "        [40, 42, 41, 43, 44, 46, 45, 47],\n",
       "        [48, 50, 49, 51, 52, 54, 53, 55],\n",
       "        [56, 58, 57, 59, 60, 62, 61, 63]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute(t.T, n_heads, dim, dim).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817beab7",
   "metadata": {
    "input_tokens": 16,
    "output_tokens": 333
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [16, 17, 18, 19, 20, 21, 22, 23],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [24, 25, 26, 27, 28, 29, 30, 31],\n",
       "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
       "        [48, 49, 50, 51, 52, 53, 54, 55],\n",
       "        [40, 41, 42, 43, 44, 45, 46, 47],\n",
       "        [56, 57, 58, 59, 60, 61, 62, 63]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute(t, 2, dim, dim)"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
