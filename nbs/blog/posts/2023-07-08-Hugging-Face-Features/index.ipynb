{
 "cells": [
  {
   "cell_type": "raw",
   "id": "565461f3",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hugging Face Features\"\n",
    "author: \"galopy\"\n",
    "date: \"July 8, 2023\"\n",
    "toc: true\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: galopyz/delicious-nbdev\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d641e0",
   "metadata": {},
   "source": [
    "![Hugging Face Datasets](Hugging_Face_Datasets.png \"Hugging Face Datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1bcac",
   "metadata": {},
   "source": [
    "In [lesson 15](https://course.fast.ai/Lessons/lesson15.html) of the Practical Deep Learning For Coders, we used Hugging Face Datasets to download Fasion MNIST data and trained our model. I faced a problem here because I could not fit my model fast enough. Even using all my CPU cores, it was not as fast as Jeremy's computer. Even on Google Colab, CPU was not strong. It was not a huge problem, but it was annoying, so I decided to find a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bd635",
   "metadata": {},
   "source": [
    "In the lesson, we downloaded images and applied a transform function to convert them into tensors. With `dsd.with_transform`, the transform happened every batch and it took the most of the time. We don't have to apply transform every batch. So, let's find a way to only do it once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7068ff",
   "metadata": {},
   "source": [
    "Initially, I just wanted to convert images into tensors with `map`, but Hugging Face used Apache Arrow, which does not support `tensors` type. So, I used Hugging Face `Features` and `Array2D` to fix this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751681d",
   "metadata": {},
   "source": [
    "Here is the orginal approach that takes a long time from the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c06bcd",
   "metadata": {},
   "source": [
    "## Original approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda45056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.conv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f2f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch import optim, nn,tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2f91eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32631324",
   "metadata": {},
   "source": [
    "First, we grab the data from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3082bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/home/kappa/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86f2d6e7237400ba4c38a93b96d7991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c52c0e",
   "metadata": {},
   "source": [
    "Here is a inplace transform function. This function is applied every batch and converts images into tensors with the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f721a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f243a",
   "metadata": {},
   "source": [
    "Since `with_transform` applies the transform function every new batch, this is good for applying data augmentations or place where we want randomness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d766e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "tds = dsd.with_transform(transformi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b67c5c",
   "metadata": {},
   "source": [
    "Now we make a Pytorch DataLoaders. We can say how many processors we want to use. We are using 4 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bedc69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 784]), tensor([7, 3, 0, 8, 5, 6, 5, 6, 6, 6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaders.from_dd(tds, bs, num_workers=4)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d91d3",
   "metadata": {},
   "source": [
    "This is the `Learner` class. It is not very flexible, but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32df4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.xb,self.yb = to_device(self.batch)\n",
    "        self.preds = self.model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad(): self.calc_stats()\n",
    "\n",
    "    def calc_stats(self):\n",
    "        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss*n)\n",
    "        self.ns.append(n)\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for self.num,self.batch in enumerate(dl): self.one_batch()\n",
    "        n = sum(self.ns)\n",
    "        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.accs,self.losses,self.ns = [],[],[]\n",
    "        self.model.to(def_device)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)\n",
    "            with torch.no_grad(): self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aceda7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,nh = 28*28,50\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfae1a",
   "metadata": {},
   "source": [
    "We fit, but this is not very fast.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d10fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 0.7011013671875 0.74685\n",
      "0 False 0.695386328125 0.7485\n",
      "CPU times: user 13.1 s, sys: 855 ms, total: 13.9 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2)\n",
    "learn.fit(1)\n",
    "# Using only 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70f90c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 1.1755578125 0.6038166666666667\n",
      "0 False 1.1214051339285713 0.6162714285714286\n",
      "CPU times: user 4.93 s, sys: 510 ms, total: 5.44 s\n",
      "Wall time: 8.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2)\n",
    "learn.fit(1)\n",
    "# Using 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4126a8",
   "metadata": {},
   "source": [
    "Okay. We used 4 processors to train the model here but it is still not very fast. Let's make it faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab31df",
   "metadata": {},
   "source": [
    "## Faster fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d66c9",
   "metadata": {},
   "source": [
    "By using Hugging Face `Features`, we can turn images into tensors when we download the data. First, we use `load_data_builder` to look at the metadata, such as the features, splits, description of the data, and etc. without actually downloading the data yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b5065e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = load_dataset_builder(name)\n",
    "builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aac38bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd_features = dsd['train'].features.copy()\n",
    "dsd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e41409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Array2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e08ae9",
   "metadata": {},
   "source": [
    "We use `Array2D` to turn the images into 2D arrays with a certain shape and dtype. It is a bit weird using `Array2D` and `shape=[1, 28*28]` instead of something like `Array` or `Array1D` and `shape=[28*28]`. However, Hugging Face does not have that. We can just use map to unsqueeze it. However, this won't be a problem with colored images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5844dc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Array2D(shape=(1, 784), dtype='float32', id=None),\n",
       " 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd_features['image'] = Array2D(shape=[1, 28*28], dtype='float32')\n",
    "dsd_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796abc6",
   "metadata": {},
   "source": [
    "Now we load the dataset using those features, but this is a `list`! Why is it not a tensor? We have to set the format to `torch` in order to make it as tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35d8d3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08432ebb783440ba965cffb138c5b1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd = load_dataset(name, features=dsd_features)\n",
    "type(dsd['train'][0][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d2f996f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd.set_format(type=\"torch\")\n",
    "type(dsd['train'][0][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c3f6aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd['train'][0][x].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91f0a7",
   "metadata": {},
   "source": [
    "Now, we just need to squeeze each tensor to get rid of useless 1 in the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2ffde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def sq(b): b[x] = [o.squeeze() for o in b[x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0b768",
   "metadata": {},
   "source": [
    "Here, we use `map` to squeeze them. With `batched=True`, it is faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40098203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1319dafed934a86a402f973eccc407c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4da1079cd7d424e8972f28fbe8f4c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = dsd.map(sq, batched=True)\n",
    "tds['train'][0][x].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64365b",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "### Why not just use `torch.tensor`?\n",
    "\n",
    "So, why didn't we just use `torch.tensor` in the beginning and used `Features` and `Array2D`? Because Hugging Face converts tensors back to images. Hugging Face uses Apache Arrow, and it does not support tensors are not supported. So data have to be either list or image, and we do not want image.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da703e",
   "metadata": {},
   "source": [
    "Now, it is in the right shape. However, the difference is that it does not have to keep converting from image to tensor every batch. With `map`, there is no calculation on flight, which is what we want here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a966a727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 784]), tensor([6, 1, 4, 7, 9, 8, 4, 8, 4, 0]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaders.from_dd(tds, bs, num_workers=0)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0c463",
   "metadata": {},
   "source": [
    "Now, it is very fast to train even with only one worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48359313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 2.30262421875 0.09783333333333333\n",
      "0 False 2.30261875 0.09814285714285714\n",
      "CPU times: user 5.26 s, sys: 178 ms, total: 5.43 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86187451",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b530e54",
   "metadata": {},
   "source": [
    "We used `Features` and `Array2D` to convert images into tensors for faster training. It was awkward using `Array2D` when we want `Array1D`, but it was not a problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
